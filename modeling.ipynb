{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tData Source:   'D:\\Dataset\\HyperspectralBlueberry\\BlueberryScansforDestructiveTesting06132022\\GoodBlueberryScans\\Good Blueberry 1-42.bil'\n",
      "\t# Rows:           1800\n",
      "\t# Samples:        1600\n",
      "\t# Bands:           462\n",
      "\tInterleave:        BIL\n",
      "\tQuantization:  16 bits\n",
      "\tData format:    uint16\n",
      "\tData Source:   'D:\\Dataset\\HyperspectralBlueberry\\BlueberryScansforDestructiveTesting06132022\\BadBlueberryScans\\Bad Blueberry 1-42.bil'\n",
      "\t# Rows:           1800\n",
      "\t# Samples:        1600\n",
      "\t# Bands:           462\n",
      "\tInterleave:        BIL\n",
      "\tQuantization:  16 bits\n",
      "\tData format:    uint16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "root_dir = r'D:\\Dataset\\Hyperspectral\\Blueberry'\n",
    "tmp_save_dir = r'D:\\test'\n",
    "sys.path.insert(0, root_dir)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from spectral import *\n",
    "from  pathlib import Path\n",
    "import cv2 as cv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import time \n",
    "\n",
    "from modeling_spectral_data import load_hyper_spectral_data, good_dir, bad_dir, good_dir1, bad_dir1\n",
    "from modeling_spectral_data import save_dir_im_data_good, save_dir_im_data_bad, save_dir_im_data_good1, save_dir_im_data_bad1\n",
    "from modeling_spectral_data import get_calibrated_image, find_nearest\n",
    "\n",
    "\"\"\"\n",
    "ref_data\n",
    "1st day 1800\n",
    "\n",
    "2nd day 1600\n",
    "    ref_data[:1600, :, 0]\n",
    "\"\"\"\n",
    "\n",
    "hyper_spectral_data_dict, data_idx_dict = load_hyper_spectral_data()\n",
    "ref_data = hyper_spectral_data_dict['ref_data']\n",
    "\n",
    "def log_xalg(*info, log_path=None, show=True, end=None, flush=False):\n",
    "    if show:\n",
    "        if end is not None:\n",
    "            print(*info, end=end)\n",
    "        else:\n",
    "            print(*info)\n",
    "    if log_path is not None:\n",
    "        if flush:\n",
    "            f_log = open(log_path, 'w', encoding=\"utf-8\")\n",
    "        else:\n",
    "            f_log = open(log_path, 'a', encoding=\"utf-8\")\n",
    "            print(*info, file=f_log)\n",
    "        f_log.close()\n",
    "\n",
    "def plotly_fig_config(fig, title):\n",
    "    fig.update_layout({\n",
    "        'plot_bgcolor': 'rgba(255,255,255,1)',\n",
    "        'paper_bgcolor': 'rgba(255,255,255,1)'\n",
    "    })\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            showline=True,\n",
    "            showgrid=False,\n",
    "            showticklabels=True,\n",
    "            zeroline=False,\n",
    "            #linecolor='rgb(204, 204, 204)',\n",
    "            linewidth=2,\n",
    "            ticks='outside',\n",
    "            tickfont=dict(\n",
    "                family='Times New Roman',\n",
    "                size=20,\n",
    "                color='rgb(0, 0, 0)',\n",
    "            ),\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            showline=True,\n",
    "            showgrid=False,\n",
    "            showticklabels=True,\n",
    "            zeroline=False,\n",
    "            #linecolor='rgb(204, 204, 204)',\n",
    "            tickfont=dict(\n",
    "                family='Times New Roman',\n",
    "                size=20,\n",
    "                color='rgb(0, 0, 0)',\n",
    "            ),\n",
    "        ),\n",
    "        autosize=False,\n",
    "        #automargin=True,\n",
    "        margin=dict(\n",
    "            autoexpand=False,\n",
    "            l=50,\n",
    "            r=50,\n",
    "            t=50,\n",
    "            b=40,\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        #plot_bgcolor='white'\n",
    "    )\n",
    "    fig.add_shape(\n",
    "            # Rectangle with reference to the plot\n",
    "                type=\"rect\",\n",
    "                xref=\"paper\",\n",
    "                yref=\"paper\",\n",
    "                x0=0,\n",
    "                y0=0,\n",
    "                x1=1.0,\n",
    "                y1=1.0,\n",
    "                line=dict(\n",
    "                    color=\"black\",\n",
    "                    width=1,\n",
    "                )\n",
    "            )\n",
    "    fig.update_layout(title_text='<b>'+title+'</b>', title_x=0.5, title_font_family=\"Times New Roman\", title_font_size=30)\n",
    "    return fig\n",
    "\n",
    "def show_statistics(x, reshape=False):\n",
    "    xxx = np.array(x)\n",
    "    if reshape:\n",
    "        xxx = xxx.reshape(-1)\n",
    "    df_describe = pd.DataFrame(xxx)\n",
    "    print(df_describe.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.8.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spectral\n",
    "spectral.__version__\n",
    "import cv2\n",
    "cv2.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_good good_1_42 \tData Source:   'D:\\Dataset\\HyperspectralBlueberry\\BlueberryScansforDestructiveTesting06132022\\GoodBlueberryScans\\Good Blueberry 1-42.bil'\n",
      "\t# Rows:           1800\n",
      "\t# Samples:        1600\n",
      "\t# Bands:           462\n",
      "\tInterleave:        BIL\n",
      "\tQuantization:  16 bits\n",
      "\tData format:    uint16\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "not use cropping now\n",
    "\"\"\"\n",
    "#data_good\n",
    "#data_bad\n",
    "data_name = 'data_good'\n",
    "data = data_idx_dict[data_name]['data']\n",
    "sample_start_idx = data_idx_dict[data_name]['start_idx']\n",
    "save_name = data_idx_dict[data_name]['name']\n",
    "\n",
    "wavelengths = data.metadata['wavelength']\n",
    "wavelengths = [float(x) for x in wavelengths]\n",
    "data_num = len(wavelengths)\n",
    "title_window = 'x'\n",
    "\n",
    "print(data_name, save_name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "waitKey_time = 1\n",
    "save_showed_im = 0\n",
    "global_im_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "imshow(winname, mat) -> None\n",
    ". The function may scale the image, depending on its depth:\n",
    ". - If the image is 8-bit unsigned, it is displayed as is.\n",
    ". - If the image is 16-bit unsigned or 32-bit integer, the pixels are divided by 256. \n",
    "    That is, the value range [0,255\\*256] is mapped to [0,255].\n",
    ". - If the image is 32-bit or 64-bit floating-point, the pixel values are multiplied by 255. That is, the\n",
    ".   value range [0,1] is mapped to [0,255].\n",
    "\"\"\"\n",
    "def im_resize_show(image, width = 600, height = None, inter = cv2.INTER_AREA, waitKey_time=waitKey_time, name='x', save_showed_im=save_showed_im):\n",
    "    # initialize the dimensions of the image to be resized and\n",
    "    # grab the image size\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "\n",
    "    # check to see if the width is None\n",
    "    if width is None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "\n",
    "    # otherwise, the height is None\n",
    "    else:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    # resize the image\n",
    "    resized = cv.resize(image, dim, interpolation = inter)\n",
    "    cv.namedWindow(name, 1)\n",
    "    cv.imshow(name, resized)\n",
    "    save_dir = os.path.join(root_dir, 'results')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    global global_im_idx\n",
    "    print('global_im_idx:', global_im_idx, 'image.dtype:', image.dtype, 'image min:', image.min(), 'image max:', image.max())\n",
    "    if image.dtype != np.uint8:\n",
    "        print('clamp image range')\n",
    "        image_norm = image*255\n",
    "        print('global_im_idx:', global_im_idx, 'image.dtype:', image_norm.dtype, 'image min:', image_norm.min(), 'image max:', image_norm.max())\n",
    "        idxes = np.where(image_norm > 255)\n",
    "        image_norm[idxes[0], idxes[1]] = 255\n",
    "        print('global_im_idx:', global_im_idx, 'image.dtype:', image_norm.dtype, 'image min:', image_norm.min(), 'image max:', image_norm.max())\n",
    "\n",
    "        idxes = np.where(image_norm < 0)\n",
    "        image_norm[idxes[0], idxes[1]] = 0\n",
    "        image_norm = np.round(image_norm).astype(np.uint8)\n",
    "        print('global_im_idx:', global_im_idx, 'image.dtype:', image_norm.dtype, 'image min:', image_norm.min(), 'image max:', image_norm.max())\n",
    "        image_norm=cv.normalize(image_norm,  None, 0, 255, cv.NORM_MINMAX)\n",
    "    else:\n",
    "        image_norm=cv.normalize(image,  None, 0, 255, cv.NORM_MINMAX)\n",
    "    if save_showed_im:\n",
    "        save_path = os.path.join(save_dir, name+'_'+str(global_im_idx)+'.jpg')\n",
    "        print('save to', save_path)\n",
    "        cv.imwrite(save_path, image_norm)\n",
    "    global_im_idx += 1\n",
    "    print('waitKey_time', waitKey_time)\n",
    "    res = cv.waitKey(waitKey_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435.88 32\n",
      "545.54 115\n",
      "700.03 231\n",
      "global_im_idx: 0 image.dtype: uint8 image min: 0 image max: 255\n",
      "waitKey_time 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "460/550/640\n",
    "435.8/546.1/700\n",
    "\n",
    "show keep aspect ratio\n",
    "700 nm (red), 546.1 nm (green) and 435.8 nm (blue).\n",
    "\"\"\"\n",
    "# 700 nm (red), 546.1 nm (green) and 435.8 nm (blue)\n",
    "b = get_calibrated_image(435.8, wavelengths, data, ref_data)[:, :, None]\n",
    "g = get_calibrated_image(546.1, wavelengths, data, ref_data)[:, :, None]\n",
    "r = get_calibrated_image(700, wavelengths, data, ref_data)[:, :, None]\n",
    "\n",
    "rgb = np.concatenate([b,g,r], 2).squeeze()\n",
    "\n",
    "#rgb1 = rgb/rgb.max()\n",
    "\n",
    "#rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
    "#rgb = np.round(rgb*255).astype(np.uint8)\n",
    "#im_resize_show(rgb)\n",
    "\n",
    "#import plotly.express as px\n",
    "#fig = px.imshow(rgb)\n",
    "#fig.show()\n",
    "\n",
    "rgb_quantization = rgb*255\n",
    "idxes = np.where(rgb_quantization > 255)\n",
    "rgb_quantization[idxes[0], idxes[1]] = 255\n",
    "rgb_quantization = np.round(rgb_quantization).astype(np.uint8)\n",
    "#diff_im_optimize = cv.cvtColor(rgb_quantization, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "im_resize_show(rgb_quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "#show datacube\n",
    "#\"\"\"\n",
    "#from spectral import open_image\n",
    "##view = imshow(b[:,:,0,0])\n",
    "##from spectral.graphics import view_cube\n",
    "#b = get_calibrated_image(435.8, wavelengths, data, ref_data)[:, :, None]\n",
    "#g = get_calibrated_image(546.1, wavelengths, data, ref_data)[:, :, None]\n",
    "#r = get_calibrated_image(700, wavelengths, data, ref_data)[:, :, None]\n",
    "#im1 = get_calibrated_image(400, wavelengths, data, ref_data)[:, :, None]\n",
    "#im2 = get_calibrated_image(500, wavelengths, data, ref_data)[:, :, None]\n",
    "#im3 = get_calibrated_image(600, wavelengths, data, ref_data)[:, :, None]\n",
    "#im4 = get_calibrated_image(750, wavelengths, data, ref_data)[:, :, None]\n",
    "#im5 = get_calibrated_image(800, wavelengths, data, ref_data)[:, :, None]\n",
    "#im6 = get_calibrated_image(900, wavelengths, data, ref_data)[:, :, None]\n",
    "#data_cube = np.concatenate([r,g,b, im1, im2, im3, im4, im5, im6], 2).squeeze()\n",
    "##data_cube = data_cube.transpose(2,0,1)\n",
    "#print(data_cube.shape)\n",
    "##data=rgb_quantization\n",
    "##import spectral\n",
    "##import wx\n",
    "##app=wx.App()\n",
    "##spectral.settings.WX_GL_DEPTH_SIZE = 16\n",
    "##spectral.view_cube(data_cube)\n",
    "##app.MainLoop()\n",
    "##data.shape\n",
    "##open_image(r'D:\\BoyangDeng\\BlueberryClassification\\datasets\\BlueberryScansforDestructiveTesting06132022\\GoodBlueberryScans\\Good Blueberry 1-42.bil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata cube\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "data cube\n",
    "\"\"\"\n",
    "#data.shape\n",
    "#right = data[:, -1, :].squeeze()\n",
    "#top = data[0, :, :].squeeze()\n",
    "#front = data[:, :, 5].squeeze()\n",
    "#print(right.shape, top.shape, front.shape)\n",
    "\n",
    "#pad=np.zeros((200,462))\n",
    "#top=np.concatenate([pad, top], 0)\n",
    "\n",
    "##im = right\n",
    "#front = front\n",
    "#front=cv.normalize(front,  None, 0, 255, cv.NORM_MINMAX)\n",
    "#front = np.round(front).astype(np.uint8)\n",
    "\n",
    "#top = top\n",
    "#top=cv.normalize(top,  None, 0, 255, cv.NORM_MINMAX)\n",
    "#top = np.round(top).astype(np.uint8)\n",
    "\n",
    "#right = right\n",
    "#right=cv.normalize(right,  None, 0, 255, cv.NORM_MINMAX)\n",
    "#right = np.round(right).astype(np.uint8)\n",
    "\n",
    "#tot=np.concatenate([front, top, right], 1)\n",
    "\n",
    "#im_resize_show(tot, save_showed_im=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679.96 216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900.58 380\n",
      "-0.5947371006753429 1.1656922697337206\n",
      "diff_im: float64\n",
      "global_im_idx: 1 image.dtype: float64 image min: -0.5947371006753429 image max: 1.1656922697337206\n",
      "clamp image range\n",
      "global_im_idx: 1 image.dtype: float64 image min: -151.65796067221245 image max: 297.2515287820988\n",
      "global_im_idx: 1 image.dtype: float64 image min: -151.65796067221245 image max: 255.0\n",
      "global_im_idx: 1 image.dtype: uint8 image min: 0 image max: 255\n",
      "waitKey_time 1\n"
     ]
    }
   ],
   "source": [
    "#im0 = get_calibrated_image(428, wavelengths, data, ref_data)\n",
    "im1 = get_calibrated_image(680, wavelengths, data, ref_data)\n",
    "im2 = get_calibrated_image(900, wavelengths, data, ref_data)\n",
    "\n",
    "#show_im(im2-im1)\n",
    "diff_im = im2-im1\n",
    "diff_im = diff_im[:, :, 0]\n",
    "print(diff_im.min(), diff_im.max())\n",
    "print('diff_im:', diff_im.dtype)\n",
    "\n",
    "im_resize_show(diff_im)\n",
    "\n",
    "#import plotly.express as px\n",
    "#fig = px.imshow(diff_im)\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnot use\\nbackground part pixels is randomly, so subtraction may be not near to zero and have some negative value to -0.5\\nforeground pixels subtraction max pixels is around 1.1 \\nso this fomula will decrease the contrast\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "not use\n",
    "background part pixels is randomly, so subtraction may be not near to zero and have some negative value to -0.5\n",
    "foreground pixels subtraction max pixels is around 1.1 \n",
    "so this fomula will decrease the contrast\n",
    "\"\"\"\n",
    "#diff_im_norm = cv.normalize(diff_im, diff_im, 0,255, cv.NORM_MINMAX)\n",
    "\n",
    "#diff_im_norm = (diff_im - diff_im.min()) / (diff_im.max() - diff_im.min())\n",
    "#diff_im_norm = np.round(diff_im_norm*255).astype(np.uint8)\n",
    "\n",
    "#import plotly.express as px\n",
    "#fig = px.imshow(diff_im_norm)\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n",
      "global_im_idx: 2 image.dtype: uint8 image min: 0 image max: 255\n",
      "waitKey_time 1\n"
     ]
    }
   ],
   "source": [
    "idxes = np.where(diff_im < 0.0)\n",
    "diff_im_optimize = diff_im.copy()\n",
    "diff_im_optimize[idxes[0], idxes[1]] = 0\n",
    "\n",
    "#idxes = np.where(diff_im_optimize > 1.0)\n",
    "#diff_im_optimize[idxes[0], idxes[1]] = 1.0\n",
    "\n",
    "diff_im_optimize = diff_im_optimize / diff_im_optimize.max()\n",
    "print(diff_im_optimize.min(), diff_im_optimize.max())\n",
    "diff_im_optimize = np.round(diff_im_optimize*255).astype(np.uint8)\n",
    "\n",
    "im_resize_show(diff_im_optimize)\n",
    "\n",
    "#import plotly.express as px\n",
    "#fig = px.imshow(diff_im_optimize)\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_im_idx: 3 image.dtype: uint8 image min: 0 image max: 255\n",
      "waitKey_time 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "maybe use in special cases to connect boundary of defect samples\n",
    "equalizeHist cause high frequency noise in background\n",
    "\"\"\"\n",
    "diff_im_optimize_eh = cv.equalizeHist(diff_im_optimize)\n",
    "im_resize_show(diff_im_optimize_eh)\n",
    "\n",
    "#import plotly.express as px\n",
    "#fig = px.imshow(diff_im_optimize)\n",
    "#fig.write_image(r\"F:\\test\\diff_im_optimize.png\", width=1400, height=1000)\n",
    "\n",
    "##pip install -U kaleido\n",
    "#fig = px.imshow(diff_im_optimize_eh)\n",
    "#fig.write_image(r\"F:\\test\\diff_im_optimize_eh.png\", width=1400, height=1000)\n",
    "\n",
    "#fig.write_html(r\"F:\\test\\xxx.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh: 145\n",
      "global_im_idx: 4 image.dtype: uint8 image min: 0 image max: 255\n",
      "waitKey_time 1\n"
     ]
    }
   ],
   "source": [
    "from skimage.filters import threshold_yen, threshold_otsu, try_all_threshold\n",
    "#thresh = threshold_yen(diff_im_optimize)\n",
    "#ret, diff_im_optimize_t = cv.threshold(diff_im_optimize, thresh, 255, cv.THRESH_BINARY)\n",
    "\"\"\"\n",
    "maybe use in special case to connect boundary of defect samples\n",
    "\"\"\"\n",
    "\n",
    "thresh = threshold_yen(diff_im_optimize_eh)\n",
    "ret, diff_im_optimize_t = cv.threshold(diff_im_optimize_eh, thresh, 255, cv.THRESH_BINARY)\n",
    "ret, diff_im_optimize_t = cv.threshold(diff_im_optimize_eh, thresh-50, 255, cv.THRESH_BINARY)\n",
    "\n",
    "print('thresh:', thresh)\n",
    "#diff_im_optimize_t = diff_im_optimize > thresh\n",
    "#diff_im_optimize_t = diff_im_optimize_t.astype(np.uint8)*255\n",
    "\n",
    "im_resize_show(diff_im_optimize_t)\n",
    "\n",
    "#import plotly.express as px\n",
    "#fig = px.imshow(diff_im_optimize_t)\n",
    "#fig.write_image(r\"F:\\test\\diff_im_optimize_t.png\", width=1400, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_im_idx: 5 image.dtype: uint8 image min: 0 image max: 255\n",
      "waitKey_time 1\n"
     ]
    }
   ],
   "source": [
    "# for convinenent use larger filter\n",
    "#kernel=cv.getStructuringElement(cv.MORPH_RECT,(20,20))\n",
    "kernel=cv.getStructuringElement(cv.MORPH_RECT,(5,5))\n",
    "#kernel=cv.getStructuringElement(cv.MORPH_RECT,(3,3))\n",
    "iterations = 1\n",
    "eroded=cv.erode(diff_im_optimize_t,kernel,iterations=iterations)\n",
    "kernel=cv.getStructuringElement(cv.MORPH_RECT,(3,3))\n",
    "\n",
    "# 4/2\n",
    "iterations = 2\n",
    "diff_im_optimize_open=cv.dilate(eroded,kernel, iterations=iterations)\n",
    "\n",
    "\"\"\"\n",
    "maybe use in special case to connect boundary of defect samples\n",
    "\"\"\"\n",
    "iterations = 8\n",
    "diff_im_optimize_open=cv.dilate(diff_im_optimize_open,kernel, iterations=iterations)\n",
    "diff_im_optimize_open=cv.erode(diff_im_optimize_open,kernel, iterations=iterations)\n",
    "\n",
    "#iterations = 4\n",
    "#diff_im_optimize_open = cv.erode(diff_im_optimize_open, kernel, iterations=iterations)\n",
    "\n",
    "im_resize_show(diff_im_optimize_open)\n",
    "\n",
    "#import plotly.express as px\n",
    "#fig = px.imshow(diff_im_optimize_open)\n",
    "#fig.write_image(r\"F:\\test\\diff_im_optimize_open.png\", width=1400, height=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter the metal plate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop_y_start = data_idx_dict[data_name]['crop_y_start']\n",
    "#crop_x_start = data_idx_dict[data_name]['crop_x_start']\n",
    "\n",
    "#diff_im_optimize_open_crop = diff_im_optimize_open[crop_y_start:, crop_x_start:]\n",
    "#cv.namedWindow(title_window, 0)\n",
    "#cv.imshow(title_window, diff_im_optimize_open_crop)\n",
    "#cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rgb_crop = rgb[crop_y_start:, crop_x_start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff_crop = diff_im_optimize[crop_y_start:, crop_x_start:]\n",
    "#diff_crop_3_channels = diff_crop[:,:,None]\n",
    "#diff_crop_3_channels = np.concatenate([diff_crop_3_channels,diff_crop_3_channels,diff_crop_3_channels], 2).squeeze()\n",
    "#diff_crop_3_channels.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### circle filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_im_idx: 6 image.dtype: uint8 image min: 0 image max: 255\n",
      "waitKey_time 1\n"
     ]
    }
   ],
   "source": [
    "diff_3_channels = diff_im_optimize[:,:,None]\n",
    "diff_3_channels = np.concatenate([diff_3_channels,diff_3_channels,diff_3_channels], 2).squeeze()\n",
    "im_resize_show(diff_3_channels)\n",
    "\n",
    "#import plotly.express as px\n",
    "#fig = px.imshow(diff_im_optimize)\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 137, 3)\n",
      "global_im_idx: 7 image.dtype: uint8 image min: 0 image max: 255\n",
      "waitKey_time 1\n"
     ]
    }
   ],
   "source": [
    "from modeling_filter_metal_plate import detect_circle\n",
    "find_circle_list, img_circle_bgr = detect_circle(diff_im_optimize_open)\n",
    "im_resize_show(img_circle_bgr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "im_copy = np.zeros_like(diff_im_optimize_open)\n",
    "res = cv.findContours(image=diff_im_optimize_open, mode=cv.RETR_EXTERNAL, method=cv.CHAIN_APPROX_SIMPLE)\n",
    "_, contours, hierarchy = res\n",
    "print(len(contours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_im_idx: 8 image.dtype: uint8 image min: 0 image max: 255\n",
      "waitKey_time 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "valid_contours = []\n",
    "im_circles = np.zeros_like(diff_im_optimize_open)\n",
    "\n",
    "for circle in find_circle_list:\n",
    "    x,y,r = circle\n",
    "    cv.circle(im_circles, (x,y), r, 255, -1)\n",
    "\n",
    "im_resize_show(im_circles)\n",
    "\n",
    "for contour in contours:\n",
    "    im_one_conotur = np.zeros_like(diff_im_optimize_open)\n",
    "    cv.drawContours(image=im_one_conotur, contours=[contour], contourIdx=-1,\n",
    "                    color=255, thickness=-1, lineType=cv.LINE_AA)\n",
    "    res_im = im_circles * im_one_conotur\n",
    "    idxes = np.where(res_im > 0)\n",
    "    if len(idxes[0]) > 0:\n",
    "        valid_contours.append(contour)\n",
    "print(len(valid_contours))\n",
    "contours = valid_contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_with_contour = rgb.copy()\n",
    "cv.drawContours(image=rgb_with_contour, contours=contours, contourIdx=-1,\n",
    "                color=[0.1, 0.5, 0.2], thickness=3, lineType=cv.LINE_AA)\n",
    "# see the results\n",
    "im_resize_show(rgb_with_contour)\n",
    "#import plotly.express as px\n",
    "#fig = px.imshow(rgb_with_contour*255)\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_im_idx: 10 image.dtype: uint8 image min: 0 image max: 255\n",
      "waitKey_time 1\n"
     ]
    }
   ],
   "source": [
    "diff_3_channels_contour = diff_3_channels.copy()\n",
    "cv.drawContours(image=diff_3_channels_contour, contours=contours, contourIdx=-1,\n",
    "                color=[10, 200, 50], thickness=2, lineType=cv.LINE_AA)\n",
    "# see the results\n",
    "im_resize_show(diff_3_channels_contour)\n",
    "\n",
    "#import plotly.express as px\n",
    "#fig = px.imshow(diff_3_channels_contour)\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "global_im_idx: 11 image.dtype: float64 image min: 0.0 image max: 200.0\n",
      "clamp image range\n",
      "global_im_idx: 11 image.dtype: float64 image min: 0.0 image max: 51000.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_im_idx: 11 image.dtype: float64 image min: 0.0 image max: 255.0\n",
      "global_im_idx: 11 image.dtype: uint8 image min: 0 image max: 255\n",
      "waitKey_time 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "note order should be correct\n",
    "\n",
    "rgb_with_contour/diff_3_channels_contour\n",
    "\"\"\"\n",
    "im_copy = rgb_with_contour.copy()\n",
    "font=cv.FONT_HERSHEY_DUPLEX\n",
    "center_pts = []\n",
    "center_refs = []\n",
    "for i_contour, contour in enumerate(contours):\n",
    "    x = int(contour[:, :, 0].mean())\n",
    "    y = int(contour[:, :, 1].mean())\n",
    "    # base\n",
    "    #center_refs.append(x+10*y)\n",
    "    # bad 127-168 / bad 1-42 / others seem all OK\n",
    "    center_refs.append(x+7*y)\n",
    "    center_pts.append((x, y))\n",
    "idx_sorted = np.argsort(center_refs)\n",
    "# [10, 50, 200] [0.1, 0.3, 0.8]\n",
    "for i_idx_sorted, idx in enumerate(idx_sorted):\n",
    "    x, y = center_pts[idx]\n",
    "    cv.putText(im_copy, str(i_idx_sorted+ 1 + sample_start_idx) , (x-5,y+5), font, 1, [10, 50, 200], 2)\n",
    "print(len(contours))\n",
    "im_resize_show(im_copy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean spectral value of each band of each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 462/462 [25:27<00:00,  3.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "from tqdm import tqdm\n",
    "\n",
    "wavelengths = data.metadata['wavelength']\n",
    "wavelengths = [float(x) for x in wavelengths]\n",
    "data_num = len(wavelengths)\n",
    "hyper_dict = {}\n",
    "for i_band in tqdm(range(data_num), ncols=80):\n",
    "    im = data[: , :, i_band]\n",
    "    ref = ref_data[:im.shape[0] , :, i_band]\n",
    "    im_corrected = im/ref\n",
    "    #im_corrected = im_corrected[crop_y_start:, crop_x_start:]\n",
    "    \n",
    "    for i_idx_sorted, idx in enumerate(idx_sorted):\n",
    "        contour = contours[idx]\n",
    "        mask = np.zeros_like(im_corrected)\n",
    "        cv.drawContours(mask, [contour], -1, 255,-1)\n",
    "        pts  = np.where(mask == 255)\n",
    "        roi = im_corrected[pts[0], pts[1]]\n",
    "        mean_spectral = round(np.mean(roi), 4)\n",
    "        if i_idx_sorted not in hyper_dict:\n",
    "            hyper_dict[i_idx_sorted] = []\n",
    "        hyper_dict[i_idx_sorted].append(mean_spectral)\n",
    "hyper_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/BoyangDeng/BlueberryClassification/datasets/mean_data_python/good_1_42.npy')"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = os.path.join(root_dir, 'mean_data_python')\n",
    "save_dir = Path(save_dir)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "save_path = save_dir / (save_name+'.npy').__str__()\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\BoyangDeng\\BlueberryClassification\\datasets\\mean_data_python\\bad_1_42.npy\n"
     ]
    }
   ],
   "source": [
    "np.save(save_path, hyper_dict) \n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\BoyangDeng\\BlueberryClassification\\datasets\\mean_data_python\\good_1_42.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "462"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_dict = np.load(save_path,allow_pickle='TRUE').item()\n",
    "print(save_path)\n",
    "len(hyper_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_spectral_data import show_mean_spectral_data\n",
    "show_mean_spectral_data(hyper_dict, 10, wavelengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_spectral_data import show_hyper_dict\n",
    "show_hyper_dict(hyper_dict, wavelengths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save each sample each band data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_origin_data_good = good_dir / 'origin_data'\n",
    "save_dir_im_data_good = good_dir / 'im_data_local'\n",
    "\n",
    "save_dir_origin_data_bad = bad_dir / 'origin_data'\n",
    "save_dir_im_data_bad = bad_dir / 'im_data_local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = data_idx_dict[data_name]['data_dir']\n",
    "save_dir_origin_data = root_dir / 'origin_data'\n",
    "save_dir_origin_data.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#save_dir_im_data_global = root_dir / 'im_data_global'\n",
    "#save_dir_im_data_global.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_dir_im_data_local = root_dir / 'im_data_local'\n",
    "save_dir_im_data_local.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/462 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 462/462 [17:44<00:00,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "from tqdm import tqdm\n",
    "\n",
    "wavelengths = data.metadata['wavelength']\n",
    "wavelengths = [float(x) for x in wavelengths]\n",
    "data_num = len(wavelengths)\n",
    "hs=[]\n",
    "ws=[]\n",
    "for i_band in tqdm(range(data_num), ncols=80):\n",
    "#for i_band in tqdm(range(0,1), ncols=80):\n",
    "    im = data[: , :, i_band]\n",
    "    ref = ref_data[:im.shape[0] , :, i_band]\n",
    "    im_corrected = im/ref\n",
    "    #im_corrected = im_corrected[crop_y_start:, crop_x_start:]\n",
    "    \n",
    "    for i_idx_sorted, idx in enumerate(idx_sorted):\n",
    "        contour = contours[idx]\n",
    "        mask = np.zeros_like(im_corrected)\n",
    "        cv.drawContours(mask, [contour], -1, 255,-1)\n",
    "        pts  = np.where(mask == 255)\n",
    "\n",
    "        y0 = pts[0].min()\n",
    "        y1 = pts[0].max()\n",
    "        x0 = pts[1].min()\n",
    "        x1 = pts[1].max()\n",
    "        hs.append(y1-y0)\n",
    "        ws.append(x1-x0)\n",
    "        #continue\n",
    "\n",
    "        one_sample = im_corrected[y0:y1+1, x0:x1+1]\n",
    "\n",
    "        save_dir_origin_data_one_sample = save_dir_origin_data / str(sample_start_idx + i_idx_sorted+1)\n",
    "        save_dir_origin_data_one_sample.mkdir(parents=True, exist_ok=True)\n",
    "        save_path = save_dir_origin_data_one_sample / f'{i_band}_{wavelengths[i_band]}.npy'\n",
    "        np.save(save_path, one_sample)\n",
    "\n",
    "        #save_dir_im_data_one_sample_global = save_dir_im_data_global / str(sample_start_idx + i_idx_sorted+1)\n",
    "        #save_dir_im_data_one_sample_global.mkdir(parents=True, exist_ok=True)\n",
    "        #save_path = save_dir_im_data_one_sample_global / f'{i_band}_{wavelengths[i_band]}.png'\n",
    "        #highest_t = 3.0\n",
    "        #one_sample = np.array(one_sample/highest_t*255, dtype=np.uint8)\n",
    "        #cv.imwrite(save_path.__str__(), one_sample)\n",
    "\n",
    "        save_dir_im_data_one_sample_local = save_dir_im_data_local / str(sample_start_idx + i_idx_sorted+1)\n",
    "        save_dir_im_data_one_sample_local.mkdir(parents=True, exist_ok=True)\n",
    "        save_path = save_dir_im_data_one_sample_local / f'{i_band}_{wavelengths[i_band]}.png'\n",
    "        one_sample = (one_sample - one_sample.min()) / (one_sample.max() - one_sample.min())\n",
    "        one_sample = np.array(one_sample*255, dtype=np.uint8)\n",
    "        cv.imwrite(save_path.__str__(), one_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130.3095238095238, 150.64285714285714)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(hs), np.average(ws)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Vision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show dimension reduction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 462)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from modeling_spectral_data import load_spectral_mean_data_xy\n",
    "x, y = load_spectral_mean_data_xy()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from modeling_spectral_data import load_spectral_mean_data\n",
    "#hyper_dict_good_tot, hyper_dict_bad_tot = load_spectral_mean_data()\n",
    "#from modeling_spectral_data import show_hyper_dict\n",
    "#show_hyper_dict(hyper_dict_bad_tot, wavelengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_norm = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, spatial_features_norm.shape\n",
    "x = spatial_features_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['PC1', 'PC2'])\n",
    "finalDf = pd.concat([principalDf, pd.Series(y, name='class')], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "#font = {'family' : 'normal',\n",
    "#        'weight' : 'normal',\n",
    "#        'size'   : 12}\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "#fig = plt.figure(figsize = (8,8))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "# ax.set_xlabel('PC1 (85.9%)', fontsize = 16)\n",
    "# ax.set_ylabel('PC2 (7.0%)', fontsize = 16)\n",
    "ax.set_xlabel('PC1 (30.2%)', fontsize = 16)\n",
    "ax.set_ylabel('PC2 (16.0%)', fontsize = 16)\n",
    "#ax.set_title('PCA with two components', fontsize = 16)\n",
    "ax.xaxis.labelpad = 5\n",
    "ax.yaxis.labelpad = 0\n",
    "#pl.xlabel(\"...\", labelpad=20)\n",
    "target_labels = ['Sound Blueberry', 'Defective Blueberry']\n",
    "targets = [0, 1]\n",
    "colors = ['#FFD300', '#8CA77B']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['class'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'PC1']\n",
    "               , finalDf.loc[indicesToKeep, 'PC2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(target_labels, fontsize = 16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "ax.tick_params(axis=\"y\",direction=\"in\")\n",
    "#ax.grid()\n",
    "fig.tight_layout()\n",
    "plt.rcParams['figure.dpi'] = 800\n",
    "plt.rcParams['savefig.dpi'] = 800\n",
    "plt.subplots_adjust(left=0.11, right=0.95, top=0.94, bottom=0.12)\n",
    "#plt.show(block=True)\n",
    "plt.savefig(r'D:\\test\\test.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['PC1', 'PC2', 'PC3'])\n",
    "finalDf = pd.concat([principalDf, pd.Series(y, name='class')], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3022, 0.16  ])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 12}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax= Axes3D(fig)\n",
    "ax.set_xlabel('Principal Component 1', fontsize = 12)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 12)\n",
    "ax.set_zlabel('Principal Component 3', fontsize = 12)\n",
    "ax.set_title('PCA with 3 components')\n",
    "ax.xaxis.labelpad = 10\n",
    "ax.yaxis.labelpad = 10\n",
    "ax.zaxis.labelpad = 10\n",
    "target_labels = ['good', 'bad']\n",
    "targets = [0, 1]\n",
    "colors = ['g', 'r']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['class'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'PC1']\n",
    "               , finalDf.loc[indicesToKeep, 'PC2']\n",
    "               , finalDf.loc[indicesToKeep, 'PC3']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(target_labels)\n",
    "ax.grid()\n",
    "plt.show(block=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detect bands by loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(x)\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.,  0.,  0.]), array([1., 1., 1.]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca_norm = (X_pca - X_pca.mean(0)) / X_pca.std(0)\n",
    "X_pca_norm.mean(0), X_pca_norm.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'normal' not found.\n",
      "findfont: Font family 'normal' not found.\n",
      "findfont: Font family 'normal' not found.\n",
      "findfont: Font family 'normal' not found.\n",
      "findfont: Font family 'normal' not found.\n",
      "findfont: Font family 'normal' not found.\n",
      "findfont: Font family 'normal' not found.\n",
      "findfont: Font family 'normal' not found.\n",
      "findfont: Font family 'normal' not found.\n",
      "findfont: Font family 'normal' not found.\n"
     ]
    }
   ],
   "source": [
    "for i_show in range(X_pca.shape[1]):\n",
    "    #if i_show>=2:\n",
    "    #    break\n",
    "    plt.plot(X_pca[:, i_show])\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import IncrementalPCA\n",
    "#ipca = IncrementalPCA(n_components=20, batch_size=10)\n",
    "#X_ipca = ipca.fit_transform(x)\n",
    "#X_ipca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.4486, 0.1996, 0.1795]), array([0.8589, 0.07  , 0.063 ]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_, pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadings_v0 = pca.components_.T\n",
    "loadings_v0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 462)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadings_v1 = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "loadings_v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.448596740037898 0 315 0.11973731156406772\n",
      "2.448596740037898 0 316 0.11973536372486668\n",
      "2.448596740037898 0 314 0.11972967619189143\n",
      "0.19959045101872908 1 254 0.036906699942220206\n",
      "0.19959045101872908 1 255 0.03689026337304528\n",
      "0.19959045101872908 1 256 0.036855523336462206\n",
      "0.1794962545654013 2 237 0.0362675675912167\n",
      "0.1794962545654013 2 236 0.036246718963353\n",
      "0.1794962545654013 2 238 0.036210753054797595\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "loadings_idxes = np.zeros_like(loadings_v1)\n",
    "show_num = 3\n",
    "feature_idx_list = []\n",
    "for i_feat in range(loadings_v1.shape[1]):\n",
    "    loadings_idxes[:, i_feat] = np.argsort(loadings_v1[:, i_feat])[::-1]\n",
    "    for i_show in range(show_num):\n",
    "        idx = loadings_idxes[i_show, i_feat]\n",
    "        idx = int(idx)\n",
    "        ev = pca.explained_variance_[i_feat]\n",
    "        print(ev, i_feat , idx, loadings_v1[idx, i_feat])\n",
    "        #if ev > 1e-3 and loadings_v1[idx, i_feat] > 1e-2:\n",
    "        if ev > 5e-4 and loadings_v1[idx, i_feat] > 1e-3:\n",
    "            if idx not in feature_idx_list:\n",
    "                feature_idx_list.append(idx)\n",
    "print(len(feature_idx_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AFSALab\\AppData\\Local\\Temp\\ipykernel_22032\\4284100505.py:16: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  return plt.cm.get_cmap(name, n)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_feat: 0 315 0.11973731156406772 156 0.011480691926757816\n",
      "i_feat: 1 254 0.036906699942220206 15 -0.030539535294593488\n",
      "i_feat: 2 237 0.0362675675912167 386 -0.02244744554321167\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "plot\n",
    "\"\"\"\n",
    "show_num = 3\n",
    "\n",
    "font = {'family' : 'Times New Roman',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "#colors = ['#FFD300', '#8CA77B', '#FFC183']\n",
    "colors = ['#FFD300', '#8CA77B', '#e6194B']\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "cmap = get_cmap(3*show_num)\n",
    "\n",
    "loadings_idxes = np.zeros_like(loadings_v1)\n",
    "feature_idx_list = []\n",
    "fig=plt.figure()\n",
    "for i_feat in range(loadings_v1.shape[1]):\n",
    "    if i_feat >= show_num:\n",
    "        break\n",
    "    loadings_per_component = loadings_v1[:, i_feat]\n",
    "    feature_idx_list.append(loadings_per_component.argmax())\n",
    "    feature_idx_list.append(loadings_per_component.argmin())\n",
    "\n",
    "    max_idx = loadings_per_component.argmax()\n",
    "    max_value = loadings_per_component.max()\n",
    "\n",
    "    min_idx = loadings_per_component.argmin()\n",
    "    min_value = loadings_per_component.min()\n",
    "    print('i_feat:', i_feat, max_idx, max_value, min_idx, min_value)\n",
    "    plt.plot(wavelengths, loadings_per_component, label=f'PC {i_feat+1}', color=colors[i_feat])\n",
    "\n",
    "    # color=cmap(3*i_feat)\n",
    "    plt.scatter(wavelengths[max_idx], max_value, color=colors[i_feat])\n",
    "    plt.scatter(wavelengths[min_idx], min_value, color=colors[i_feat])\n",
    "#plt.title('PCA loadings of the first three primary components')\n",
    "plt.xlabel('Wavelength (nm)', labelpad=5)\n",
    "plt.ylabel('Loadings of principal components', labelpad=0)\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "ax.tick_params(axis=\"y\",direction=\"in\")\n",
    "\n",
    "#plt.margins(0.2)\n",
    "plt.legend()\n",
    "plt.xlim([wavelengths[0], wavelengths[-1]])\n",
    "fig.tight_layout()\n",
    "plt.rcParams['figure.dpi'] = 800\n",
    "plt.rcParams['savefig.dpi'] = 800\n",
    "plt.subplots_adjust(left=0.11, right=0.95, top=0.94, bottom=0.11)\n",
    "#plt.show(block=True)\n",
    "plt.savefig(r'D:\\test\\PCA_loadings.png', bbox_inches='tight')\n",
    "#plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, [315, 156, 15, 258, 240, 388])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_idx_list), feature_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([812.8, 599.97, 413.52, 736.18, 712.06, 911.41],\n",
       " [413.52, 599.97, 712.06, 736.18, 812.8, 911.41])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "band_list = []\n",
    "for i in feature_idx_list:\n",
    "    band_list.append(wavelengths[i])\n",
    "band_list1=sorted(band_list)\n",
    "band_list, band_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pca] >Column labels are auto-completed.\n",
      "[pca] >Row labels are auto-completed.\n",
      "[pca] >The PCA reduction is performed to capture [99.0%] explained variance using the [462] columns of the input data.\n",
      "[pca] >Fit using PCA.\n",
      "[pca] >Compute loadings and PCs.\n",
      "[pca] >Compute explained variance.\n",
      "[pca] >Number of components is [3] that covers the [99.00%] explained variance.\n",
      "[pca] >The PCA reduction is performed on the [462] columns of the input dataframe.\n",
      "[pca] >Fit using PCA.\n",
      "[pca] >Compute loadings and PCs.\n",
      "[pca] >Outlier detection using Hotelling T2 test with alpha=[0.05] and n_components=[3]\n",
      "[pca] >Outlier detection using SPE/DmodX with n_std=[2]\n"
     ]
    }
   ],
   "source": [
    "from pca import pca\n",
    "\n",
    "# Initialize to reduce the data up to the number of componentes that explains 95% of the variance.\n",
    "model = pca(n_components=0.99)\n",
    "# Or reduce the data towards 2 PCs\n",
    "#model = pca(n_components=2)\n",
    "results = model.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['PC'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot explained variance\n",
    "fig, ax = model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter first 2 PCs\n",
    "fig, ax = model.scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pca] >Plot PC1 vs PC2 vs PC3 with loadings.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "#%matplotlib inline\n",
    "\n",
    "# Make biplot with the number of features\n",
    "fig, ax = model.biplot(y=y, n_feat=4, PC=[0,1,2], d3=True)\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414.83\n",
      "713.4\n",
      "814.14\n"
     ]
    }
   ],
   "source": [
    "#find_nearest(wavelengths)\n",
    "importances = [16, 241, 316]\n",
    "for i in importances:\n",
    "    print(wavelengths[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA/QDA -> rLDA (and other models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,RepeatedStratifiedKFold,GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import ConfusionMatrixDisplay,precision_score,recall_score,confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((420, 462), (420,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling_spectral_data import load_spectral_mean_data_xy\n",
    "x, y, hyper_dict_tot = load_spectral_mean_data_xy(use_part_1=1, use_part_2=1, return_dict=True)\n",
    "x_norm = (x-x.mean(0))/x.std(0)\n",
    "X =  pd.DataFrame(x)\n",
    "X_norm =  pd.DataFrame(x_norm)\n",
    "label = pd.DataFrame(y, columns=['label'])\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_spectral_data import show_hyper_dict\n",
    "show_hyper_dict(hyper_dict_tot, wavelengths, title='Blueberry Spectra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from modeling_spectral_data import load_spectral_mean_data_matlab\n",
    "#x, y = load_spectral_mean_data_matlab()\n",
    "\n",
    "from modeling_spectral_data import load_spectral_mean_data\n",
    "hyper_dict_good_tot, hyper_dict_bad_tot = load_spectral_mean_data(use_part_1=1, use_part_2=1)\n",
    "\n",
    "from modeling_spectral_data import show_hyper_dict\n",
    "show_hyper_dict(hyper_dict_bad_tot, wavelengths, title='Blueberry Spectra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "df = pd.concat([X, label], axis= 1)\n",
    "\n",
    "label = le.fit_transform(df['label'])\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(int(time.time()))\n",
    "#random_state = np.random.randint(0, 1e4)\n",
    "random_state=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, label, test_size=0.30, random_state=random_state, shuffle=True, stratify=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(data=df, x='label')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA from lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0., -0., -0., -0.,  0.]), array([1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "can not be used\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x_norm = StandardScaler().fit_transform(X_train)\n",
    "x_norm.mean(axis=0)[:5], x_norm.std(axis=0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0., -0., -0., -0.,  0.]), array([1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "x_norm1 = scale(X_train)\n",
    "x_norm1.mean(axis=0)[:5], x_norm1.std(axis=0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = x_norm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0., -0.,  0., -0.,  0.]), array([1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = scale(X_test)\n",
    "X_test.mean(axis=0)[:5], X_test.std(axis=0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA.fit_transform(X_train,y_train)\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "LDA.fit(X_train,y_train)\n",
    "predictions = LDA.predict(X_test)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, predictions)\n",
    "tn, fp, fn, tp = confusion_matrix(list(y_test.values.squeeze()), list(predictions), labels=[0, 1]).ravel()\n",
    "#tn, fp, fn, tp = confusion_matrix(list(y_test), list(predictions), labels=[0, 1]).ravel()\n",
    "\n",
    "print('True Positive :', tp)\n",
    "print('True Negative :', tn)\n",
    "print('False Positive :', fp)\n",
    "print('False Negative :', fn)\n",
    "print(\"Precision score\", precision_score(y_test, predictions))\n",
    "print(\"Recall score\", recall_score(y_test, predictions))\n",
    "plt.show(block=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9126984126984127"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-11/126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = LDA.predict(X_train)\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, train_predictions)\n",
    "tn, fp, fn, tp = confusion_matrix(list(y_train.values.squeeze()), list(train_predictions), labels=[0, 1]).ravel()\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda = LDA.transform(X_train)\n",
    "if X_lda.shape[1] >= 2:\n",
    "    plt.xlabel('LD1')\n",
    "    plt.ylabel('LD2')\n",
    "    plt.scatter(\n",
    "        X_lda[:,0],\n",
    "        X_lda[:,1],\n",
    "        c=y,\n",
    "        cmap='rainbow',\n",
    "        alpha=0.5,\n",
    "        edgecolors='b'\n",
    "    )\n",
    "    plt.show(block=True)\n",
    "print(LDA.explained_variance_ratio_)\n",
    "print(X_lda[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 22.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.911066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.036459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.859155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.904762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.919355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.966102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  100.000000\n",
       "mean     0.911066\n",
       "std      0.036459\n",
       "min      0.859155\n",
       "25%      0.904762\n",
       "50%      0.906250\n",
       "75%      0.919355\n",
       "max      0.966102"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "repeated tests\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "test_num = 100\n",
    "accuracies = []\n",
    "for i_test in tqdm(range(test_num)):\n",
    "    np.random.seed(int(time.time()))\n",
    "    random_state = np.random.randint(0, 1e4)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.30, random_state=random_state, shuffle=True, stratify=label)\n",
    "    y_train = y_train.squeeze()\n",
    "    LDA = LinearDiscriminantAnalysis()\n",
    "    LDA.fit(X_train, y_train)\n",
    "    predictions = LDA.predict(X_test)\n",
    "    accuracy = precision_score(y_test, predictions)\n",
    "    accuracies.append(accuracy)\n",
    "accuracies = np.array(accuracies)\n",
    "df_describe = pd.DataFrame(accuracies)\n",
    "df_describe.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repeated tests (many models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,cross_val_score,RepeatedStratifiedKFold,GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X_train, y_train, X_test, y_test, model, precisions, recalls, accuracies, one_hot_y):\n",
    "    if one_hot_y:\n",
    "        y_train = pd.get_dummies(y_train)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    if one_hot_y:\n",
    "        predictions = np.array([np.argmax(i) for i in predictions])\n",
    "    #conf_mat = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "    if type(y_test) == np.ndarray:\n",
    "        diffs = np.where(predictions!=y_test.squeeze())\n",
    "    else:\n",
    "        diffs = np.where(predictions!=y_test.values.squeeze())\n",
    "    accuracy = 1-len(diffs[0])/len(y_test)\n",
    "    accuracies.append(accuracy)\n",
    "    return precisions, recalls, accuracies\n",
    "\n",
    "def show_results(precisions, recalls, accuracies, log_path=None):\n",
    "    precisions = np.array(precisions)\n",
    "    df_describe = pd.DataFrame(precisions)\n",
    "    log_xalg('precision:', log_path=log_path)\n",
    "    log_xalg(df_describe.describe(), '\\n', log_path=log_path)\n",
    "    recalls = np.array(recalls)\n",
    "    df_describe = pd.DataFrame(recalls)\n",
    "    log_xalg('recall:', log_path=log_path)\n",
    "    log_xalg(df_describe.describe(), '\\n', log_path=log_path)\n",
    "\n",
    "    accuracies = np.array(accuracies)\n",
    "    df_describe = pd.DataFrame(accuracies)\n",
    "    log_xalg('accuracy:', log_path=log_path)\n",
    "    log_xalg(df_describe.describe(), log_path=log_path)\n",
    "\n",
    "def repeated_tests(model_class, X, label, split_ratio=0.3, test_num=100, model_params=None, seed=None, log_path=None, one_hot_y=False):\n",
    "    from tqdm import tqdm\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    accuracies = []\n",
    "\n",
    "    if type(model_class) is list:\n",
    "        precisions = {}\n",
    "        recalls = {}\n",
    "        accuracies = {}\n",
    "\n",
    "    for i_test in tqdm(range(test_num)):\n",
    "        if seed is None:\n",
    "            np.random.seed(int(time.time()))\n",
    "        random_state = np.random.randint(0, 1e4)\n",
    "        if type(model_class) is list:\n",
    "            for i_model, one_model_class in enumerate(model_class):\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X[i_model], label, test_size=split_ratio, random_state=random_state, shuffle=True, stratify=label)\n",
    "                y_train = y_train.squeeze()\n",
    "                    \n",
    "                if model_params is None:\n",
    "                    model = one_model_class()\n",
    "                else:\n",
    "                    assert type(model_params) is list\n",
    "                    if model_params[i_model] is None:\n",
    "                        model = one_model_class()\n",
    "                    else:\n",
    "                        params = dict(model_params[i_model])\n",
    "                        model = one_model_class(**params)\n",
    "\n",
    "                if i_model not in precisions:\n",
    "                    precisions[i_model] = []\n",
    "                    recalls[i_model] = []\n",
    "                    accuracies[i_model] = []\n",
    "                fit_model(X_train, y_train, X_test, y_test, model, precisions[i_model], recalls[i_model], accuracies[i_model], one_hot_y=one_hot_y)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, label, test_size=split_ratio, random_state=random_state, shuffle=True, stratify=label)\n",
    "            y_train = y_train.squeeze()\n",
    "\n",
    "            if model_params is None:\n",
    "                model = model_class()\n",
    "            else:\n",
    "                model = model_class(**model_params)\n",
    "            fit_model(X_train, y_train, X_test, y_test, model, precisions, recalls, accuracies, one_hot_y=one_hot_y)\n",
    "\n",
    "    print('start show results')\n",
    "    if type(model_class) is list:\n",
    "        for i_model, model in enumerate(model_class):\n",
    "            print('i_model:', i_model)\n",
    "            show_results(precisions[i_model], recalls[i_model], accuracies[i_model], log_path)\n",
    "    else:\n",
    "        print(X.shape, X_train.shape, X_test.shape, '\\n')\n",
    "        show_results(precisions, recalls, accuracies, log_path)\n",
    "    return precisions, recalls, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AFSALab\\.conda\\envs\\pytorch\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning:\n",
      "\n",
      "The total space of parameters 3 is smaller than n_iter=10. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: 0.749\n",
      "model_params= {'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "#,'max_depth':[2,4,6,8,10]\n",
    "model = DecisionTreeClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=2, random_state=0)\n",
    "grid = dict()\n",
    "grid['criterion'] = ['gini', 'entropy', 'log_loss']\n",
    "#RandomizedSearchCV GridSearchCV\n",
    "search = RandomizedSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "results = search.fit(X_norm_with_spatial_features_norm, label.values)\n",
    "print('metric: %.3f' % results.best_score_)\n",
    "print('model_params=',results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 42.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start show results\n",
      "(420, 69) (294, 69) (126, 69) \n",
      "\n",
      "precision:\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.760307\n",
      "std      0.048162\n",
      "min      0.650794\n",
      "25%      0.729437\n",
      "50%      0.762712\n",
      "75%      0.790323\n",
      "max      0.881356 \n",
      "\n",
      "recall:\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.756508\n",
      "std      0.046523\n",
      "min      0.650794\n",
      "25%      0.730159\n",
      "50%      0.753968\n",
      "75%      0.793651\n",
      "max      0.888889 \n",
      "\n",
      "accuracy:\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.757778\n",
      "std      0.040570\n",
      "min      0.650794\n",
      "25%      0.730159\n",
      "50%      0.753968\n",
      "75%      0.785714\n",
      "max      0.857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "np.random.seed(0)\n",
    "seed=True\n",
    "#repeated_tests(DecisionTreeClassifier, X_norm, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "#repeated_tests(DecisionTreeClassifier, spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "#repeated_tests(DecisionTreeClassifier, X_norm_with_spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "\n",
    "#repeated_tests(DecisionTreeClassifier, X_norm_with_spatial_features_norm_MRMR_norm, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "\n",
    "#repeated_tests(DecisionTreeClassifier, X_norm_mrmr, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "#repeated_tests(DecisionTreeClassifier, spatial_features_mrmr, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "_=repeated_tests(DecisionTreeClassifier, X_norm_mrmr_with_spatial_features_norm_mrmr, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "\n",
    "#repeated_tests(DecisionTreeClassifier, X_norm_with_spatial_features_norm_PCA_norm, label, split_ratio=0.3, test_num=100, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AFSALab\\.conda\\envs\\pytorch\\lib\\site-packages\\sklearn\\model_selection\\_search.py:909: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: 0.835\n",
      "model_params= {'criterion': 'entropy', 'n_estimators': 90}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import numpy as np\n",
    "#,'max_depth':[2,4,6,8,10]\n",
    "model = RandomForestClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=2, random_state=0)\n",
    "grid = dict()\n",
    "grid['n_estimators'] = np.arange(10, 110, 10).tolist()\n",
    "# grid['min_samples_leaf'] = [1,2,4]\n",
    "# grid['max_features'] = ['sqrt', 'log2', None]\n",
    "grid['criterion'] = ['gini', 'entropy', 'log_loss']\n",
    "#RandomizedSearchCV GridSearchCV\n",
    "search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "results = search.fit(X_norm_with_spatial_features_norm, label.values)\n",
    "print('metric: %.3f' % results.best_score_)\n",
    "print('model_params=',results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start show results\n",
      "(420, 69) (294, 69) (126, 69) \n",
      "\n",
      "precision:\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.859404\n",
      "std      0.038785\n",
      "min      0.774194\n",
      "25%      0.828125\n",
      "50%      0.862069\n",
      "75%      0.891396\n",
      "max      0.959184 \n",
      "\n",
      "recall:\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.790476\n",
      "std      0.046291\n",
      "min      0.650794\n",
      "25%      0.761905\n",
      "50%      0.793651\n",
      "75%      0.825397\n",
      "max      0.888889 \n",
      "\n",
      "accuracy:\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.829762\n",
      "std      0.029853\n",
      "min      0.761905\n",
      "25%      0.809524\n",
      "50%      0.833333\n",
      "75%      0.849206\n",
      "max      0.896825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "np.random.seed(0)\n",
    "seed=True\n",
    "#model_params={'n_estimators': 100, 'min_samples_leaf': 1, 'max_features': None, 'criterion': 'gini'}\n",
    "#_=repeated_tests(RandomForestClassifier, X_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "#model_params= {'n_estimators': 30, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'criterion': 'log_loss'}\n",
    "#_=repeated_tests(RandomForestClassifier, spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#model_params= {'n_estimators': 30, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'criterion': 'entropy'}\n",
    "#repeated_tests(RandomForestClassifier, X_norm_with_spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#model_params={'criterion': 'gini', 'max_features': None, 'min_samples_leaf': 1, 'n_estimators': 30}\n",
    "#precisions, recalls, accuracies = repeated_tests(RandomForestClassifier, X_norm_with_spatial_features_norm_MRMR_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "model_params= {'n_estimators': 60, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'criterion': 'entropy'}\n",
    "precisions, recalls, accuracies = repeated_tests(RandomForestClassifier, X_norm_mrmr_with_spatial_features_norm_mrmr, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#model_params= {'n_estimators': 90, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'criterion': 'gini'}\n",
    "#precisions, recalls, accuracies = repeated_tests(RandomForestClassifier, X_norm_with_spatial_features_norm_PCA_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AFSALab\\.conda\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: 0.871\n",
      "model_params= {'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AFSALab\\.conda\\envs\\pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "#,'max_depth':[2,4,6,8,10]\n",
    "model = LogisticRegression()\n",
    "cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=2, random_state=0)\n",
    "grid = dict()\n",
    "grid['solver'] = ['lbfgs', 'liblinear',  'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "# grid['penalty'] = [\"l1\", \"l2\", \"elasticnet\", None]\n",
    "\n",
    "grid['solver'] = ['lbfgs', 'newton-cg', 'newton-cholesky', 'sag', ]\n",
    "grid['penalty'] = [\"l2\", None]\n",
    "\n",
    "# grid['solver'] = ['liblinear', ]\n",
    "# grid['penalty'] = [\"l2\", 'l1']\n",
    "\n",
    "# grid['solver'] = ['saga', ]\n",
    "# grid['penalty'] = [\"l1\", \"l2\", \"elasticnet\", None]\n",
    "\n",
    "#RandomizedSearchCV GridSearchCV\n",
    "search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "results = search.fit(X_norm_with_spatial_features_norm, label.values)\n",
    "print('metric: %.3f' % results.best_score_)\n",
    "print('model_params=',results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#solver = lbfgs liblinear\n",
    "np.random.seed(0)\n",
    "seed=True\n",
    "model_params={'solver': 'liblinear'}\n",
    "#repeated_tests(LogisticRegression, X_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "#repeated_tests(LogisticRegression, spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "#_=repeated_tests(LogisticRegression, X_norm_with_spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#repeated_tests(LogisticRegression, X_norm_with_spatial_features_norm_MRMR_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "model_params= {'solver': 'saga'}\n",
    "_=repeated_tests(LogisticRegression, X_norm_mrmr_with_spatial_features_norm_mrmr, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#repeated_tests(LogisticRegression, X_norm_with_spatial_features_norm_PCA_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "model = LinearSVC()\n",
    "cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=2, random_state=0)\n",
    "grid = dict()\n",
    "#grid['C'] = [1,5, 10, 50, 100, 500, 1000]\n",
    "grid['C'] = np.linspace(0.0, 5, 20)\n",
    "#grid['gamma'] = [1,0.1,0.001,0.0001]\n",
    "#grid['kernel'] = ['linear','rbf']\n",
    "#RandomizedSearchCV GridSearchCV\n",
    "search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# X_norm\n",
    "# spatial_features_norm\n",
    "# X_norm_with_spatial_features_norm\n",
    "results = search.fit(X_norm, label.values)\n",
    "print('metric: %.3f' % results.best_score_)\n",
    "print('model_params=',results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "np.random.seed(0)\n",
    "seed=True\n",
    "#C=1.0\n",
    "\n",
    "#model_params={'C': 5}\n",
    "#_=repeated_tests(LinearSVC, X_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#model_params= {'C': 1}\n",
    "#_=repeated_tests(LinearSVC, spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#model_params= {'C': 1}\n",
    "#_=repeated_tests(LinearSVC, X_norm_with_spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "model_params= {'C': 1}\n",
    "#_=repeated_tests(LinearSVC, X_norm_with_spatial_features_norm_MRMR_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "_=repeated_tests(LinearSVC, X_norm_mrmr_with_spatial_features_norm_mrmr, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#model_params= {'C': 1}\n",
    "#_=repeated_tests(LinearSVC, X_norm_with_spatial_features_norm_PCA_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: 0.844\n",
      "model_params= {'subsample': 1.0, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 4, 'gamma': 0.5, 'colsample_bytree': 0.6}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import numpy as np\n",
    "#,'max_depth':[2,4,6,8,10]\n",
    "model = XGBClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=2, random_state=0)\n",
    "grid = dict()\n",
    "grid['n_estimators'] = np.arange(50, 500, 50).tolist()\n",
    "grid['min_child_weight'] = [1, 5, 10]\n",
    "grid['gamma'] = [0.5, 1, 1.5, 2, 5]\n",
    "grid['subsample'] = [0.6, 0.8, 1.0]\n",
    "grid['colsample_bytree'] = [0.6, 0.8, 1.0]\n",
    "grid['max_depth'] = [3, 4, 5]\n",
    "#RandomizedSearchCV GridSearchCV\n",
    "search = RandomizedSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "results = search.fit(X_norm_mrmr_with_spatial_features_norm_mrmr, label.values)\n",
    "print('metric: %.3f' % results.best_score_)\n",
    "print('model_params=',results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "#from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "\n",
    "np.random.seed(0)\n",
    "seed=True\n",
    "#model_params={'n_estimators':400, 'learning_rate': 0.1}\n",
    "#model_params=None\n",
    "\n",
    "#model_params={'subsample': 0.6, 'n_estimators': 150, 'min_child_weight': 1, 'max_depth': 4, 'gamma': 0.5, 'colsample_bytree': 1.0}\n",
    "#repeated_tests(XGBClassifier, X_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#model_params= {'subsample': 0.8, 'n_estimators': 450, 'min_child_weight': 10, 'max_depth': 4, 'gamma': 0.5, 'colsample_bytree': 0.8}\n",
    "#repeated_tests(XGBClassifier, spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#model_params= {'subsample': 1.0, 'n_estimators': 250, 'min_child_weight': 10, 'max_depth': 4, 'gamma': 1, 'colsample_bytree': 1.0}\n",
    "#repeated_tests(XGBClassifier, X_norm_with_spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "model_params= {'subsample': 1.0, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 4, 'gamma': 0.5, 'colsample_bytree': 0.6}\n",
    "#repeated_tests(XGBClassifier, X_norm_with_spatial_features_norm_MRMR_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "repeated_tests(XGBClassifier, X_norm_mrmr_with_spatial_features_norm_mrmr, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#model_params={'subsample': 0.6, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 3, 'gamma': 1, 'colsample_bytree': 0.8}\n",
    "#precisions, recalls, accuracies =repeated_tests(XGBClassifier, X_norm_with_spatial_features_norm_PCA_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "section PLS-DA\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "np.random.seed(0)\n",
    "seed=True\n",
    "#_=repeated_tests(LinearDiscriminantAnalysis, X_norm, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "\n",
    "#_=repeated_tests(LinearDiscriminantAnalysis, spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "#_=repeated_tests(LinearDiscriminantAnalysis, X_norm_with_spatial_features_norm_no_bsif, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "#_=repeated_tests(LinearDiscriminantAnalysis, bsif_features_norm, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "\n",
    "#_=repeated_tests(LinearDiscriminantAnalysis, X_norm_with_spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "#_=repeated_tests(LinearDiscriminantAnalysis, X_norm_with_spatial_features_norm_hara_lbp_hu, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "\n",
    "#_=repeated_tests(LinearDiscriminantAnalysis, X_norm_with_spatial_features_norm_MRMR_norm, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "_=repeated_tests(LinearDiscriminantAnalysis, X_norm_mrmr_with_spatial_features_norm_mrmr, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "\n",
    "#_=repeated_tests(LinearDiscriminantAnalysis, X_norm_with_spatial_features_norm_PCA_norm, label, split_ratio=0.3, test_num=100, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 18.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start show results\n",
      "(420, 69) (294, 69) (126, 69) \n",
      "\n",
      "precision:\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.797211\n",
      "std      0.042206\n",
      "min      0.698630\n",
      "25%      0.771429\n",
      "50%      0.794118\n",
      "75%      0.826462\n",
      "max      0.919355 \n",
      "\n",
      "recall:\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.858254\n",
      "std      0.049647\n",
      "min      0.730159\n",
      "25%      0.825397\n",
      "50%      0.857143\n",
      "75%      0.904762\n",
      "max      0.968254 \n",
      "\n",
      "accuracy:\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.818175\n",
      "std      0.031120\n",
      "min      0.730159\n",
      "25%      0.799603\n",
      "50%      0.825397\n",
      "75%      0.841270\n",
      "max      0.912698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "np.random.seed(0)\n",
    "seed=True\n",
    "#_=repeated_tests(QuadraticDiscriminantAnalysis, X_norm, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "#_=repeated_tests(QuadraticDiscriminantAnalysis, spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "#_=repeated_tests(QuadraticDiscriminantAnalysis, X_norm_with_spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "#_=repeated_tests(QuadraticDiscriminantAnalysis, X_norm_with_spatial_features_norm_MRMR_norm, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "_=repeated_tests(QuadraticDiscriminantAnalysis, X_norm_mrmr_with_spatial_features_norm_mrmr, label, split_ratio=0.3, test_num=100, seed=seed)\n",
    "#_=repeated_tests(QuadraticDiscriminantAnalysis, X_norm_with_spatial_features_norm_PCA_norm, label, split_ratio=0.3, test_num=100, seed=seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE / GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,RepeatedStratifiedKFold,GridSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import ConfusionMatrixDisplay,precision_score,recall_score,confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((210, 462), (252, 462), (176, 462), (76, 462))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversample = SMOTE()\n",
    "X_smote, y_smote = oversample.fit_resample(X, y)\n",
    "random_state = 42\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(X_smote, y_smote, test_size=0.30, random_state=random_state, shuffle=True, stratify=y_smote)\n",
    "X.shape, X_smote.shape, Xs_train.shape, Xs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test\n",
    "oversample = SMOTE()\n",
    "Xs_train, ys_train = oversample.fit_resample(X_train, y_train)\n",
    "Xs_test, ys_test = X_test, y_test\n",
    "X.shape, X_train.shape, Xs_train.shape, Xs_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repeated_tests_with_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_tests_with_SMOTE(model_class, X, label, split_ratio=0.3, test_num=30, model_params=None):\n",
    "    from tqdm import tqdm\n",
    "    accuracies = []\n",
    "    for i_test in tqdm(range(test_num)):\n",
    "        np.random.seed(int(time.time()))\n",
    "        random_state = np.random.randint(0, 1e4)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, label, test_size=split_ratio, random_state=random_state, shuffle=True, stratify=label)\n",
    "        oversample = SMOTE()\n",
    "        Xs_train, ys_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "        ys_train = ys_train.squeeze()\n",
    "        if model_params is None:\n",
    "            model = model_class()\n",
    "        else:\n",
    "            model = model_class(**model_params)\n",
    "\n",
    "        model.fit(Xs_train, ys_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        precision = precision_score(y_test, predictions)\n",
    "        recall = recall_score(y_test, predictions)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "        diffs = np.where(predictions!=y_test.values.squeeze())\n",
    "        accuracy = 1-len(diffs[0])/len(y_test)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print(X.shape, X_train.shape, X_test.shape)\n",
    "    precisions = np.array(precisions)\n",
    "    df_describe = pd.DataFrame(precisions)\n",
    "    print(df_describe.describe())\n",
    "    recalls = np.array(recalls)\n",
    "    df_describe = pd.DataFrame(recalls)\n",
    "    print(df_describe.describe())\n",
    "\n",
    "    accuracies = np.array(accuracies)\n",
    "    df_describe = pd.DataFrame(accuracies)\n",
    "    print(df_describe.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((420, 462), (294, 2964), (294, 2964), (126, 2964), (126, 1))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X\n",
    "# X_norm\n",
    "# spatial_features_norm\n",
    "# X_norm_with_spatial_features_norm\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm_with_spatial_features_norm, label, test_size=0.30, random_state=42, shuffle=True, stratify=label)\n",
    "oversample = SMOTE()\n",
    "Xs_train, ys_train = oversample.fit_resample(X_train, y_train)\n",
    "Xs_test, ys_test = X_test, y_test\n",
    "X.shape, X_train.shape, Xs_train.shape, Xs_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AFSALab\\.conda\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: 0.845\n",
      "model_params= {'tol': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=2, random_state=0)\n",
    "\n",
    "grid = dict()\n",
    "#grid['solver'] = ['eigen','lsqr', None]\n",
    "# LinearDiscriminantAnalysis\n",
    "# shrinkages = np.linspace(0.000001, 0.00001, 10).tolist()\n",
    "\n",
    "grid['tol'] = np.linspace(1e-5, 1e-3, 21).tolist()\n",
    "\n",
    "# grid['shrinkage'] = ['auto'] + shrinkages\n",
    "#RandomizedSearchCV GridSearchCV\n",
    "search = GridSearchCV(LDA, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "#search = GridSearchCV(LDA, grid, scoring='precision', cv=cv, n_jobs=-1)\n",
    "#results = search.fit(Xs_train, ys_train)\n",
    "\n",
    "# X_norm\n",
    "# spatial_features_norm\n",
    "# X_norm_with_spatial_features_norm\n",
    "results = search.fit(X_norm_with_spatial_features_norm, label.values)\n",
    "print('metric: %.3f' % results.best_score_)\n",
    "print('model_params=',results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AFSALab\\.conda\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "c:\\Users\\AFSALab\\.conda\\envs\\pytorch\\lib\\site-packages\\sklearn\\discriminant_analysis.py:926: UserWarning:\n",
      "\n",
      "Variables are collinear\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric: 0.492\n",
      "model_params= {'tol': 1e-06}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "QDA = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=2, random_state=0)\n",
    "\n",
    "grid = dict()\n",
    "#grid['solver'] = ['eigen','lsqr', None]\n",
    "# LinearDiscriminantAnalysis\n",
    "# shrinkages = np.linspace(0.000001, 0.00001, 10).tolist()\n",
    "\n",
    "grid['tol'] = np.linspace(1e-6, 1e-5, 21).tolist()\n",
    "\n",
    "# grid['shrinkage'] = ['auto'] + shrinkages\n",
    "#RandomizedSearchCV GridSearchCV\n",
    "search = GridSearchCV(QDA, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# X_norm\n",
    "# spatial_features_norm\n",
    "# X_norm_with_spatial_features_norm\n",
    "results = search.fit(spatial_features_norm, label.values)\n",
    "print('metric: %.3f' % results.best_score_)\n",
    "print('model_params=',results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_final=LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')\n",
    "LDA_final.fit_transform(Xs_train, ys_train)\n",
    "Xs_test_predictions=LDA_final.predict(Xs_test)\n",
    "\n",
    "#ConfusionMatrixDisplay.from_predictions(y_test, X_test_predictions)\n",
    "ConfusionMatrixDisplay.from_predictions(ys_test, Xs_test_predictions)\n",
    "plt.show()\n",
    "\n",
    "if type(y_test)==pd.DataFrame:\n",
    "    tn, fp, fn, tp = confusion_matrix(list(y_test.values), list(Xs_test_predictions), labels=[0, 1]).ravel()\n",
    "else:\n",
    "    tn, fp, fn, tp = confusion_matrix(list(ys_test), list(Xs_test_predictions), labels=[0, 1]).ravel()\n",
    "\n",
    "print('True Positive :', tp)\n",
    "print('True Negative :', tn)\n",
    "print('False Positive :', fp)\n",
    "print('False Negative :', fn)\n",
    "if type(y_test)==pd.DataFrame:\n",
    "    print(\"Precision score\",np.round(precision_score(y_test.values, Xs_test_predictions),3))\n",
    "else:\n",
    "    print(\"Precision score\",np.round(precision_score(ys_test, Xs_test_predictions),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params ={'shrinkage':'auto', 'solver':'eigen'}\n",
    "repeated_tests(LinearDiscriminantAnalysis, X, label, split_ratio=0.3, test_num=100, model_params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "seed=True\n",
    "#model_params ={'shrinkage': 5e-06, 'solver':'eigen'}\n",
    "##repeated_tests(LinearDiscriminantAnalysis, X, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "#repeated_tests(LinearDiscriminantAnalysis, X_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#model_params ={'shrinkage': 'auto', 'solver':'eigen'}\n",
    "#repeated_tests(LinearDiscriminantAnalysis, spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#model_params ={'shrinkage': 'auto', 'solver':'eigen'}\n",
    "#repeated_tests(LinearDiscriminantAnalysis, X_norm_with_spatial_features_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#model_params ={'solver': 'eigen', 'shrinkage': 'auto'}\n",
    "#_=repeated_tests(LinearDiscriminantAnalysis, X_norm_with_spatial_features_norm_MRMR_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "model_params ={'solver': 'eigen', 'shrinkage': 1e-06}\n",
    "_=repeated_tests(LinearDiscriminantAnalysis, X_norm_mrmr_with_spatial_features_norm_mrmr, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#model_params ={'shrinkage': 'auto', 'solver':'eigen'}\n",
    "#_=repeated_tests(LinearDiscriminantAnalysis, X_norm_with_spatial_features_norm_PCA_norm, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "\n",
    "#model_params= {'solver': 'eigen', 'shrinkage': 3e-06}\n",
    "#_=repeated_tests(LinearDiscriminantAnalysis, X_norm_mrmr, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)\n",
    "#model_params= {'solver': 'eigen', 'shrinkage': 'auto'}\n",
    "#_=repeated_tests(LinearDiscriminantAnalysis, spatial_features_norm_MRMR, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params ={'shrinkage':'auto', 'solver':'eigen'}\n",
    "repeated_tests_with_SMOTE(LinearDiscriminantAnalysis, X, label, split_ratio=0.3, test_num=100, model_params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_tests_with_SMOTE(LinearDiscriminantAnalysis, X, label, split_ratio=0.3, test_num=100, model_params=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rLDA + spatial features (load data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,cross_val_score,RepeatedStratifiedKFold,GridSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import ConfusionMatrixDisplay,accuracy_score,precision_score,recall_score,confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling_spectral_data import load_im_dirs\n",
    "im_dirs = load_im_dirs()\n",
    "len(im_dirs)\n",
    "\n",
    "from modeling_spectral_data import load_spectral_mean_data\n",
    "hyper_dict_good_tot, hyper_dict_bad_tot = load_spectral_mean_data(use_part_1=1, use_part_2=1)\n",
    "len(hyper_dict_good_tot)\n",
    "\n",
    "from modeling_spectral_data import spli_mean_spectral_data\n",
    "hyper_dict_good_tot_0, hyper_dict_good_tot_1, hyper_dict_bad_tot_0, hyper_dict_bad_tot_1 = spli_mean_spectral_data(hyper_dict_good_tot, hyper_dict_bad_tot)\n",
    "len(hyper_dict_good_tot_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#band_list = [600, 750, 900]\n",
    "#band_list = [812.8,\n",
    "# 808.75,\n",
    "# 811.44,\n",
    "# 413.52,\n",
    "# 410.88,\n",
    "# 412.2,\n",
    "# 712.06,\n",
    "# 713.4,\n",
    "# 710.72,\n",
    "# 803.36,\n",
    "# 804.71,\n",
    "# 800.68,\n",
    "# 690.66,\n",
    "# 692.0,\n",
    "# 689.32,\n",
    "# 625.26,\n",
    "# 622.6,\n",
    "# 623.93,\n",
    "# 635.92,\n",
    "# 634.59,\n",
    "# 633.26]\n",
    "\n",
    "# good\n",
    "band_list = [413.52, 599.97, 712.06, 736.18, 812.8, 911.41]\n",
    "# worsen\n",
    "#band_list = [599.97, 736.18, 911.41]\n",
    "#band_list = [413.52, 599.97, 712.06, 812.8, 911.41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "generate data\n",
    "\"\"\"\n",
    "\n",
    "from modeling_CNN_blueberry import HyperData\n",
    "from tqdm import tqdm\n",
    "\n",
    "good_dataset = HyperData(save_dir_im_data_good, band_list, 0, hyper_dict_good_tot_0, wavelengths)\n",
    "bad_dataset = HyperData(save_dir_im_data_bad, band_list, 1, hyper_dict_bad_tot_0, wavelengths)\n",
    "good_dataset1 = HyperData(save_dir_im_data_good1, band_list, 0, hyper_dict_good_tot_1, wavelengths)\n",
    "bad_dataset1 = HyperData(save_dir_im_data_bad1, band_list, 1, hyper_dict_bad_tot_1, wavelengths)\n",
    "\n",
    "all_spatial_features = []\n",
    "all_mean_spectral_features = []\n",
    "for i_sample, im_dir in tqdm(enumerate(im_dirs)):\n",
    "    if i_sample < len(good_dataset):\n",
    "        dataset = good_dataset\n",
    "        sample_idx = i_sample\n",
    "    elif i_sample < len(good_dataset) + len(good_dataset1):\n",
    "        dataset = good_dataset1\n",
    "        sample_idx = i_sample - len(good_dataset)\n",
    "    elif i_sample < len(good_dataset) + len(good_dataset1) + len(bad_dataset):\n",
    "        dataset = bad_dataset\n",
    "        sample_idx = i_sample -len(good_dataset) - len(good_dataset1)\n",
    "    else:\n",
    "        dataset = bad_dataset1\n",
    "        sample_idx = i_sample - len(good_dataset) - len(good_dataset1) - len(bad_dataset)\n",
    "    imgs, labels, spectral_features, spatial_features, sample_dir, spatial_features_num_dict = dataset.__getitem__(sample_idx)\n",
    "    all_spatial_features.append(spatial_features)\n",
    "    all_mean_spectral_features.append(spectral_features)\n",
    "all_spatial_features = np.array(all_spatial_features)\n",
    "all_mean_spectral_features = np.array(all_mean_spectral_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_spatial_features.shape)\n",
    "print(spatial_features_num_dict)\n",
    "print('feature number per waveband:')\n",
    "for key in spatial_features_num_dict:\n",
    "    if key != 'order':\n",
    "        print(key, spatial_features_num_dict[key]/len(band_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_spatial_features all_spatial_features_21bands all_spatial_features_3bands all_spatial_features_5bands\n",
    "save_path_all_spatial_features = r'D:\\Dataset\\HyperspectralBlueberry\\mean_data_python\\all_spatial_features_6bands.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(save_path_all_spatial_features, all_spatial_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 2502)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_spatial_features = np.load(save_path_all_spatial_features, allow_pickle='TRUE')\n",
    "all_spatial_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.1229,   0.545 , 698.3388,   0.545 ,   0.7853,   5.3088,\n",
       "          3.1833,   2.1255,   2.5266,   0.0533]),\n",
       " array([  0.0634,   0.1455, 541.0698,   0.1455,   0.0425,   1.0179,\n",
       "          0.7727,   0.2713,   0.3602,   0.0078]),\n",
       " array([ 0., -0., -0., -0.,  0., -0., -0., -0., -0., -0.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial_feature_means = all_spatial_features.mean(axis=0)\n",
    "spatial_feature_stds = all_spatial_features.std(axis=0)\n",
    "\n",
    "\"\"\"\n",
    "spatial feature norm\n",
    "\"\"\"\n",
    "all_spatial_features_norm = (all_spatial_features - spatial_feature_means) / spatial_feature_stds\n",
    "all_spatial_features.mean(0)[:10], all_spatial_features.std(0)[:10], all_spatial_features_norm.mean(0)[:10], all_spatial_features_norm.std(0)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((420, 462),\n",
       " (420, 1),\n",
       " array([0.1516, 0.1501, 0.1481, 0.1467, 0.1463]),\n",
       " array([0.0411, 0.0426, 0.0439, 0.0451, 0.0462]),\n",
       " array([-0., -0., -0., -0.,  0.]),\n",
       " array([1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling_spectral_data import load_spectral_mean_data_xy\n",
    "x, y = load_spectral_mean_data_xy(use_part_1=1, use_part_2=1)\n",
    "X =  pd.DataFrame(x)\n",
    "label = pd.DataFrame(y, columns=['label'])\n",
    "#label = label.values.ravel()\n",
    "\n",
    "X = x\n",
    "X_norm = (x-x.mean(0))/x.std(0)\n",
    "\n",
    "X.shape, label.shape, x.mean(0)[:5], x.std(0)[:5], X_norm.mean(0)[:5], X_norm.std(0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spatial_features_num_dict={'haralick_features': 39,\n",
    "# 'lbp_features': 78,\n",
    "# 'hu_moments_features': 21,\n",
    "# 'gabor_features': 300,\n",
    "# 'bsif_features': 300,\n",
    "# 'order': ['haralick_features',\n",
    "#  'lbp_features',\n",
    "#  'hu_moments_features',\n",
    "#  'gabor_features',\n",
    "#  'bsif_features']}\n",
    "\n",
    "#spatial_features_num_dict={'haralick_features': 273,\n",
    "# 'lbp_features': 546,\n",
    "# 'hu_moments_features': 147,\n",
    "# 'gabor_features': 2100,\n",
    "# 'bsif_features': 2100,\n",
    "# 'order': ['haralick_features',\n",
    "#  'lbp_features',\n",
    "#  'hu_moments_features',\n",
    "#  'gabor_features',\n",
    "#  'bsif_features']}\n",
    "\n",
    "#spatial_features_num_dict={'haralick_features': 78,\n",
    "# 'lbp_features': 156,\n",
    "# 'hu_moments_features': 42,\n",
    "# 'gabor_features': 600,\n",
    "# 'bsif_features': 600,\n",
    "# 'order': ['haralick_features',\n",
    "#  'lbp_features',\n",
    "#  'hu_moments_features',\n",
    "#  'gabor_features',\n",
    "#  'bsif_features']}\n",
    "\n",
    "spatial_features_num_dict={\n",
    "    'haralick_features': 168, 'lbp_features': 354, 'hu_moments_features': 42, 'gabor_features': 402, 'bsif_features': 1536,\n",
    "    'order': ['haralick_features', 'lbp_features', 'hu_moments_features', 'gabor_features', 'bsif_features']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 2502)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "average\n",
    "\"\"\"\n",
    "im_num_each_sample = len(band_list)\n",
    "haralick_num = spatial_features_num_dict['haralick_features']\n",
    "lbp_num = spatial_features_num_dict['lbp_features']\n",
    "hu_num = spatial_features_num_dict['hu_moments_features']\n",
    "gabor_num = spatial_features_num_dict['gabor_features']\n",
    "bsif_num = spatial_features_num_dict['bsif_features']\n",
    "# haralick_num + lbp_num + hu_num + gabor_num + bsif_num\n",
    "start = 0\n",
    "\n",
    "spatial_features_order = spatial_features_num_dict['order']\n",
    "\n",
    "haralick_features = all_spatial_features[:, start: start+haralick_num]\n",
    "haralick_features_mean = haralick_features.reshape(-1, haralick_num//im_num_each_sample, im_num_each_sample).mean(2)\n",
    "\n",
    "lbp_features = all_spatial_features[:, start+haralick_num: start+haralick_num+lbp_num]\n",
    "lbp_features_mean = lbp_features.reshape(-1, lbp_num//im_num_each_sample, im_num_each_sample).mean(2)\n",
    "\n",
    "hu_moments_features = all_spatial_features[:, start+haralick_num+lbp_num: start+haralick_num+lbp_num+hu_num]\n",
    "hu_moments_features_mean = hu_moments_features.reshape(-1, hu_num//im_num_each_sample, im_num_each_sample).mean(2)\n",
    "\n",
    "gabor_features = all_spatial_features[:, start+haralick_num+lbp_num+hu_num: start+haralick_num+lbp_num+hu_num+gabor_num]\n",
    "gabor_features_mean = gabor_features.reshape(-1, gabor_num//im_num_each_sample, im_num_each_sample).mean(2)\n",
    "\n",
    "bsif_features = all_spatial_features[:, start+haralick_num+lbp_num+hu_num+gabor_num: start+haralick_num+lbp_num+hu_num+gabor_num+bsif_num]\n",
    "bsif_features_mean = bsif_features.reshape(-1, bsif_num//im_num_each_sample, im_num_each_sample).mean(2)\n",
    "\n",
    "haralick_features_norm =  (haralick_features - haralick_features.mean(axis=0))/haralick_features.std(axis=0)\n",
    "lbp_features_norm = (lbp_features - lbp_features.mean(axis=0))/lbp_features.std(axis=0)\n",
    "hu_moments_features_norm =  (hu_moments_features - hu_moments_features.mean(axis=0))/hu_moments_features.std(axis=0)\n",
    "gabor_features_norm =  (gabor_features - gabor_features.mean(axis=0))/gabor_features.std(axis=0)\n",
    "bsif_features_norm =  (bsif_features - bsif_features.mean(axis=0))/bsif_features.std(axis=0)\n",
    "\n",
    "haralick_features_norm_mean =  (haralick_features_mean - haralick_features_mean.mean(axis=0))/haralick_features_mean.std(axis=0)\n",
    "lbp_features_norm_mean = (lbp_features_mean - lbp_features_mean.mean(axis=0))/lbp_features_mean.std(axis=0)\n",
    "hu_moments_features_norm_mean =  (hu_moments_features_mean - hu_moments_features_mean.mean(axis=0))/hu_moments_features_mean.std(axis=0)\n",
    "gabor_features_norm_mean =  (gabor_features_mean - gabor_features_mean.mean(axis=0))/gabor_features_mean.std(axis=0)\n",
    "bsif_features_norm_mean =  (bsif_features_mean - bsif_features_mean.mean(axis=0))/bsif_features_mean.std(axis=0)\n",
    "\n",
    "# best combination? [haralick_features, lbp_features, hu_moments_features]\n",
    "spatial_features = np.concatenate([haralick_features, lbp_features, hu_moments_features, gabor_features, bsif_features], axis=1)\n",
    "spatial_features_norm = np.concatenate([haralick_features_norm, lbp_features_norm, hu_moments_features_norm, gabor_features_norm, bsif_features_norm], axis=1)\n",
    "spatial_features_norm_no_bsif = np.concatenate([haralick_features_norm, lbp_features_norm, hu_moments_features_norm, gabor_features_norm], axis=1)\n",
    "\n",
    "spatial_features_mean = np.concatenate([haralick_features_mean, lbp_features_mean, hu_moments_features_mean, gabor_features_mean, bsif_features_mean], axis=1)\n",
    "spatial_features_norm_mean = np.concatenate([haralick_features_norm_mean, lbp_features_norm_mean, hu_moments_features_norm_mean, gabor_features_norm_mean, bsif_features_norm_mean], axis=1)\n",
    "spatial_features_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 2964)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_with_spatial_features = np.concatenate([X, spatial_features], axis=1)\n",
    "X_norm_with_spatial_features_norm = np.concatenate([X_norm, spatial_features_norm], axis=1)\n",
    "X_norm_with_spatial_features_norm_no_bsif = np.concatenate([X_norm, spatial_features_norm_no_bsif], axis=1)\n",
    "X_norm_with_spatial_features_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io\n",
    "# scipy.io.savemat(r'D:\\BoyangDeng\\BlueberryClassification\\datasets\\mean_data_python\\label.mat', {'label': np.squeeze(label.values)})\n",
    "# scipy.io.savemat(r'D:\\BoyangDeng\\BlueberryClassification\\datasets\\mean_data_python\\spectral_features.mat', {'X_norm': X_norm, 'X': X})\n",
    "# scipy.io.savemat(r'D:\\BoyangDeng\\BlueberryClassification\\datasets\\mean_data_python\\spatial_features_norm_6bands.mat', {'spatial_features_norm': spatial_features_norm, 'spatial_features': spatial_features})\n",
    "# scipy.io.savemat(r'D:\\BoyangDeng\\BlueberryClassification\\datasets\\mean_data_python\\spectral_features_with_spatial_features_norm_6bands.mat', {'X_norm_with_spatial_features_norm': X_norm_with_spatial_features_norm, 'X_with_spatial_features': X_with_spatial_features})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRMR\n",
    "relative to feature extraction -> MRMR section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "spectral_features_mrmr_idxes=scipy.io.loadmat(r'D:\\BoyangDeng\\test\\spectral_features_mrmr_idxes.mat')\n",
    "spectral_features_mrmr_scores=scipy.io.loadmat(r'D:\\BoyangDeng\\test\\spectral_features_mrmr_scores.mat')\n",
    "spectral_features_mrmr_idxes=spectral_features_mrmr_idxes['idx'][0]\n",
    "spectral_features_mrmr_scores=spectral_features_mrmr_scores['score'][0]\n",
    "\n",
    "haralick_features_mrmr_idxes=scipy.io.loadmat(r'D:\\BoyangDeng\\test\\haralick_features_mrmr_idxes.mat')\n",
    "haralick_features_mrmr_scores=scipy.io.loadmat(r'D:\\BoyangDeng\\test\\haralick_features_mrmr_scores.mat')\n",
    "haralick_features_mrmr_idxes=haralick_features_mrmr_idxes['idx_1'][0]\n",
    "haralick_features_mrmr_scores=haralick_features_mrmr_scores['score_1'][0]\n",
    "\n",
    "lbp_features_mrmr_idxes=scipy.io.loadmat(r'D:\\BoyangDeng\\test\\lbp_features_mrmr_idxes.mat')\n",
    "lbp_features_mrmr_scores=scipy.io.loadmat(r'D:\\BoyangDeng\\test\\lbp_features_mrmr_scores.mat')\n",
    "lbp_features_mrmr_idxes=lbp_features_mrmr_idxes['idx_2'][0]\n",
    "lbp_features_mrmr_scores=lbp_features_mrmr_scores['score_2'][0]\n",
    "\n",
    "hu_features_mrmr_idxes=scipy.io.loadmat(r'D:\\BoyangDeng\\test\\hu_features_mrmr_idxes.mat')\n",
    "hu_features_mrmr_scores=scipy.io.loadmat(r'D:\\BoyangDeng\\test\\hu_features_mrmr_scores.mat')\n",
    "hu_features_mrmr_idxes=hu_features_mrmr_idxes['idx_3'][0]\n",
    "hu_features_mrmr_scores=hu_features_mrmr_scores['score_3'][0]\n",
    "\n",
    "gabor_features_mrmr_idxes=scipy.io.loadmat(r'D:\\BoyangDeng\\test\\gabor_features_mrmr_idxes.mat')\n",
    "gabor_features_mrmr_scores=scipy.io.loadmat(r'D:\\BoyangDeng\\test\\gabor_features_mrmr_scores.mat')\n",
    "gabor_features_mrmr_idxes=gabor_features_mrmr_idxes['idx_4'][0]\n",
    "gabor_features_mrmr_scores=gabor_features_mrmr_scores['score_4'][0]\n",
    "\n",
    "bsif_features_mrmr_idxes=scipy.io.loadmat(r'D:\\BoyangDeng\\test\\bsif_features_mrmr_idxes.mat')\n",
    "bsif_features_mrmr_scores=scipy.io.loadmat(r'D:\\BoyangDeng\\test\\bsif_features_mrmr_scores.mat')\n",
    "bsif_features_mrmr_idxes=bsif_features_mrmr_idxes['idx_5'][0]\n",
    "bsif_features_mrmr_scores=bsif_features_mrmr_scores['score_5'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0\n",
      "count  1536.000000\n",
      "mean      0.013512\n",
      "std       0.017470\n",
      "min       0.000000\n",
      "25%       0.003486\n",
      "50%       0.008422\n",
      "75%       0.016159\n",
      "max       0.158046\n"
     ]
    }
   ],
   "source": [
    "show_statistics(bsif_features_mrmr_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33,)\n",
      "(5,)\n",
      "(3,)\n",
      "(1,)\n",
      "(1,)\n",
      "(26,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "score_t=0.05\n",
    "#score_t=0.005\n",
    "valid_spectral_features_mrmr_idxes = spectral_features_mrmr_idxes[:np.sum(spectral_features_mrmr_scores>score_t)]-1\n",
    "\n",
    "valid_haralick_features_mrmr_idxes = haralick_features_mrmr_idxes[:np.sum(haralick_features_mrmr_scores>score_t)]-1\n",
    "valid_lbp_features_mrmr_idxes = lbp_features_mrmr_idxes[:np.sum(lbp_features_mrmr_scores>score_t)]-1\n",
    "valid_hu_features_mrmr_idxes = hu_features_mrmr_idxes[:np.sum(hu_features_mrmr_scores>score_t)]-1\n",
    "valid_gabor_features_mrmr_idxes = gabor_features_mrmr_idxes[:np.sum(gabor_features_mrmr_scores>score_t)]-1\n",
    "\n",
    "score_t=0.05\n",
    "#score_t=0.1\n",
    "valid_bsif_features_mrmr_idxes = bsif_features_mrmr_idxes[:np.sum(bsif_features_mrmr_scores>score_t)]-1\n",
    "print(valid_spectral_features_mrmr_idxes.shape)\n",
    "print(valid_haralick_features_mrmr_idxes.shape)\n",
    "print(valid_lbp_features_mrmr_idxes.shape)\n",
    "print(valid_hu_features_mrmr_idxes.shape)\n",
    "print(valid_gabor_features_mrmr_idxes.shape)\n",
    "print(valid_bsif_features_mrmr_idxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((420, 36), (420, 69))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mrmr = X[:, valid_spectral_features_mrmr_idxes]\n",
    "\n",
    "haralick_features_mrmr = haralick_features[:, valid_haralick_features_mrmr_idxes]\n",
    "lbp_features_mrmr = lbp_features[:, valid_lbp_features_mrmr_idxes]\n",
    "hu_moments_features_mrmr = hu_moments_features[:, valid_hu_features_mrmr_idxes]\n",
    "gabor_features_mrmr = gabor_features[:, valid_gabor_features_mrmr_idxes]\n",
    "bsif_features_mrmr = bsif_features[:, valid_bsif_features_mrmr_idxes]\n",
    "\n",
    "X_norm_mrmr = X_norm[:, valid_spectral_features_mrmr_idxes]\n",
    "\n",
    "haralick_features_norm_mrmr = haralick_features_norm[:, valid_haralick_features_mrmr_idxes]\n",
    "lbp_features_norm_mrmr = lbp_features_norm[:, valid_lbp_features_mrmr_idxes]\n",
    "hu_moments_features_norm_mrmr = hu_moments_features_norm[:, valid_hu_features_mrmr_idxes]\n",
    "gabor_features_norm_mrmr = gabor_features_norm[:, valid_gabor_features_mrmr_idxes]\n",
    "bsif_features_norm_mrmr = bsif_features_norm[:, valid_bsif_features_mrmr_idxes]\n",
    "\n",
    "spatial_features_mrmr = np.concatenate([haralick_features_mrmr, lbp_features_mrmr, hu_moments_features_mrmr, gabor_features_mrmr, bsif_features_mrmr], axis=1)\n",
    "spatial_features_norm_mrmr = np.concatenate([haralick_features_norm_mrmr, lbp_features_norm_mrmr, hu_moments_features_norm_mrmr, gabor_features_norm_mrmr, bsif_features_norm_mrmr], axis=1)\n",
    "\n",
    "X_mrmr_with_spatial_features_mrmr = np.concatenate([X_mrmr, spatial_features_mrmr], axis=1)\n",
    "X_norm_mrmr_with_spatial_features_norm_mrmr = np.concatenate([X_norm_mrmr, spatial_features_norm_mrmr], axis=1)\n",
    "\n",
    "spatial_features_norm_mrmr.shape, X_norm_mrmr_with_spatial_features_norm_mrmr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rLDA_model_params_grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversample = SMOTE()\n",
    "#X_smote, y_smote = oversample.fit_resample(X_with_spatial_features, y)\n",
    "#Xs_train, Xs_test, ys_train, ys_test = train_test_split(X_smote, y_smote, test_size=0.30, random_state=42, shuffle=True, stratify=y_smote)\n",
    "#X_with_spatial_features.shape, X_smote.shape, Xs_train.shape, Xs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((420, 462), (294, 2964), (294, 2964), (126, 2964))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_with_spatial_features, label, test_size=0.30, random_state=42, shuffle=True, stratify=label)\n",
    "oversample = SMOTE()\n",
    "Xs_train, ys_train = oversample.fit_resample(X_train, y_train)\n",
    "#Xs_train, ys_train = X_train, y_train\n",
    "Xs_test, ys_test = X_test, y_test\n",
    "X.shape, X_train.shape, Xs_train.shape, Xs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Software\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive : 47\n",
      "True Negative : 52\n",
      "False Positive : 11\n",
      "False Negative : 16\n",
      "Precision score 0.81\n"
     ]
    }
   ],
   "source": [
    "LDA_final=LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')\n",
    "LDA_final.fit_transform(Xs_train, ys_train)\n",
    "Xs_test_predictions=LDA_final.predict(Xs_test)\n",
    "\n",
    "#ConfusionMatrixDisplay.from_predictions(y_test, X_test_predictions)\n",
    "ConfusionMatrixDisplay.from_predictions(ys_test, Xs_test_predictions)\n",
    "\n",
    "if type(y_test)==pd.DataFrame:\n",
    "    tn, fp, fn, tp = confusion_matrix(list(y_test.values), list(Xs_test_predictions), labels=[0, 1]).ravel()\n",
    "else:\n",
    "    tn, fp, fn, tp = confusion_matrix(list(ys_test), list(Xs_test_predictions), labels=[0, 1]).ravel()\n",
    "\n",
    "print('True Positive :', tp)\n",
    "print('True Negative :', tn)\n",
    "print('False Positive :', fp)\n",
    "print('False Negative :', fn)\n",
    "if type(y_test)==pd.DataFrame:\n",
    "    print(\"Precision score\",np.round(precision_score(y_test.values, Xs_test_predictions),3))\n",
    "else:\n",
    "    print(\"Precision score\",np.round(precision_score(ys_test, Xs_test_predictions),3))\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "best_score = 0.0\n",
    "best_params = None\n",
    "\n",
    "def rLDA_model_params_grid_search(used_feature, label, shrinkage_range, min_gap=1e-5, log_path=None, random_state=None):\n",
    "    #global best_score, best_params\n",
    "    LDA = LinearDiscriminantAnalysis()\n",
    "\n",
    "    cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=2, random_state=random_state)\n",
    "\n",
    "    grid = dict()\n",
    "    grid['solver'] = ['eigen']\n",
    "    shrinkages = np.linspace(shrinkage_range[0], shrinkage_range[1], shrinkage_range[2]).tolist()\n",
    "    #shrinkages = np.linspace(0.0, 1.0, 21).tolist()\n",
    "    #shrinkages = np.linspace(0.1, 0.9, 9).tolist()\n",
    "    #shrinkages = np.linspace(0.53, 0.55, 50).tolist()\n",
    "    #shrinkages = np.linspace(0.01, 0.1, 10).tolist()\n",
    "    #shrinkages = np.linspace(0.001, 0.01, 10).tolist()\n",
    "    #shrinkages = np.linspace(0.000001, 0.00001, 10).tolist()\n",
    "\n",
    "    grid['shrinkage'] = ['auto'] + shrinkages\n",
    "    #scoring='precision'\n",
    "    search = GridSearchCV(LDA, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "    # only_spatial_features  only_spatial_features_norm\n",
    "    # X_with_spatial_features X_with_spatial_features_norm\n",
    "    # X_norm_with_spatial_features_norm X_norm\n",
    "    # only_spatial_features_norm_ave\n",
    "    # X X_pca_norm\n",
    "    results = search.fit(used_feature, label)\n",
    "    log_xalg('result: %.4f' % results.best_score_, 'configuration:', results.best_params_, log_path=log_path)\n",
    "    #if best_score == 0 and best_params is None:\n",
    "    #    best_score = results.best_score_ \n",
    "    #    best_params = results.best_params_ \n",
    "    #elif results.best_score_ > best_score:\n",
    "    #    best_score = results.best_score_ \n",
    "    #    best_params = results.best_params_ \n",
    "    #else:\n",
    "    #    return best_score, best_params\n",
    "\n",
    "    if results.best_params_['shrinkage'] == 'auto':\n",
    "        log_xalg('within_class_shrinkages:', search.best_estimator_.within_class_shrinkages, 'tot_class_shrinkage:', search.best_estimator_.tot_class_shrinkage, log_path=log_path)\n",
    "        good_score1 = results.best_score_ \n",
    "        good_params1 = results.best_params_ \n",
    "    else:\n",
    "        gap = (shrinkage_range[1]-shrinkage_range[0]) / (shrinkage_range[2]-1)\n",
    "        #log_xalg('gap:', gap, 'shrinkage_range:', shrinkage_range)\n",
    "        if gap < min_gap:\n",
    "            return results.best_score_ , results.best_params_\n",
    "        shrinkage_range=(results.best_params_['shrinkage']-gap, results.best_params_['shrinkage']+gap, shrinkage_range[2])\n",
    "        good_score1, good_params1 = rLDA_model_params_grid_search(used_feature, label, shrinkage_range, log_path=log_path)\n",
    "        if good_score1 < results.best_score_ :\n",
    "            good_score1 = results.best_score_ \n",
    "            good_params1 = results.best_params_ \n",
    "    return good_score1, good_params1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(selected_features_list) 63\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from itertools import combinations\n",
    "log_path=r'D:\\BoyangDeng\\test\\exhuasted_search.txt'\n",
    "if os.path.exists(log_path):\n",
    "    os.remove(log_path)\n",
    "features=spatial_features_norm\n",
    "\n",
    "haralick_length_per_feature = spatial_features_num_dict['haralick_features']/len(band_list)\n",
    "lbp_length_per_feature = spatial_features_num_dict['lbp_features']/len(band_list)\n",
    "hu_moments_length_per_feature = spatial_features_num_dict['hu_moments_features']/len(band_list)\n",
    "features_lengths = [haralick_length_per_feature, lbp_length_per_feature, hu_moments_length_per_feature]\n",
    "\n",
    "start_idx_haralick = 0\n",
    "start_idx_lbp = spatial_features_num_dict['haralick_features']\n",
    "start_idx_hu_moments = (spatial_features_num_dict['haralick_features']+spatial_features_num_dict['lbp_features'])\n",
    "features_start_idxes = [start_idx_haralick, start_idx_lbp, start_idx_hu_moments]\n",
    "\n",
    "band_num = len(band_list)\n",
    "band_idx_list = np.arange(0, band_num).tolist()\n",
    "combine_num = 0\n",
    "#band_combines = combinations(band_idx_list, r=6)\n",
    "band_combines = combine_list_sort\n",
    "#band_combines=[(0,1,3,4)]\n",
    "\n",
    "selected_features_list = []\n",
    "for band_combine in band_combines:\n",
    "    if len(band_combine) == 0:\n",
    "        continue\n",
    "    combine_num += 1\n",
    "    #log_xalg('band_combine:', band_combine, log_path=log_path)\n",
    "    band_combine = np.array(band_combine)\n",
    "\n",
    "    feature_idxes = []\n",
    "    for features_length, features_start_idx in zip(features_lengths, features_start_idxes):\n",
    "        for start_idx in band_combine*features_length:\n",
    "            start_idx = int(start_idx)\n",
    "            features_length = int(features_length)\n",
    "            feature_idxes += list(range(features_start_idx+start_idx, features_start_idx+start_idx+features_length))\n",
    "    selected_features = features[:, feature_idxes]\n",
    "    selected_features_list.append(selected_features)\n",
    "    #log_xalg('selected_features.shape', selected_features.shape, log_path=log_path)\n",
    "\n",
    "    #seed_int = 0\n",
    "    #np.random.seed(seed_int)\n",
    "    ##np.random.seed(int(time.time()))\n",
    "    #random_state = np.random.randint(0, 1e4)\n",
    "    #shrinkage_range=(0.0, 1.0, 21)\n",
    "    #good_score, good_params = rLDA_model_params_grid_search(selected_features, y, shrinkage_range, min_gap=1e-5, log_path=log_path, random_state=seed)\n",
    "    #log_xalg('random_state: ', random_state,'good_score: %.3f' % good_score, 'good_params:', good_params, log_path=log_path)\n",
    "\n",
    "    #np.random.seed(seed_int)\n",
    "    #seed=True\n",
    "    #model_params =best_params\n",
    "    #xxx=repeated_tests(LinearDiscriminantAnalysis, only_spatial_features_norm, y, split_ratio=0.3, test_num=100, model_params=model_params, seed=seed, log_path=log_path)\n",
    "\n",
    "log_xalg('len(selected_features_list)', len(selected_features_list), log_path=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_exhausted_search(band_list, features, y, spatial_features_num_dict, min_gap, log_path, seed):\n",
    "    from itertools import product\n",
    "    from itertools import combinations\n",
    "    global best_score, best_params\n",
    "\n",
    "    features=only_spatial_features_norm\n",
    "\n",
    "    haralick_length_per_feature = spatial_features_num_dict['haralick_features']/len(band_list)\n",
    "    lbp_length_per_feature = spatial_features_num_dict['lbp_features']/len(band_list)\n",
    "    hu_moments_length_per_feature = spatial_features_num_dict['hu_moments_features']/len(band_list)\n",
    "    features_lengths = [haralick_length_per_feature, lbp_length_per_feature, hu_moments_length_per_feature]\n",
    "\n",
    "    start_idx_haralick = 0\n",
    "    start_idx_lbp = spatial_features_num_dict['haralick_features']\n",
    "    start_idx_hu_moments = (spatial_features_num_dict['haralick_features']+spatial_features_num_dict['lbp_features'])\n",
    "    features_start_idxes = [start_idx_haralick, start_idx_lbp, start_idx_hu_moments]\n",
    "\n",
    "    band_num = len(band_list)\n",
    "    band_idx_list = np.arange(0, band_num).tolist()\n",
    "    combine_num = 0\n",
    "\n",
    "    good_score_list = []\n",
    "    good_params_list = []\n",
    "    combine_list = []\n",
    "    if os.path.exists(log_path):\n",
    "        os.remove(log_path)\n",
    "    for i_num in range(band_num+1):\n",
    "        band_combines = combinations(band_idx_list, r=i_num)\n",
    "        for band_combine in band_combines:\n",
    "            if len(band_combine) == 0:\n",
    "                continue\n",
    "            combine_num += 1\n",
    "            log_xalg('band_combine:', band_combine, log_path=log_path)\n",
    "            band_combine = np.array(band_combine)\n",
    "\n",
    "            feature_idxes = []\n",
    "            for features_length, features_start_idx in zip(features_lengths, features_start_idxes):\n",
    "                for start_idx in band_combine*features_length:\n",
    "                    start_idx = int(start_idx)\n",
    "                    features_length = int(features_length)\n",
    "                    feature_idxes += list(range(features_start_idx+start_idx, features_start_idx+start_idx+features_length))\n",
    "            selected_features = features[:, feature_idxes]\n",
    "            log_xalg('selected_features.shape', selected_features.shape, log_path=log_path)\n",
    "            seed_int = 0\n",
    "            np.random.seed(seed_int)\n",
    "            #np.random.seed(int(time.time()))\n",
    "            random_state = np.random.randint(0, 1e4)\n",
    "            shrinkage_range=(0.0, 1.0, 21)\n",
    "            good_score, good_params = rLDA_model_params_grid_search(selected_features, y, shrinkage_range, min_gap=1e-5, log_path=log_path, random_state=seed)\n",
    "            log_xalg('random_state: ', random_state,'good_score: %.3f' % good_score, 'good_params:', good_params, log_path=log_path)\n",
    "\n",
    "            good_score_list.append(good_score)\n",
    "            good_params_list.append(good_params)\n",
    "            combine_list.append(band_combine)\n",
    "            #np.random.seed(seed)\n",
    "            #seed=True\n",
    "            #model_params=good_params\n",
    "            #xxx=repeated_tests(LinearDiscriminantAnalysis, only_spatial_features_norm, y, split_ratio=0.3, test_num=100, model_params=model_params, seed=seed, log_path=log_path)\n",
    "            #log_xalg('reset best_score, best_params', log_path=log_path)\n",
    "            best_score = 0.0\n",
    "            best_params = None\n",
    "    log_xalg('combine_num:', combine_num, log_path=log_path)\n",
    "    return good_score_list, good_params_list, combine_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path=r'F:\\test\\exhuasted_search.txt'\n",
    "good_score_list, good_params_list, combine_list=band_exhausted_search(band_list, spatial_features_norm, y, spatial_features_num_dict, min_gap=1e-5, log_path=log_path, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_score_list=np.array(good_score_list)\n",
    "good_params_list=np.array(good_params_list)\n",
    "combine_list=np.array(combine_list)\n",
    "idxes=np.argsort(good_score_list)[::-1]\n",
    "good_score_list_sort = good_score_list[idxes]\n",
    "good_params_list_sort = good_params_list[idxes]\n",
    "combine_list_sort = combine_list[idxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_tests(LinearDiscriminantAnalysis, X_with_spatial_features, label, split_ratio=0.3, test_num=100, model_params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_tests_with_SMOTE(LinearDiscriminantAnalysis, X_with_spatial_features, label, split_ratio=0.3, test_num=100, model_params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params ={'shrinkage': 'auto', 'solver':'eigen'}\n",
    "repeated_tests(LinearDiscriminantAnalysis, only_spatial_features_norm, label, split_ratio=0.3, test_num=100, model_params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "seed=True\n",
    "model_params ={'shrinkage': 6e-6, 'solver':'eigen'}\n",
    "xxx=repeated_tests(LinearDiscriminantAnalysis, X_norm, label, split_ratio=0.3, test_num=100, model_params=model_params, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "seed=True\n",
    "# precision\n",
    "#model_params ={'shrinkage': 0.35, 'solver':'eigen'}\n",
    "# accuracy\n",
    "model_params ={'shrinkage': 0.3089, 'solver':'eigen'}\n",
    "xxx=repeated_tests(LinearDiscriminantAnalysis, only_spatial_features_norm, label, split_ratio=0.3, test_num=10, model_params=model_params, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "seed=True\n",
    "model_params ={'shrinkage': 0.1722, 'solver':'eigen'}\n",
    "repeated_tests(LinearDiscriminantAnalysis, only_spatial_features_norm_ave, label, split_ratio=0.3, test_num=100, model_params=model_params, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "21 bands\n",
    "\"\"\"\n",
    "np.random.seed(0)\n",
    "seed=True\n",
    "model_params ={'shrinkage': 0.62, 'solver':'eigen'}\n",
    "xxx=repeated_tests(LinearDiscriminantAnalysis, only_spatial_features_norm, label, split_ratio=0.3, test_num=100, model_params=model_params, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 49.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420, 276) (294, 276) (126, 276) \n",
      "\n",
      "accuracy:\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.874127\n",
      "std      0.026262\n",
      "min      0.801587\n",
      "25%      0.857143\n",
      "50%      0.880952\n",
      "75%      0.888889\n",
      "max      0.936508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "6 bands / 3 bands / 5 bands\n",
    "\"\"\"\n",
    "np.random.seed(0)\n",
    "seed=True\n",
    "model_params ={'shrinkage': 0.363, 'solver':'eigen'}\n",
    "#model_params ={'shrinkage': 0.2716, 'solver':'eigen'}\n",
    "#model_params ={'shrinkage': 0.5349, 'solver':'eigen'}\n",
    "xxx=repeated_tests(LinearDiscriminantAnalysis, only_spatial_features_norm, label, split_ratio=0.3, test_num=100, model_params=model_params, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = X_norm_with_spatial_features_norm.copy()\n",
    "X_norm_with_spatial_features_norm_shuffle = tmp.transpose(1,0)\n",
    "np.random.shuffle(X_norm_with_spatial_features_norm_shuffle)\n",
    "X_norm_with_spatial_features_norm_shuffle = X_norm_with_spatial_features_norm_shuffle.transpose(1,0)\n",
    "X_norm_with_spatial_features_norm_shuffle.shape\n",
    "idxes = np.where(X_norm_with_spatial_features_norm_shuffle != X_norm_with_spatial_features_norm)\n",
    "len(idxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 52)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Precision: 0.939\n",
    "'shrinkage': 8.5789e-05\n",
    "\"\"\"\n",
    "#X_norm_with_spatial_features_norm_scale = np.concatenate([X_norm, 0.1*all_spatial_features_norm[:, start: end]], axis=1)\n",
    "#X_pca_norm_with_spatial_features_norm = np.concatenate([X_pca_norm, only_spatial_features_norm], axis=1)\n",
    "X_pca_norm_with_spatial_features_pca_norm = np.concatenate([X_pca_norm, results['PC'].values], axis=1)\n",
    "X_pca_norm_with_spatial_features_pca_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA(n_components=20)\n",
    "#X_pca_spatial = pca.fit_transform(only_spatial_features_norm)\n",
    "#X_pca_spatial.shape\n",
    "#pca.explained_variance_\n",
    "\n",
    "from pca import pca\n",
    "model = pca(n_components=0.95)\n",
    "results = model.fit_transform(only_spatial_features_norm)\n",
    "results.keys(), results['PC'].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0.35, 0.37, 20).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params ={'shrinkage':'auto', 'solver':'eigen'}\n",
    "repeated_tests_with_SMOTE(LinearDiscriminantAnalysis, X_with_spatial_features, label, split_ratio=0.3, test_num=100, model_params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "seed=True\n",
    "model_params =[{'shrinkage': 6e-6, 'solver':'eigen'}, {'shrinkage': 0.3089, 'solver':'eigen'}]\n",
    "model_class = [LinearDiscriminantAnalysis, LinearDiscriminantAnalysis]\n",
    "data = [X_norm, only_spatial_features_norm]\n",
    "precisions, recalls, accuracies = repeated_tests(model_class, data, label, split_ratio=0.3, test_num=200, model_params=model_params, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=55.323994137780055, pvalue=8.702115684323326e-123)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "stats.ttest_rel(accuracies[0], accuracies[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bands exhuasted search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code in rLDA + image features part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sprectral/spatial features -> ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((294, 462), (294, 2502))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random_state=42\n",
    "random_state = np.random.randint(0, 1e4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, label, test_size=0.30, random_state=random_state, shuffle=True, stratify=label)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(spatial_features_norm, label, test_size=0.30, random_state=random_state, shuffle=True, stratify=label)\n",
    "X_train.shape, X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_state: 0\n",
      "model_spectral precision: 0.967741935483871\n",
      "model_spectral recall: 0.9523809523809523\n",
      "model_spatial precision: 0.8888888888888888\n",
      "model_spatial recall: 0.7619047619047619\n",
      "accuracy: 0.9603174603174603 (array([ 32,  41,  93, 104, 118], dtype=int64),)\n",
      "accuracy: 0.8333333333333334 (array([  8,  10,  16,  43,  44,  56,  59,  63,  66,  70,  71,  77,  85,\n",
      "        87,  89,  93,  96, 100, 109, 110, 118], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "#random_state = 7313\n",
    "#random_state = np.random.randint(0, 1e4)\n",
    "random_state = 0\n",
    "print('random_state:', random_state)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, label, test_size=0.30, random_state=random_state, shuffle=True, stratify=label)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(spatial_features_norm, label, test_size=0.30, random_state=random_state, shuffle=True, stratify=label)\n",
    "y_train = y_train.squeeze()\n",
    "y_train1 = y_train1.squeeze()\n",
    "\n",
    "model_class=LinearDiscriminantAnalysis\n",
    "#model_params ={'shrinkage': 6e-6, 'solver':'eigen'}\n",
    "model_params ={'shrinkage': 5e-6, 'solver':'eigen'}\n",
    "model_spectral = model_class(**model_params)\n",
    "\n",
    "#model_params1 ={'shrinkage': 0.35, 'solver':'eigen'}\n",
    "model_params1 ={'shrinkage': 1e-06, 'solver':'eigen'}\n",
    "model_spatial = model_class(**model_params1)\n",
    "\n",
    "model_spectral.fit(X_train, y_train)\n",
    "model_spatial.fit(X_train1, y_train1)\n",
    "\n",
    "predictions = model_spectral.predict(X_test)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "print('model_spectral precision:', precision)\n",
    "print('model_spectral recall:', recall)\n",
    "\n",
    "predictions1 = model_spatial.predict(X_test1)\n",
    "precision = precision_score(y_test1, predictions1)\n",
    "recall = recall_score(y_test1, predictions1)\n",
    "print('model_spatial precision:', precision)\n",
    "print('model_spatial recall:', recall)\n",
    "\n",
    "diffs = np.where(predictions!=y_test.values.squeeze())\n",
    "print('accuracy:', 1-len(diffs[0])/len(y_test), diffs)\n",
    "\n",
    "diffs=np.where(predictions1!=y_test1.values.squeeze())\n",
    "print('accuracy:', 1-len(diffs[0])/len(y_test), diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sprectral = model_spectral.decision_function(X_train)\n",
    "df_describe = pd.DataFrame(score_sprectral)\n",
    "print(df_describe.describe())\n",
    "statistics = df_describe.describe()\n",
    "mean_spectral = statistics.loc['mean'].values\n",
    "std_spectral = statistics.loc['std'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_spatial = model_spatial.decision_function(X_train1)\n",
    "df_describe = pd.DataFrame(score_spatial)\n",
    "print(df_describe.describe())\n",
    "statistics = df_describe.describe()\n",
    "mean_spatial = statistics.loc['mean'].values\n",
    "std_spatial = statistics.loc['std'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_spectral = model_spectral.decision_function(X_test)\n",
    "score_spatial = model_spatial.decision_function(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_final = []\n",
    "i_scores = np.array([  4,  40,  55,  58, 111, 121])\n",
    "i_scores = np.array([  4,   7,  20,  24,  29,  36,  40,  41,  79,  84,  86,  88,  92, 99, 114])\n",
    "for i_score in i_scores:\n",
    "    score_spectral_norm = (score_spectral[i_score]-mean_spectral)/std_spectral\n",
    "    score_spatial_norm = (score_spatial[i_score]-mean_spatial)/std_spatial\n",
    "    bias = 0.4\n",
    "    if score_spatial_norm > 0:\n",
    "        if abs(score_spectral_norm) > abs(score_spatial_norm) - bias:\n",
    "            final_score = score_spectral_norm\n",
    "        else:\n",
    "            final_score = score_spatial_norm\n",
    "    else:\n",
    "        final_score = score_spectral_norm\n",
    "\n",
    "    if final_score > 0:\n",
    "        final_cls = 1\n",
    "    else:\n",
    "        final_cls = 0\n",
    "    predictions_final.append(final_cls)\n",
    "    print('i_score:', i_score)\n",
    "    print('score_spectral_norm:', score_spectral_norm)\n",
    "    print('score_spatial_norm:', score_spatial_norm)\n",
    "    print((y_test.values[i_score]==final_cls)[0], ', y:', y_test.values[i_score], ', final_cls:', final_cls)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "use abs(score_spectral_norm) - abs(score_spatial_norm) as x\n",
    "\"\"\"\n",
    "def diff_trust(x):\n",
    "    trust_level=[0,0]\n",
    "    if x<-0.4:\n",
    "        trust_level[0]=abs(x+0.4)\n",
    "        trust_level[1]=0\n",
    "    if x>=-0.4:\n",
    "        trust_level[0]=0\n",
    "        trust_level[1]=abs(x+0.4)\n",
    "    return trust_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "use abs(score_spectral_norm) and abs(score_spatial_norm) as x1, x2\n",
    "low trust / high trust\n",
    "\"\"\"\n",
    "def spectral_trust(x):\n",
    "    x = abs(x)\n",
    "    trust_level=[0,0]\n",
    "    if 0.0<=x<0.7:\n",
    "        trust_level[0]=(0.7-x)/0.7 + 0.5\n",
    "        trust_level[1]=0.0\n",
    "    if 0.7<=x<=1.3:\n",
    "        trust_level[0]=0.5*(-5/3*x+13/6)\n",
    "        trust_level[1]=0.5*(x-0.7)/0.6\n",
    "    elif 1.3<x<=1e6:\n",
    "        trust_level[0]=0.0\n",
    "        trust_level[1]=(x-1.3) + 0.5\n",
    "    return trust_level\n",
    "\n",
    "def spatial_trust(x):\n",
    "    x = abs(x)\n",
    "    trust_level=[0,0]\n",
    "    if 0.0<=x<0.7:\n",
    "        trust_level[0]=(0.7-x)/0.7 + 0.5\n",
    "        trust_level[1]=0.0\n",
    "    if 0.7<=x<=1.3:\n",
    "        trust_level[0]=0.5*(-5/3*x+13/6)\n",
    "        trust_level[1]=0.5*(x-0.7)/0.6\n",
    "    elif 1.3<x<=1e6:\n",
    "        trust_level[0]=0.0\n",
    "        trust_level[1]=(x-1.3) + 0.5\n",
    "    return trust_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "every place can sum to 1.0\n",
    "\n",
    "-3.0*std->3.0*std to 0.0->1.0\n",
    "\"\"\"\n",
    "def spectral_level(x):\n",
    "    level=[0,0]\n",
    "    level[0]=max((1.0-x), 0)\n",
    "    level[1]=max(x, 0)\n",
    "    return level\n",
    "\n",
    "def spatial_level(x):\n",
    "    level=[0,0]\n",
    "    level[0]=max((1.0-x), 0)\n",
    "    level[1]=max(x, 0)\n",
    "    return level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "every place can sum to 1.0\n",
    "\n",
    "0: good\n",
    "1: bad\n",
    "\"\"\"\n",
    "def quality_level(x):\n",
    "    level=[0,0]\n",
    "    level[0]=max((1.0-x), 0)\n",
    "    level[1]=max(x, 0)\n",
    "    return level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules(spectral, spatial):\n",
    "    \"\"\"\n",
    "    0: trust spectral\n",
    "    1: trust spectral\n",
    "    2: trust spatial\n",
    "    3: trust spectral\n",
    "    \"\"\"\n",
    "    rule_values = [0, 0, 0, 0]\n",
    "    rule_values[0]=min(spectral[0], spatial[0])\n",
    "    rule_values[1]=min(spectral[1], spatial[0])\n",
    "    rule_values[2]=min(spectral[0], spatial[1])\n",
    "    rule_values[3]=min(spectral[1], spatial[1])\n",
    "    return rule_values\n",
    "\n",
    "def infer(rule_values, spectral, spatial, bias=0):\n",
    "    quality_level = [0, 0]\n",
    "    trust_spectral = max(rule_values[0], rule_values[1], rule_values[3])+bias\n",
    "    trust_spatial = rule_values[2]\n",
    "    quality_level[0]=spectral[0]*trust_spectral + spatial[0]*trust_spatial\n",
    "    quality_level[1]=spectral[1]*trust_spectral + spatial[1]*trust_spatial\n",
    "    return quality_level\n",
    "\n",
    "def area_gravity(quality_level):\n",
    "    confidence=[0,0]\n",
    "    confidence[0]=1.0 - quality_level[0]\n",
    "    confidence[1]=quality_level[1]\n",
    "    sum_1=confidence[0]*quality_level[0]+confidence[1]*quality_level[1]\n",
    "    sum_2=quality_level[0]+quality_level[1]\n",
    "    final_confidence=sum_1/sum_2\n",
    "    return final_confidence\n",
    "\n",
    "def maximum_membership(confidence):\n",
    "    good=1.0-confidence\n",
    "    bad=confidence\n",
    "    if(bad>good):\n",
    "        #'bad'\n",
    "        quality_level=1\n",
    "    else:\n",
    "        # 'good'\n",
    "        quality_level=-1\n",
    "    return quality_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fuzzy_logic_infer(spectral_value, spatial_value, bias=0.4):\n",
    "    spectral=spectral_level(spectral_value)\n",
    "    spatial=spatial_level(spatial_value)\n",
    "    rules_value=rules(spectral, spatial)\n",
    "    quality_level=infer(rules_value, spectral, spatial, bias)\n",
    "    result_1=area_gravity(quality_level)\n",
    "    result_2=maximum_membership(result_1)\n",
    "    #print(\"class: {}, predict confidence: {}\".format(result_2,int(result_1+0.5)))\n",
    "    return result_2\n",
    "spectral_value=0.4\n",
    "spatial_value=0.6\n",
    "fuzzy_logic_infer(spectral_value, spatial_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a4a29d0cd0>]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.linspace(-0.5,-0.3,100)\n",
    "y=[]\n",
    "y1=[]\n",
    "for i in x:\n",
    "    j,k=diff_trust(i)\n",
    "    y.append(j)\n",
    "    y1.append(k)\n",
    "plt.plot(x, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9603174603174603, (array([ 41,  66,  93, 104, 118], dtype=int64),))"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_final = []\n",
    "for i_score, _ in enumerate(score_spectral):\n",
    "    score_spectral_norm = (score_spectral[i_score]-mean_spectral)/std_spectral\n",
    "    score_spatial_norm = (score_spatial[i_score]-mean_spatial)/std_spatial\n",
    "\n",
    "    bias = 0.4\n",
    "    if score_spatial_norm > 0:\n",
    "        if abs(score_spectral_norm) > abs(score_spatial_norm) - bias:\n",
    "            final_score = score_spectral_norm\n",
    "        else:\n",
    "            final_score = score_spatial_norm\n",
    "    else:\n",
    "        final_score = score_spectral_norm\n",
    "\n",
    "    if final_score > 0:\n",
    "        final_cls = 1\n",
    "    else:\n",
    "        final_cls = 0\n",
    "    predictions_final.append(final_cls)\n",
    "\n",
    "    #print('i_score:', i_score)\n",
    "    #print('score_spectral_norm:', score_spectral_norm)\n",
    "    #print('score_spatial_norm:', score_spatial_norm)\n",
    "    #print((y_test.values[i_score]==final_cls)[0], ', y:', y_test.values[i_score], ', final_cls:', final_cls)\n",
    "    #print('')\n",
    "\n",
    "#np.where(predictions!=y_test.values.squeeze())\n",
    "diffs=np.where(predictions_final!=y_test.values.squeeze())\n",
    "1-len(diffs[0])/len(y_test), diffs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeated_test_ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random_state = 100\n",
    "random_state = 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, label, test_size=0.30, random_state=random_state, shuffle=True, stratify=label)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_norm, label, test_size=0.30, random_state=random_state, shuffle=True, stratify=label)\n",
    "np.where(X_train != X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ensemble_method:\n",
    "    statistics_driven\n",
    "    fuzzy_logic\n",
    "\"\"\"\n",
    "# shrinkage_spectral=6e-6, shrinkage_spatial=0.363\n",
    "# shrinkage_spectral=5e-6, shrinkage_spatial=1e-6\n",
    "def repeated_test_ensamble(model_spectral, model_spatial, feature1, feature2, label, test_num=100, seed=None, model_params_spectral=None, model_params_spatial=None, ensemble_method='statistics_driven'):\n",
    "    from tqdm import tqdm\n",
    "    accuracy_spectrals = []\n",
    "    accuracy_spatials = []\n",
    "    accuracy_ensemble = []\n",
    "\n",
    "    precision_spectrals = []\n",
    "    precision_spatials = []\n",
    "    precision_ensemble = []\n",
    "\n",
    "    recall_spectrals = []\n",
    "    recall_spatials = []\n",
    "    recall_ensemble = []\n",
    "\n",
    "    all_score_spectrals_train = []\n",
    "    all_score_spatials_train = []\n",
    "\n",
    "    all_score_spectral_norms_train = []\n",
    "    all_score_spatial_norms_train = []\n",
    "\n",
    "    all_pred_spectral_norms_train = []\n",
    "    all_pred_spatial_norms_train = []\n",
    "\n",
    "    all_score_spectrals_test = []\n",
    "    all_score_spatials_test = []\n",
    "\n",
    "    all_score_spectral_norms_test = []\n",
    "    all_score_spatial_norms_test = []\n",
    "\n",
    "    all_pred_spectral_norms_test = []\n",
    "    all_pred_spatial_norms_test = []\n",
    "\n",
    "    all_y_test = []\n",
    "    all_y_train = []\n",
    "\n",
    "    all_bias = []\n",
    "\n",
    "    if 'PLS' in model_spectral.__name__ or 'PLS' in model_spatial.__name__:\n",
    "        boundary_center = 0.5\n",
    "\n",
    "    #print('feature1.shape', feature1.shape)\n",
    "    #print('feature2.shape', feature2.shape)\n",
    "    for i_test in tqdm(range(test_num)):\n",
    "        if seed is None:\n",
    "            np.random.seed(int(time.time()))\n",
    "        random_state = np.random.randint(0, 1e4)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(feature1, label, test_size=0.30, random_state=random_state, shuffle=True, stratify=label)\n",
    "        y_train = y_train.squeeze()\n",
    "        y_test = y_test.squeeze()\n",
    "\n",
    "        if type(feature2) is list:\n",
    "            X_train1_list=[]\n",
    "            X_test1_list=[]\n",
    "            y_train1_list=[]\n",
    "            y_test1_list=[]\n",
    "            for feature2_ in feature2:\n",
    "                X_train1_, X_test1_, y_train1_, y_test1_ = train_test_split(feature2_, label, test_size=0.30, random_state=random_state, shuffle=True, stratify=label)\n",
    "                y_train1_ = y_train1_.squeeze()\n",
    "                y_test1_ = y_test1_.squeeze()\n",
    "                X_train1_list.append(X_train1_)\n",
    "                X_test1_list.append(X_test1_)\n",
    "                y_train1_list.append(y_train1_)\n",
    "                y_test1_list.append(y_test1_)\n",
    "        else:\n",
    "            X_train1, X_test1, y_train1, y_test1 = train_test_split(feature2, label, test_size=0.30, random_state=random_state, shuffle=True, stratify=label)\n",
    "            y_train1 = y_train1.squeeze()\n",
    "            y_test1 = y_test1.squeeze()\n",
    "\n",
    "        if model_params_spectral is not None:\n",
    "            model_ = model_spectral(**model_params_spectral)\n",
    "        else:\n",
    "            model_ = model_spectral()\n",
    "        if 'PLS' in model_spectral.__name__:\n",
    "            y_train = pd.get_dummies(y_train)\n",
    "\n",
    "        model_.fit(X_train, y_train)\n",
    "        predictions = model_.predict(X_test)\n",
    "        \n",
    "        if 'PLS' in model_spectral.__name__:\n",
    "            predictions = np.array([np.argmax(i) for i in predictions])\n",
    "\n",
    "        precision = precision_score(y_test, predictions)\n",
    "        recall = recall_score(y_test, predictions)\n",
    "        diffs = np.where(predictions!=y_test.values.squeeze())\n",
    "        accuracy_spectral = 1-len(diffs[0])/len(y_test)\n",
    "\n",
    "        precision_spectrals.append(precision)\n",
    "        recall_spectrals.append(recall)\n",
    "        accuracy_spectrals.append(accuracy_spectral)\n",
    "        if 'PLS' in model_spectral.__name__:\n",
    "            score_spectral_train = model_.predict(X_train)\n",
    "            score_spectral_train = np.max(score_spectral_train, 1) - boundary_center\n",
    "            score_spectral_train[score_spectral_train<=0] = 0\n",
    "        else:\n",
    "            score_spectral_train = model_.decision_function(X_train)\n",
    "        df_describe = pd.DataFrame(score_spectral_train)\n",
    "        statistics = df_describe.describe()\n",
    "        mean_spectral_train = statistics.loc['mean'].values\n",
    "        std_spectral_train = statistics.loc['std'].values\n",
    "\n",
    "        #model_params1 ={'shrinkage': 0.35, 'solver':'eigen'}\n",
    "        #model_params1 ={'shrinkage': 0.62, 'solver':'eigen'}\n",
    "        #model_params1 ={'shrinkage': 0.363, 'solver':'eigen'}\n",
    "        #model_params1 ={'shrinkage': 0.5349, 'solver':'eigen'}\n",
    "        if type(feature2) is list:\n",
    "            score_spatial_train = None\n",
    "            precision_spatial_bands = []\n",
    "            recall_spatial_bands = []\n",
    "            accuracy_spatial_bands = []\n",
    "            model_spatial_all = []\n",
    "            assert type(model_params_spectral) is list\n",
    "            for i_model, model_param in enumerate(model_params_spectral):\n",
    "                model_params1 = model_param\n",
    "                if model_params1 is not None:\n",
    "                    model_1 = model_spatial(**model_params1)\n",
    "                else:\n",
    "                    model_1 = model_spatial()\n",
    "                if 'PLS' in model_spatial.__name__:\n",
    "                    y_train1_list[i_model] = pd.get_dummies(y_train1_list[i_model])\n",
    "\n",
    "                model_1.fit(X_train1_list[i_model], y_train1_list[i_model])\n",
    "                predictions1 = model_1.predict(X_test1_list[i_model])\n",
    "\n",
    "                if 'PLS' in model_spatial.__name__:\n",
    "                    predictions1 = np.array([np.argmax(i) for i in predictions1])\n",
    "\n",
    "                precision = precision_score(y_test1_list[i_model], predictions1)\n",
    "                recall = recall_score(y_test1_list[i_model], predictions1)\n",
    "                diffs=np.where(predictions1!=y_test1_list[i_model].values.squeeze())\n",
    "                accuracy_spatial = 1-len(diffs[0])/len(y_test1_list[i_model])\n",
    "                precision_spatial_bands.append(precision)\n",
    "                recall_spatial_bands.append(recall)\n",
    "                accuracy_spatial_bands.append(accuracy_spatial)\n",
    "\n",
    "                if score_spatial_train is None:\n",
    "                    score_spatial_train = model_1.decision_function(X_train1_list[i_model])\n",
    "                else:\n",
    "                    score_spatial_train += model_1.decision_function(X_train1_list[i_model])\n",
    "                model_spatial_all.append(model_1)\n",
    "\n",
    "            precision_spatials.append(precision_spatial_bands)\n",
    "            recall_spatials.append(recall_spatial_bands)\n",
    "            accuracy_spatials.append(accuracy_spatial_bands)\n",
    "\n",
    "            df_describe = pd.DataFrame(score_spatial_train)\n",
    "            statistics = df_describe.describe()\n",
    "            mean_spatial_train = statistics.loc['mean'].values\n",
    "            std_spatial_train = statistics.loc['std'].values\n",
    "        else:\n",
    "            model_params1 = model_params_spatial\n",
    "            if model_params1 is not None:\n",
    "                model_1 = model_spatial(**model_params1)\n",
    "            else:\n",
    "                model_1 = model_spatial()\n",
    "            if 'PLS' in model_spatial.__name__:\n",
    "                y_train1 = pd.get_dummies(y_train1)\n",
    "\n",
    "            model_1.fit(X_train1, y_train1)\n",
    "            predictions1 = model_1.predict(X_test1)\n",
    "\n",
    "            if 'PLS' in model_spatial.__name__:\n",
    "                predictions1 = np.array([np.argmax(i) for i in predictions1])\n",
    "\n",
    "            precision = precision_score(y_test1, predictions1)\n",
    "            recall = recall_score(y_test1, predictions1)\n",
    "            diffs=np.where(predictions1!=y_test1.values.squeeze())\n",
    "            accuracy_spatial = 1-len(diffs[0])/len(y_test1)\n",
    "            if 'PLS' in model_spatial.__name__:\n",
    "                score_spatial_train = model_1.predict(X_train1)\n",
    "                score_spatial_train = np.max(score_spatial_train, 1) - boundary_center\n",
    "                score_spatial_train[score_spatial_train<=0] = 0\n",
    "            else:\n",
    "                score_spatial_train = model_1.decision_function(X_train1)\n",
    "\n",
    "            df_describe = pd.DataFrame(score_spatial_train)\n",
    "            statistics = df_describe.describe()\n",
    "            mean_spatial_train = statistics.loc['mean'].values\n",
    "            std_spatial_train = statistics.loc['std'].values\n",
    "\n",
    "            precision_spatials.append(precision)\n",
    "            recall_spatials.append(recall)\n",
    "            accuracy_spatials.append(accuracy_spatial)\n",
    "\n",
    "        score_spectrals = []\n",
    "        score_spatials = []\n",
    "        score_spectral_norms = []\n",
    "        score_spatial_norms = []\n",
    "        pred_spectral_norms = []\n",
    "        pred_spatial_norms = []\n",
    "        for i_score, _ in enumerate(score_spectral_train):\n",
    "            score_spectral_norm = (score_spectral_train[i_score]-mean_spectral_train)/std_spectral_train\n",
    "            score_spatial_norm = (score_spatial_train[i_score]-mean_spatial_train)/std_spatial_train\n",
    "\n",
    "            score_spectrals.append(score_spectral_train[i_score])\n",
    "            score_spatials.append(score_spatial_train[i_score])\n",
    "\n",
    "            score_spectral_norms.append(score_spectral_norm)\n",
    "            score_spatial_norms.append(score_spatial_norm)\n",
    "\n",
    "            pred_spectral_norm = 1 if score_spectral_norm > 0 else 0\n",
    "            pred_spatial_norm = 1 if score_spatial_norm > 0 else 0\n",
    "\n",
    "            pred_spectral_norms.append(pred_spectral_norm)\n",
    "            pred_spatial_norms.append(pred_spatial_norm)\n",
    "\n",
    "        score_spectrals = np.array(score_spectrals)\n",
    "        score_spatials = np.array(score_spatials)\n",
    "        all_score_spectrals_train.append(score_spectrals)\n",
    "        all_score_spatials_train.append(score_spatials)\n",
    "\n",
    "        score_spectral_norms = np.array(score_spectral_norms)\n",
    "        score_spatial_norms = np.array(score_spatial_norms)\n",
    "        all_score_spectral_norms_train.append(score_spectral_norms)\n",
    "        all_score_spatial_norms_train.append(score_spatial_norms)\n",
    "\n",
    "        pred_spectral_norms = np.array(pred_spectral_norms)\n",
    "        pred_spatial_norms = np.array(pred_spatial_norms)\n",
    "        all_pred_spectral_norms_train.append(pred_spectral_norms)\n",
    "        all_pred_spatial_norms_train.append(pred_spatial_norms)\n",
    "\n",
    "        all_y_train.append(y_train)\n",
    "        if 'PLS' in model_spectral.__name__:\n",
    "            score_spectral_test = model_.predict(X_test)\n",
    "            score_spectral_test = np.max(score_spectral_test, 1) - boundary_center\n",
    "            score_spectral_test[score_spectral_test<=0] = 0\n",
    "        else:\n",
    "            score_spectral_test = model_.decision_function(X_test)\n",
    "        \n",
    "        if type(feature2) is list:\n",
    "            score_spatial_test = None\n",
    "            for i_model, _ in enumerate(X_test1_list):\n",
    "                if score_spatial_test is None:\n",
    "                    score_spatial_test = model_spatial_all[i_model].decision_function(X_test1_list[i_model])\n",
    "                else:\n",
    "                    score_spatial_test += model_spatial_all[i_model].decision_function(X_test1_list[i_model])\n",
    "        else:\n",
    "            if 'PLS' in model_spatial.__name__:\n",
    "                score_spatial_test = model_1.predict(X_test1)\n",
    "                score_spatial_test = np.max(score_spatial_test, 1) - boundary_center\n",
    "                score_spatial_test[score_spatial_test<=0] = 0\n",
    "            else:\n",
    "                score_spatial_test = model_1.decision_function(X_test1)\n",
    "\n",
    "        score_spectrals = []\n",
    "        score_spatials = []\n",
    "        score_spectral_norms = []\n",
    "        score_spatial_norms = []\n",
    "        pred_spectral_norms = []\n",
    "        pred_spatial_norms = []\n",
    "\n",
    "        predictions_final = []\n",
    "        #if type(feature2) is list:\n",
    "        #    bias = np.array(score_spatial_train).reshape(-1).std() / len(feature2) / np.array(score_spectral_train).reshape(-1).std()\n",
    "        #else:\n",
    "        #    bias = np.array(score_spatial_train).reshape(-1).std() / np.array(score_spectral_train).reshape(-1).std()\n",
    "        bias = mean_spatial_train - mean_spectral_train\n",
    "        all_bias.append(bias)\n",
    "        for i_score, _ in enumerate(score_spectral_test):\n",
    "            score_spectral_norm = (score_spectral_test[i_score]-mean_spectral_train)/std_spectral_train\n",
    "            score_spatial_norm = (score_spatial_test[i_score]-mean_spatial_train)/std_spatial_train\n",
    "\n",
    "            score_spectrals.append(score_spectral_test[i_score])\n",
    "            score_spatials.append(score_spatial_test[i_score])\n",
    "\n",
    "            score_spectral_norms.append(score_spectral_norm)\n",
    "            score_spatial_norms.append(score_spatial_norm)\n",
    "\n",
    "            pred_spectral_norm = 1 if score_spectral_norm > 0 else 0\n",
    "            pred_spatial_norm = 1 if score_spatial_norm > 0 else 0\n",
    "\n",
    "            pred_spectral_norms.append(pred_spectral_norm)\n",
    "            pred_spatial_norms.append(pred_spatial_norm)\n",
    "\n",
    "            if np.sign(score_spectral_norm) != np.sign(score_spatial_norm):\n",
    "                if ensemble_method == 'statistics_driven':\n",
    "                    # this bias more like SVM (hinge loss) than fuzzy logic\n",
    "                    #bias = 0.22\n",
    "                    if score_spatial_norm > 0:\n",
    "                        if abs(score_spectral_norm) + bias > abs(score_spatial_norm):\n",
    "                            final_score = score_spectral_norm\n",
    "                        else:\n",
    "                            final_score = score_spatial_norm\n",
    "                    else:\n",
    "                        final_score = score_spectral_norm\n",
    "                #if ensemble_method == 'svc_model':\n",
    "                    #res = svc_model.predict([[score_spectral_norm[0], score_spatial_norm[0]]])\n",
    "                    #final_score = score_spectral_norm if res ==0 else score_spatial_norm\n",
    "                elif ensemble_method == 'fuzzy_logic':\n",
    "                    # bias=0.62\n",
    "                    spectral_value = ((score_spectral_norm/3.0)+1.0)/2.0\n",
    "                    spatial_value = ((score_spatial_norm/3.0)+1.0)/2.0\n",
    "                    final_score = fuzzy_logic_infer(spectral_value, spatial_value, bias=3*bias)\n",
    "                else:\n",
    "                    assert False\n",
    "            else:\n",
    "                final_score = score_spectral_norm\n",
    "\n",
    "            if final_score > 0:\n",
    "                final_cls = 1\n",
    "            else:\n",
    "                final_cls = 0\n",
    "            predictions_final.append(final_cls)\n",
    "\n",
    "        score_spectrals = np.array(score_spectrals)\n",
    "        score_spatials = np.array(score_spatials)\n",
    "        all_score_spectrals_test.append(score_spectrals)\n",
    "        all_score_spatials_test.append(score_spatials)\n",
    "\n",
    "        score_spectral_norms = np.array(score_spectral_norms)\n",
    "        score_spatial_norms = np.array(score_spatial_norms)\n",
    "        all_score_spectral_norms_test.append(score_spectral_norms)\n",
    "        all_score_spatial_norms_test.append(score_spatial_norms)\n",
    "\n",
    "        pred_spectral_norms = np.array(pred_spectral_norms)\n",
    "        pred_spatial_norms = np.array(pred_spatial_norms)\n",
    "        all_pred_spectral_norms_test.append(pred_spectral_norms)\n",
    "        all_pred_spatial_norms_test.append(pred_spatial_norms)\n",
    "\n",
    "        all_y_test.append(y_test)\n",
    "\n",
    "        precision = precision_score(y_test, predictions_final)\n",
    "        recall = recall_score(y_test, predictions_final)\n",
    "\n",
    "        diffs=np.where(predictions_final!=y_test.values.squeeze())\n",
    "        final_accuracy = 1-len(diffs[0])/len(y_test)\n",
    "\n",
    "        precision_ensemble.append(precision)\n",
    "        recall_ensemble.append(recall)\n",
    "        accuracy_ensemble.append(final_accuracy)\n",
    "    return precision_spectrals, precision_spatials, precision_ensemble, recall_spectrals, recall_spatials, recall_ensemble, accuracy_spectrals, accuracy_spatials, accuracy_ensemble, all_y_train, all_score_spectral_norms_train, all_score_spatial_norms_train,  all_pred_spectral_norms_train, all_pred_spatial_norms_train, all_score_spectrals_train, all_score_spatials_train, all_y_test, all_score_spectral_norms_test, all_score_spatial_norms_test,  all_pred_spectral_norms_test, all_pred_spatial_norms_test, all_score_spectrals_test, all_score_spatials_test, all_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_accuracy_list = []\n",
    "spectral_accuracy_list = []\n",
    "spatial_accuracy_list = []\n",
    "\n",
    "score_t = 0.875\n",
    "idxes = np.where(good_score_list_sort >= score_t)[0]\n",
    "print('candidate num:', len(idxes))\n",
    "for i_select in range(len(selected_features_list)):\n",
    "    if good_score_list_sort[i_select] < score_t:\n",
    "        break\n",
    "    feature1 = X_norm\n",
    "    #feature2 = only_spatial_features_norm\n",
    "    feature2 = selected_features_list[i_select]\n",
    "    combine = combine_list_sort[i_select]\n",
    "    shrinkage_spectral = 6e-6\n",
    "    shrinkage_spatial = good_params_list_sort[i_select]['shrinkage']\n",
    "    print('i_select:', i_select, ', combine:', combine, ', shrinkage_spatial:', shrinkage_spatial, ', score:', good_score_list_sort[i_select])\n",
    "\n",
    "    np.random.seed(0)\n",
    "    seed = True\n",
    "    (precision_spectrals, precision_spatials, precision_ensemble, recall_spectrals, recall_spatials, recall_ensemble, accuracy_spectrals, accuracy_spatials, accuracy_ensemble, all_y_train, all_score_spectral_norms_train, all_score_spatial_norms_train,  all_pred_spectral_norms_train, all_pred_spatial_norms_train, all_score_spectrals_train, all_score_spatials_train, all_y_test, all_score_spectral_norms_test, all_score_spatial_norms_test,  all_pred_spectral_norms_test, all_pred_spatial_norms_test, all_score_spectrals_test, all_score_spatials_test, all_bias\n",
    "    )=repeated_test_ensamble(feature1, feature2, label, test_num=100, seed=seed, shrinkage_spectral=shrinkage_spectral, shrinkage_spatial=shrinkage_spatial)\n",
    "    accuracy_ensemble=np.array(accuracy_ensemble)\n",
    "    accuracy_spectrals=np.array(accuracy_spectrals)\n",
    "    accuracy_spatials=np.array(accuracy_spatials)\n",
    "    mean_accuracy_final = accuracy_ensemble.mean()\n",
    "    mean_accuracy_spectral = accuracy_spectrals.mean()\n",
    "    mean_accuracy_spatial = accuracy_spatials.mean()\n",
    "    print('mean_accuracy_final:', mean_accuracy_final)\n",
    "    final_accuracy_list.append(mean_accuracy_final)\n",
    "    spectral_accuracy_list.append(mean_accuracy_spectral)\n",
    "    spatial_accuracy_list.append(mean_accuracy_spatial)\n",
    "final_accuracy_list = np.array(final_accuracy_list)\n",
    "idxes = np.argsort(final_accuracy_list)[::-1]\n",
    "#idx = np.argmax(final_accuracy_list)\n",
    "for idx in idxes:\n",
    "    shrinkage = good_params_list_sort[idx]['shrinkage']\n",
    "    if shrinkage != 'auto':\n",
    "        shrinkage = round(shrinkage, 5)\n",
    "    print(\"%2d\"%idx, \"%.4f\"%round(final_accuracy_list[idx],4), \"%.4f\"%round(spectral_accuracy_list[idx], 4), \"%.4f\"%round(spatial_accuracy_list[idx], 4), combine_list_sort[idx], shrinkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxes = np.argsort(final_accuracy_list)[::-1]\n",
    "print('candidate num:', len(good_score_list_sort))\n",
    "for i_rank, idx in enumerate(idxes):\n",
    "    shrinkage = good_params_list_sort[idx]['shrinkage']\n",
    "    if shrinkage != 'auto':\n",
    "        shrinkage = round(shrinkage, 5)\n",
    "    print(\"%2d\"%i_rank, \"%2d\"%idx, \"%.4f\"%round(final_accuracy_list[idx],4), \"%.4f\"%round(spectral_accuracy_list[idx], 4), \"%.4f\"%round(spatial_accuracy_list[idx], 4), combine_list_sort[idx], shrinkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:28<00:00,  3.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "\n",
    "shrinkage_spectral = 5e-6\n",
    "shrinkage_spatial = 'auto'\n",
    "#i_select=8\n",
    "#feature2 = selected_features_list[i_select]\n",
    "#combine = combine_list_sort[i_select]\n",
    "#shrinkage_spatial = good_params_list_sort[i_select]['shrinkage']\n",
    "\n",
    "seed_int=0\n",
    "#seed_int=1673588891  \n",
    "#seed_int = int(time.time())\n",
    "np.random.seed(seed_int)\n",
    "seed = True\n",
    "\n",
    "#print('seed_int:', seed_int, 'combine:', combine, ', shrinkage_spatial:', round(shrinkage_spatial,5), ', score:', round(good_score_list_sort[i_select], 4))\n",
    "\n",
    "# statistics_driven\n",
    "# fuzzy_logic\n",
    "\n",
    "model_params_spectral={'shrinkage': shrinkage_spectral, 'solver': 'eigen'}\n",
    "#model_params_spectral={'n_components': 28,}\n",
    "#model_params_spectral=None\n",
    "\n",
    "#model_params_spatial={'shrinkage': shrinkage_spatial, 'solver': 'eigen'}\n",
    "model_params_spatial=None\n",
    "#model_params_spatial={'n_components': 5,}\n",
    "\n",
    "# LinearDiscriminantAnalysis\n",
    "\n",
    "# 28 5 6 22 1\n",
    "#model_params={'n_components': 1,}\n",
    "#PLSRegression\n",
    "\n",
    "spectral_model = LinearDiscriminantAnalysis\n",
    "spatial_model = LinearDiscriminantAnalysis\n",
    "\n",
    "#X_norm\n",
    "#spatial_features_norm\n",
    "#X_norm_with_spatial_features_norm\n",
    "#X_norm_with_spatial_features_norm_MRMR_norm\n",
    "#X_norm_with_spatial_features_norm_PCA_norm\n",
    "#X_norm_mrmr\n",
    "#spatial_features_mrmr\n",
    "#spatial_features_norm_MRMR\n",
    "\n",
    "feature1 = X_norm\n",
    "feature2 = spatial_features_norm\n",
    "\n",
    "(precision_spectrals, precision_spatials, precision_ensemble, recall_spectrals, recall_spatials, recall_ensemble, accuracy_spectrals, accuracy_spatials, accuracy_ensemble, all_y_train, all_score_spectral_norms_train, all_score_spatial_norms_train,  all_pred_spectral_norms_train, all_pred_spatial_norms_train, all_score_spectrals_train, all_score_spatials_train, all_y_test, all_score_spectral_norms_test, all_score_spatial_norms_test,  all_pred_spectral_norms_test, all_pred_spatial_norms_test, all_score_spectrals_test, all_score_spatials_test, all_bias\n",
    ")=repeated_test_ensamble(spectral_model, spatial_model, feature1, feature2, label, test_num=100, seed=seed, model_params_spectral=model_params_spectral, model_params_spatial=model_params_spatial, ensemble_method='statistics_driven')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:36<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "i_select=20\n",
    "feature1 = X_norm\n",
    "#feature2 = only_spatial_features_norm\n",
    "feature2 = selected_features_list[:i_select]\n",
    "combine = combine_list_sort[:i_select]\n",
    "shrinkage_spectral = 6e-6\n",
    "#shrinkage_spatial = good_params_list_sort[:i_select]['shrinkage']\n",
    "shrinkage_spatial = []\n",
    "for good_params in good_params_list_sort[:i_select]:\n",
    "    shrinkage_spatial += [good_params['shrinkage']]\n",
    "\n",
    "seed_int=0\n",
    "#seed_int=1673588746\n",
    "#seed_int = int(time.time())\n",
    "np.random.seed(seed_int)\n",
    "\n",
    "seed = True\n",
    "(precision_spectrals, precision_spatials, precision_ensemble, recall_spectrals, recall_spatials, recall_ensemble, accuracy_spectrals, accuracy_spatials, accuracy_ensemble, all_y_train, all_score_spectral_norms_train, all_score_spatial_norms_train,  all_pred_spectral_norms_train, all_pred_spatial_norms_train, all_score_spectrals_train, all_score_spatials_train, all_y_test, all_score_spectral_norms_test, all_score_spatial_norms_test,  all_pred_spectral_norms_test, all_pred_spatial_norms_test, all_score_spectrals_test, all_score_spatials_test, all_bias\n",
    ")=repeated_test_ensamble(feature1, feature2, label, test_num=100, seed=seed, shrinkage_spectral=shrinkage_spectral, shrinkage_spatial=shrinkage_spatial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0\n",
      "count  100.000000\n",
      "mean     0.964841\n",
      "std      0.025180\n",
      "min      0.887324\n",
      "25%      0.953125\n",
      "50%      0.967742\n",
      "75%      0.983871\n",
      "max      1.000000\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.967143\n",
      "std      0.023365\n",
      "min      0.888889\n",
      "25%      0.952381\n",
      "50%      0.968254\n",
      "75%      0.984127\n",
      "max      1.000000\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.965556\n",
      "std      0.016120\n",
      "min      0.928571\n",
      "25%      0.952381\n",
      "50%      0.968254\n",
      "75%      0.976190\n",
      "max      1.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_statistics(precision_ensemble), show_statistics(recall_ensemble), show_statistics(accuracy_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0\n",
      "count  100.000000\n",
      "mean     0.976644\n",
      "std      0.021128\n",
      "min      0.910448\n",
      "25%      0.966667\n",
      "50%      0.983051\n",
      "75%      1.000000\n",
      "max      1.000000\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.908405\n",
      "std      0.035110\n",
      "min      0.833333\n",
      "25%      0.884295\n",
      "50%      0.913037\n",
      "75%      0.932203\n",
      "max      0.981818\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.964841\n",
      "std      0.025180\n",
      "min      0.887324\n",
      "25%      0.953125\n",
      "50%      0.967742\n",
      "75%      0.983871\n",
      "max      1.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_statistics(precision_spectrals), show_statistics(precision_spatials), show_statistics(precision_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0\n",
      "count  100.000000\n",
      "mean     0.936508\n",
      "std      0.033004\n",
      "min      0.841270\n",
      "25%      0.920635\n",
      "50%      0.936508\n",
      "75%      0.968254\n",
      "max      1.000000\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.786349\n",
      "std      0.050771\n",
      "min      0.634921\n",
      "25%      0.757937\n",
      "50%      0.793651\n",
      "75%      0.813492\n",
      "max      0.888889\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.967143\n",
      "std      0.023365\n",
      "min      0.888889\n",
      "25%      0.952381\n",
      "50%      0.968254\n",
      "75%      0.984127\n",
      "max      1.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_statistics(recall_spectrals), show_statistics(recall_spatials), show_statistics(recall_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0\n",
      "count  100.000000\n",
      "mean     0.956746\n",
      "std      0.017380\n",
      "min      0.920635\n",
      "25%      0.944444\n",
      "50%      0.952381\n",
      "75%      0.968254\n",
      "max      0.992063\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.852937\n",
      "std      0.029209\n",
      "min      0.785714\n",
      "25%      0.833333\n",
      "50%      0.849206\n",
      "75%      0.873016\n",
      "max      0.920635\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.965556\n",
      "std      0.016120\n",
      "min      0.928571\n",
      "25%      0.952381\n",
      "50%      0.968254\n",
      "75%      0.976190\n",
      "max      1.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_statistics(accuracy_spectrals), show_statistics(accuracy_spatials), show_statistics(accuracy_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_statistics(all_score_spectrals_train, reshape=1), show_statistics(all_score_spatials_train, reshape=1), show_statistics(all_bias, reshape=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(all_y, all_score_spectral_norms, all_score_spatial_norms, all_pred_spectral_norms, all_pred_spatial_norms):\n",
    "    all_score_spectral_norms=np.array(all_score_spectral_norms).squeeze().reshape(-1)\n",
    "    all_score_spatial_norms=np.array(all_score_spatial_norms).squeeze().reshape(-1)\n",
    "    all_pred_spectral_norms=np.array(all_pred_spectral_norms).squeeze().reshape(-1)\n",
    "    all_pred_spatial_norms=np.array(all_pred_spatial_norms).squeeze().reshape(-1)\n",
    "    all_labels = []\n",
    "    for xxx in all_y:\n",
    "        all_labels += xxx.values.tolist()\n",
    "    #show_statistics(all_score_spectral_norms), show_statistics(all_score_spatial_norms)\n",
    "    return all_labels, all_score_spectral_norms, all_score_spatial_norms, all_pred_spectral_norms, all_pred_spatial_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_statistics(all_labels, all_pred_spectral_norms, all_pred_spatial_norms, all_score_spectral_norms, all_score_spatial_norms):\n",
    "    both_right_num=0\n",
    "    only_sprectral_right_num=0\n",
    "    only_spatial_right_num=0\n",
    "    both_wrong=0\n",
    "    tot_num = len(all_labels)\n",
    "    print('tot_num:', tot_num)\n",
    "    only_spectral_right_pairs = []\n",
    "    only_spatial_right_pairs = []\n",
    "    for i_sample in range(tot_num):\n",
    "        if all_labels[i_sample]==all_pred_spectral_norms[i_sample] and all_labels[i_sample]==all_pred_spatial_norms[i_sample]:\n",
    "            both_right_num+=1\n",
    "        elif all_labels[i_sample]==all_pred_spectral_norms[i_sample] and all_labels[i_sample]!=all_pred_spatial_norms[i_sample]:\n",
    "            only_sprectral_right_num+=1\n",
    "            only_spectral_right_pairs.append((all_score_spectral_norms[i_sample], all_score_spatial_norms[i_sample]))\n",
    "        elif all_labels[i_sample]!=all_pred_spectral_norms[i_sample] and all_labels[i_sample]==all_pred_spatial_norms[i_sample]:\n",
    "            only_spatial_right_num+=1\n",
    "            only_spatial_right_pairs.append((all_score_spectral_norms[i_sample], all_score_spatial_norms[i_sample]))\n",
    "        elif all_labels[i_sample]!=all_pred_spectral_norms[i_sample] and all_labels[i_sample]!=all_pred_spatial_norms[i_sample]:\n",
    "            both_wrong+=1\n",
    "        else:\n",
    "            assert True\n",
    "    print(\"both_right ratio:\", both_right_num/tot_num)\n",
    "    print(\"only_sprectral_right ratio:\", only_sprectral_right_num/tot_num)\n",
    "    print(\"only_spatial_right ratio:\", only_spatial_right_num/tot_num)\n",
    "    print(\"both_wrong ratio:\", both_wrong/tot_num)\n",
    "    return only_spectral_right_pairs, only_spatial_right_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29400,) 29400\n",
      "tot_num: 29400\n",
      "both_right ratio: 0.9168707482993197\n",
      "only_sprectral_right ratio: 0.08312925170068028\n",
      "only_spatial_right ratio: 0.0\n",
      "both_wrong ratio: 0.0\n"
     ]
    }
   ],
   "source": [
    "all_labels, all_score_spectral_norms, all_score_spatial_norms, all_pred_spectral_norms, all_pred_spatial_norms=gather_data(all_y_train, all_score_spectral_norms_train, all_score_spatial_norms_train, all_pred_spectral_norms_train, all_pred_spatial_norms_train)\n",
    "only_spectral_right_pairs, only_spatial_right_pairs=result_statistics(all_labels, all_pred_spectral_norms, all_pred_spatial_norms, all_score_spectral_norms, all_score_spatial_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot_num: 12600\n",
      "both_right ratio: 0.846984126984127\n",
      "only_sprectral_right ratio: 0.11428571428571428\n",
      "only_spatial_right ratio: 0.02642857142857143\n",
      "both_wrong ratio: 0.012301587301587301\n"
     ]
    }
   ],
   "source": [
    "all_labels, all_score_spectral_norms, all_score_spatial_norms, all_pred_spectral_norms, all_pred_spatial_norms=gather_data(all_y_test, all_score_spectral_norms_test, all_score_spatial_norms_test, all_pred_spectral_norms_test, all_pred_spatial_norms_test)\n",
    "only_spectral_right_pairs, only_spatial_right_pairs=result_statistics(all_labels, all_pred_spectral_norms, all_pred_spatial_norms, all_score_spectral_norms, all_score_spatial_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "tmp_list=[]\n",
    "tmp_list1=[]\n",
    "for pair in only_spectral_right_pairs:\n",
    "    tmp_list.append(pair[0])\n",
    "    tmp_list1.append(pair[1])\n",
    "plt.figure()\n",
    "plt.plot(tmp_list, tmp_list1, 'g.')\n",
    "plt.xlim([-3,3])\n",
    "plt.ylim([-3,3])\n",
    "tmp_list=[]\n",
    "tmp_list1=[]\n",
    "for pair in only_spatial_right_pairs:\n",
    "    tmp_list.append(pair[0])\n",
    "    tmp_list1.append(pair[1])\n",
    "plt.figure()\n",
    "plt.plot(tmp_list, tmp_list1, 'r.')\n",
    "plt.xlim([-3,3])\n",
    "plt.ylim([-3,3])\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_bias: 0.16000000000000075\n",
      "fuzzy_logic_infer:\n",
      "ratio: 1.0 ratio1: 0.024024024024024024\n",
      "total info: 1448 0.8166948674562888\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tmp_list = []\n",
    "tmp_list1 = []\n",
    "# only_sprectral_right_pairs only_spatial_right_pairs\n",
    "\n",
    "#init_bias=0.01\n",
    "init_bias=-1.0\n",
    "bias = init_bias\n",
    "step = 0.01\n",
    "last_ratio = 0\n",
    "last_ratio1 = 0\n",
    "last_choose_num = 0\n",
    "last_choose_num1 = 0\n",
    "best_sum = 0\n",
    "\n",
    "def judge_func(pair, bias):\n",
    "    #yyy = abs(pair[0])+bias-abs(pair[1])\n",
    "\n",
    "    if pair[1] > 0:\n",
    "        yyy = abs(pair[0])+bias-abs(pair[1])\n",
    "    else:\n",
    "        yyy = 1\n",
    "\n",
    "    #yyy = abs(pair[0])/abs(pair[1])-bias\n",
    "\n",
    "    #res = svc_model.predict([[pair[0], pair[1]]])\n",
    "    #yyy = 1 if res == 0 else -1\n",
    "    return yyy\n",
    "\n",
    "#if True:\n",
    "while True:\n",
    "    choose_num = 0 \n",
    "    for pair in only_spectral_right_pairs:\n",
    "        yyy = judge_func(pair, bias)\n",
    "        if yyy >= 0:\n",
    "            choose_num+=1\n",
    "\n",
    "        #score_spectral_norm = pair[0]\n",
    "        #score_spatial_norm = pair[1]\n",
    "        #spectral_value = ((score_spectral_norm/3.0)+1.0)/2.0\n",
    "        #spatial_value = ((score_spatial_norm/3.0)+1.0)/2.0\n",
    "        #yyy = fuzzy_logic_infer(spectral_value, spatial_value, bias)\n",
    "        #if np.sign(pair[0]) == np.sign(yyy):\n",
    "        #    choose_num+=1\n",
    "\n",
    "    ratio = choose_num/len(only_spectral_right_pairs)\n",
    "\n",
    "    choose_num1 = 0 \n",
    "    for pair in only_spatial_right_pairs:\n",
    "        yyy = judge_func(pair, bias)\n",
    "        if yyy < 0:\n",
    "            choose_num1+=1\n",
    "\n",
    "        #score_spectral_norm = pair[0]\n",
    "        #score_spatial_norm = pair[1]\n",
    "        #spectral_value = ((score_spectral_norm/3.0)+1.0)/2.0\n",
    "        #spatial_value = ((score_spatial_norm/3.0)+1.0)/2.0\n",
    "        #yyy = fuzzy_logic_infer(spectral_value, spatial_value, bias)\n",
    "        #if np.sign(pair[1]) == np.sign(yyy):\n",
    "        #    choose_num+=1\n",
    "\n",
    "    ratio1 = choose_num1/len(only_spatial_right_pairs)\n",
    "    if choose_num + choose_num1 > best_sum:\n",
    "        bias += step\n",
    "        best_bias = bias\n",
    "        best_sum = choose_num + choose_num1\n",
    "    else:\n",
    "        bias += step\n",
    "        #best_bias = bias\n",
    "        #break\n",
    "    if bias >= 2.0:\n",
    "        break\n",
    "    last_ratio = ratio\n",
    "    last_ratio1 = ratio1\n",
    "    last_choose_num = choose_num\n",
    "    last_choose_num1 = choose_num1\n",
    "    #print(bias, ratio, ratio1, choose_num, choose_num1, choose_num+choose_num1)\n",
    "print(\"best_bias:\", best_bias)\n",
    "#print(\"svm model:\")\n",
    "print(\"fuzzy_logic_infer:\")\n",
    "print(\"ratio:\", ratio, \"ratio1:\", ratio1)\n",
    "print(\"total info:\", choose_num+choose_num1, (choose_num+choose_num1)/(len(only_spectral_right_pairs)+len(only_spatial_right_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "show_num = 1\n",
    "y_show = all_y_tests[show_num].values*2 - 1\n",
    "plt.plot(np.arange(len(all_score_spectral_norms[show_num])), all_score_spectral_norms[show_num], 'g.')\n",
    "plt.plot(np.arange(len(all_score_spectral_norms[show_num])), all_score_spatial_norms[show_num], 'r')\n",
    "plt.plot(np.arange(len(all_score_spectral_norms[show_num])), y_show, '*')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train a model for ensemble by spectral/spatial accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "change seed = np.random.seed(10) to generate training data\n",
    "\"\"\"\n",
    "all_score_spectral_norms=np.array(all_score_spectral_norms).squeeze().reshape(-1, 1)\n",
    "all_score_spatial_norms=np.array(all_score_spatial_norms).squeeze().reshape(-1, 1)\n",
    "train_data = np.concatenate([all_score_spectral_norms, all_score_spatial_norms], 1)\n",
    "train_label = []\n",
    "for xxx in all_y_tests:\n",
    "    train_label += xxx.values.tolist()\n",
    "len(train_label), train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1887, (1887, 2))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=[]\n",
    "train_label = []\n",
    "for pair in only_sprectral_right_pairs:\n",
    "    train_data.append([pair[0], pair[1]])\n",
    "    train_label.append(0)\n",
    "\n",
    "for pair in only_spatial_right_pairs:\n",
    "    train_data.append([pair[0], pair[1]])\n",
    "    train_label.append(1)\n",
    "train_data = np.array(train_data)\n",
    "len(train_label), train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Software\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8791732909379968"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "#squared_hinge hinge False\n",
    "svc_model = LinearSVC(C=10.0, random_state=42, penalty='l2', loss='hinge', dual=True)\n",
    "svc_model.fit(train_data, train_label)\n",
    "svc_model.score(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.3042,  0.0054]), array([0]), 0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0], svc_model.predict([train_data[0]]), train_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reault: 0.966\n",
      "Configuration: {'shrinkage': 'auto', 'solver': 'eigen'}\n",
      "(12600, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Software\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=2, random_state=42)\n",
    "\n",
    "grid = dict()\n",
    "grid['solver'] = ['eigen']\n",
    "shrinkages = np.linspace(0.1, 0.9, 10).tolist()\n",
    "#shrinkages = np.linspace(0.62, 0.64, 10).tolist()\n",
    "shrinkages = np.linspace(0.01, 0.1, 10).tolist()\n",
    "#shrinkages = np.linspace(0.001, 0.01, 10).tolist()\n",
    "shrinkages = np.linspace(0.000001, 0.00001, 10).tolist()\n",
    "\n",
    "grid['shrinkage'] = ['auto'] + shrinkages\n",
    "#search = GridSearchCV(LDA, grid, scoring='precision', cv=cv, n_jobs=-1)\n",
    "search = GridSearchCV(LDA, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# only_spatial_features  only_spatial_features_norm\n",
    "# X_with_spatial_features X_with_spatial_features_norm\n",
    "# X_norm_with_spatial_features_norm X_norm\n",
    "# only_spatial_features_norm_ave\n",
    "# X\n",
    "used_feature = train_data\n",
    "results = search.fit(used_feature, train_label)\n",
    "print('reault: %.3f' % results.best_score_)\n",
    "print('Configuration:',results.best_params_)\n",
    "print(used_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0\n",
      "count  10.000000\n",
      "mean    0.972222\n",
      "std     0.023139\n",
      "min     0.944444\n",
      "25%     0.948413\n",
      "50%     0.976190\n",
      "75%     0.994048\n",
      "max     1.000000\n"
     ]
    }
   ],
   "source": [
    "show_statistics(accuracy_ensemble)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=30.92230963449535, pvalue=1.1299642198011607e-52, df=99)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "stats.ttest_rel(accuracy_spectrals, accuracy_spatials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=-6.650454379738139, pvalue=1.6200024805567017e-09, df=99)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_rel(accuracy_spectrals, accuracy_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-3.7163234396153975, pvalue=0.00026298441219991126)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(accuracy_spectrals, accuracy_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=-37.635072960917725, pvalue=1.8418585152070884e-60, df=99)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_rel(accuracy_spatials, accuracy_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F_onewayResult(statistic=836.0034700315449, pvalue=1.020038496149818e-122)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import f_oneway\n",
    "f_oneway(accuracy_spectrals, accuracy_spatials, accuracy_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.960317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.849206</td>\n",
       "      <td>0.976190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.976190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.968254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.936508</td>\n",
       "      <td>0.849206</td>\n",
       "      <td>0.968254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.849206</td>\n",
       "      <td>0.936508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.960317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2\n",
       "0   0.960317  0.888889  0.960317\n",
       "1   0.968254  0.849206  0.976190\n",
       "2   0.976190  0.880952  0.984127\n",
       "3   0.928571  0.880952  0.944444\n",
       "4   0.968254  0.888889  0.984127\n",
       "..       ...       ...       ...\n",
       "95  0.968254  0.880952  0.976190\n",
       "96  0.952381  0.888889  0.968254\n",
       "97  0.936508  0.849206  0.968254\n",
       "98  0.928571  0.849206  0.936508\n",
       "99  0.952381  0.841270  0.960317\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([accuracy_spectrals, accuracy_spatials, accuracy_ensemble])\n",
    "df = df.T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i_data in range(df.shape[1]):\n",
    "    stats.probplot(df[i_data], dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Probability Plot - \" +  str(i_data))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6255747139657795"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = df.std(0).max() / df.std(0).min()\n",
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000367\n",
       "1    0.000739\n",
       "2    0.000265\n",
       "dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.std(0)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SS</th>\n",
       "      <th>df</th>\n",
       "      <th>MS</th>\n",
       "      <th>F</th>\n",
       "      <th>P-value</th>\n",
       "      <th>F crit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source of Variation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Between Groups</th>\n",
       "      <td>0.019222</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009611</td>\n",
       "      <td>405.461683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.832792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Within Groups</th>\n",
       "      <td>0.002299</td>\n",
       "      <td>97</td>\n",
       "      <td>0.000024</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.021521</td>\n",
       "      <td>99</td>\n",
       "      <td>0.000217</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           SS  df        MS           F P-value    F crit\n",
       "Source of Variation                                                      \n",
       "Between Groups       0.019222   2  0.009611  405.461683     0.0  3.832792\n",
       "Within Groups        0.002299  97  0.000024                              \n",
       "Total                0.021521  99  0.000217                              "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ANOVA backbone table\n",
    "data = [['Between Groups', '', '', '', '', '', ''], ['Within Groups', '', '', '', '', '', ''], ['Total', '', '', '', '', '', '']] \n",
    "anova_table = pd.DataFrame(data, columns = ['Source of Variation', 'SS', 'df', 'MS', 'F', 'P-value', 'F crit']) \n",
    "anova_table.set_index('Source of Variation', inplace = True)\n",
    "\n",
    "num_sample = df.shape[0]\n",
    "num_cls = df.shape[1]\n",
    "# calculate SSTR and update anova table\n",
    "SSTR = num_cls * (df.mean(0) - df.mean().mean())**2\n",
    "anova_table['SS']['Between Groups'] = SSTR.sum()\n",
    "\n",
    "# calculate SSE and update anova table\n",
    "#tmp = 0\n",
    "#for i_data in range(len(data)):\n",
    "#    tmp += (data.iloc[i_data]- data.iloc[i_data].std())**2\n",
    "#SSE = (data.count() - 1) * tmp.sum()\n",
    "\n",
    "SSE = (num_cls - 1) * df.std(0)**2\n",
    "anova_table['SS']['Within Groups'] = SSE.sum()\n",
    "\n",
    "# calculate SSTR and update anova table\n",
    "SSTR = SSTR.sum() + SSE.sum()\n",
    "anova_table['SS']['Total'] = SSTR\n",
    "\n",
    "# update degree of freedom\n",
    "anova_table['df']['Between Groups'] = num_cls - 1\n",
    "anova_table['df']['Within Groups'] = num_sample - num_cls\n",
    "anova_table['df']['Total'] = num_sample - 1\n",
    "\n",
    "# calculate MS\n",
    "anova_table['MS'] = anova_table['SS'] / anova_table['df']\n",
    "\n",
    "# calculate F \n",
    "F = anova_table['MS']['Between Groups'] / anova_table['MS']['Within Groups']\n",
    "anova_table['F']['Between Groups'] = F\n",
    "\n",
    "# p-value\n",
    "anova_table['P-value']['Between Groups'] = 1 - stats.f.cdf(F, anova_table['df']['Between Groups'], anova_table['df']['Within Groups'])\n",
    "\n",
    "# F critical \n",
    "alpha = 0.05\n",
    "# possible types \"right-tailed, left-tailed, two-tailed\"\n",
    "tail_hypothesis_type = \"two-tailed\"\n",
    "if tail_hypothesis_type == \"two-tailed\":\n",
    "    alpha /= 2\n",
    "anova_table['F crit']['Between Groups'] = stats.f.ppf(1-alpha, anova_table['df']['Between Groups'], anova_table['df']['Within Groups'])\n",
    "\n",
    "# Final ANOVA Table\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0016115000713620834"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LSD = t.025, DFw * √MSW(1/n1 + 1/n1)\n",
    "LSD = 2.326 * (0.000024*(1/df.shape[0] + 1/df.shape[0]))**0.5\n",
    "LSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09333333333333316"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(df[0].mean() - df[1].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10214285714285687"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(df[1].mean() - df[2].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008809523809523712"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(df[0].mean() - df[2].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach 1: The p-value approach to hypothesis testing in the decision rule\n",
      "F-score is: 405.4616829652971  and p value is: 1.1102230246251565e-16\n",
      "Null Hypothesis is rejected.\n",
      "\n",
      "--------------------------------------------------------------------------------------\n",
      "Approach 2: The critical value approach to hypothesis testing in the decision rule\n",
      "F-score is: 405.4616829652971  and critical value is: 3.8327917620793706\n",
      "Null Hypothesis is rejected.\n"
     ]
    }
   ],
   "source": [
    "# The p-value approach\n",
    "print(\"Approach 1: The p-value approach to hypothesis testing in the decision rule\")\n",
    "conclusion = \"Failed to reject the null hypothesis.\"\n",
    "if anova_table['P-value']['Between Groups'] <= alpha:\n",
    "    conclusion = \"Null Hypothesis is rejected.\"\n",
    "print(\"F-score is:\", anova_table['F']['Between Groups'], \" and p value is:\", anova_table['P-value']['Between Groups'])    \n",
    "print(conclusion)\n",
    "    \n",
    "# The critical value approach\n",
    "print(\"\\n--------------------------------------------------------------------------------------\")\n",
    "print(\"Approach 2: The critical value approach to hypothesis testing in the decision rule\")\n",
    "conclusion = \"Failed to reject the null hypothesis.\"\n",
    "if anova_table['F']['Between Groups'] > anova_table['F crit']['Between Groups']:\n",
    "    conclusion = \"Null Hypothesis is rejected.\"\n",
    "print(\"F-score is:\", anova_table['F']['Between Groups'], \" and critical value is:\", anova_table['F crit']['Between Groups'])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0\n",
      "count  1000.000000\n",
      "mean      0.961357\n",
      "std       0.016712\n",
      "min       0.904762\n",
      "25%       0.952381\n",
      "50%       0.960317\n",
      "75%       0.976190\n",
      "max       1.000000\n",
      "                 0\n",
      "count  1000.000000\n",
      "mean      0.863659\n",
      "std       0.026149\n",
      "min       0.777778\n",
      "25%       0.849206\n",
      "50%       0.865079\n",
      "75%       0.880952\n",
      "max       0.936508\n",
      "                 0\n",
      "count  1000.000000\n",
      "mean      0.968246\n",
      "std       0.014980\n",
      "min       0.920635\n",
      "25%       0.960317\n",
      "50%       0.968254\n",
      "75%       0.976190\n",
      "max       1.000000\n"
     ]
    }
   ],
   "source": [
    "show_statistics(accuracy_spectrals)\n",
    "show_statistics(accuracy_spatials)\n",
    "show_statistics(accuracy_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.959, 0.224, 0.001)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_num = 0\n",
    "bad_cases = []\n",
    "good_cases = []\n",
    "for i_acc, final_accuracy in enumerate(accuracy_ensemble):\n",
    "    if final_accuracy >= max(accuracy_spectrals[i_acc], accuracy_spatials[i_acc]):\n",
    "        valid_num += 1\n",
    "        gap = final_accuracy - max(accuracy_spectrals[i_acc], accuracy_spatials[i_acc])\n",
    "        if gap > 0.01:\n",
    "            good_cases.append({'f': final_accuracy, 'sprectral': accuracy_spectrals[i_acc], 'spatial': accuracy_spatials[i_acc]})\n",
    "    else:\n",
    "        gap = max(accuracy_spectrals[i_acc], accuracy_spatials[i_acc]) - final_accuracy\n",
    "        if gap > 0.01:\n",
    "            bad_cases.append({'f': final_accuracy, 'sprectral': accuracy_spectrals[i_acc], 'spatial': accuracy_spatials[i_acc]})\n",
    "valid_num / len(accuracy_ensemble), len(good_cases) / len(accuracy_ensemble), len(bad_cases) / len(accuracy_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'f': 0.9523809523809523,\n",
       "  'sprectral': 0.9682539682539683,\n",
       "  'spatial': 0.9047619047619048}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015873015873015928"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9682539682539683-0.9523809523809523\n",
    "#0.9761904761904762-0.9682539682539683\n",
    "#0.9523809523809523-0.9444444444444444"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = pd.concat([X, label], axis= 1)\n",
    "xxx = dict(list(df.groupby('label')))\n",
    "yyy = xxx[0].mean(axis=0)\n",
    "yyy.drop(['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125274</td>\n",
       "      <td>0.112998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.124180</td>\n",
       "      <td>0.111487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.122471</td>\n",
       "      <td>0.109218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.120787</td>\n",
       "      <td>0.107771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.120364</td>\n",
       "      <td>0.107212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>0.270462</td>\n",
       "      <td>0.237536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>0.270214</td>\n",
       "      <td>0.236737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>0.276498</td>\n",
       "      <td>0.243368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>0.277633</td>\n",
       "      <td>0.243970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>0.278975</td>\n",
       "      <td>0.245444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>462 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1\n",
       "0    0.125274  0.112998\n",
       "1    0.124180  0.111487\n",
       "2    0.122471  0.109218\n",
       "3    0.120787  0.107771\n",
       "4    0.120364  0.107212\n",
       "..        ...       ...\n",
       "457  0.270462  0.237536\n",
       "458  0.270214  0.236737\n",
       "459  0.276498  0.243368\n",
       "460  0.277633  0.243970\n",
       "461  0.278975  0.245444\n",
       "\n",
       "[462 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = pd.concat([X, label], axis= 1)\n",
    "\n",
    "class_feature_means = pd.DataFrame(columns=[0, 1])\n",
    "for cls, rows in df.groupby('label'):\n",
    "    class_feature_means[cls] = rows.mean(axis=0)\n",
    "class_feature_means = class_feature_means.drop(['label'])\n",
    "class_feature_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = len(class_feature_means)\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_class_scatter_matrix = np.zeros((dim,dim))\n",
    "for c, rows in df.groupby('label'):\n",
    "    rows = rows.drop(['label'], axis=1)\n",
    "    s = np.zeros((dim,dim))\n",
    "    for index, row in rows.iterrows():\n",
    "        x  = row.values.reshape(dim, 1)\n",
    "        mc  = class_feature_means[c].values.reshape(dim,1)\n",
    "        s += (x - mc).dot((x - mc).T)\n",
    "        within_class_scatter_matrix += s\n",
    "within_class_scatter_matrix_show = pd.DataFrame(within_class_scatter_matrix)\n",
    "within_class_scatter_matrix_show.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_means = df.mean().drop(['label'])\n",
    "between_class_scatter_matrix = np.zeros((dim,dim))\n",
    "for c in class_feature_means:    \n",
    "    n = len(df.loc[df['label'] == c].index)\n",
    "    \n",
    "    mc, m = class_feature_means[c].values.reshape(dim,1), feature_means.values.reshape(dim,1)\n",
    "    \n",
    "    between_class_scatter_matrix += n * (mc - m).dot((mc - m).T)\n",
    "\n",
    "between_class_scatter_matrix_show = pd.DataFrame(between_class_scatter_matrix)\n",
    "between_class_scatter_matrix_show.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values, eigen_vectors = np.linalg.eig(np.linalg.inv(within_class_scatter_matrix).dot(between_class_scatter_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(np.abs(eigen_values[i]), eigen_vectors[:,i]) for i in range(len(eigen_values))]\n",
    "pairs = sorted(pairs, key=lambda x: x[0], reverse=True)\n",
    "for pair in pairs:\n",
    "    print(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_value_sums = sum(eigen_values)\n",
    "print('Explained Variance')\n",
    "for i, pair in enumerate(pairs):\n",
    "    print('Eigenvector {}: {}'.format(i, (pair[0]/eigen_value_sums).real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_matrix = np.hstack((pairs[0][1].reshape(dim,1), pairs[1][1].reshape(dim,1))).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda = np.array(X.dot(w_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_lda.shape[1] >= 2:\n",
    "    plt.xlabel('LD1')\n",
    "    plt.ylabel('LD2')\n",
    "    plt.scatter(\n",
    "        X_lda[:,0],\n",
    "        X_lda[:,1],\n",
    "        c=y,\n",
    "        cmap='rainbow',\n",
    "        alpha=0.5,\n",
    "        edgecolors='b'\n",
    "    )\n",
    "    plt.show(block=True)\n",
    "print(X_lda[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLS-DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "wl = np.linspace(393, 1011, x.shape[1])\n",
    "with plt.style.context('ggplot'):\n",
    "    plt.plot(wl, x.T)\n",
    "    plt.xlabel(\"Wavelengths (nm)\")\n",
    "    plt.ylabel(\"Reflectance\")\n",
    "    plt.show(block=True)\n",
    "#X2 = savgol_filter(x, 17, polyorder=2, deriv=2)\n",
    "\n",
    "#plt.figure(figsize=(8, 4.5))\n",
    "#with plt.style.context('ggplot'):\n",
    "#    plt.plot(wl, X2.T)\n",
    "#    plt.xlabel(\"Wavelengths (nm)\")\n",
    "#    plt.ylabel(\"D2 Absorbance\")\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_pls_cv(X, y, n_comp):\n",
    "    # Define PLS object\n",
    "    pls = PLSRegression(n_components=n_comp)\n",
    "\n",
    "    # Cross-validation\n",
    "    y_cv = cross_val_predict(pls, X, y, cv=10)\n",
    "\n",
    "    # Calculate scores\n",
    "    r2 = r2_score(y, y_cv)\n",
    "    mse = mean_squared_error(y, y_cv)\n",
    "    rpd = y.std()/np.sqrt(mse)\n",
    "    \n",
    "    return (y_cv, r2, mse, rpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:27<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# test with 40 components\n",
    "from tqdm import tqdm\n",
    "r2s = []\n",
    "mses = []\n",
    "rpds = []\n",
    "xticks = np.arange(1, 41)\n",
    "for n_comp in tqdm(xticks):\n",
    "    y_cv, r2, mse, rpd = optimise_pls_cv(x, y, n_comp)\n",
    "    r2s.append(r2)\n",
    "    mses.append(mse)\n",
    "    rpds.append(rpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mses\n",
    "def plot_metrics(vals, ylabel, objective):\n",
    "    with plt.style.context('ggplot'):\n",
    "        plt.plot(xticks, np.array(vals), '-v', color='blue', mfc='blue')\n",
    "        if objective=='min':\n",
    "            idx = np.argmin(vals)\n",
    "        else:\n",
    "            idx = np.argmax(vals)\n",
    "        plt.plot(xticks[idx], np.array(vals)[idx], 'P', ms=10, mfc='red')\n",
    "\n",
    "        plt.xlabel('Number of PLS components')\n",
    "        plt.xticks = xticks\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title('PLS')\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(mses, 'MSE', 'min')\n",
    "plot_metrics(rpds, 'RPD', 'max')\n",
    "plot_metrics(r2s, 'R2', 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.4420, MSE: 0.1395, RPD: 1.3387\n"
     ]
    }
   ],
   "source": [
    "#y_cv, r2, mse, rpd = optimise_pls_cv(X2, y, 6)\n",
    "y_cv, r2, mse, rpd = optimise_pls_cv(x, y, 17)\n",
    "print('R2: %0.4f, MSE: %0.4f, RPD: %0.4f' %(r2, mse, rpd))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "with plt.style.context('ggplot'):\n",
    "    plt.scatter(y, y_cv, color='red')\n",
    "    plt.plot(y, y, '-g', label='Expected regression line')\n",
    "    z = np.polyfit(y, y_cv, 1)\n",
    "    plt.plot(np.polyval(z, y), y, color='blue', label='Predicted regression line')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.legend()\n",
    "    plt.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix,recall_score,classification_report,accuracy_score,precision_score\n",
    "\n",
    "def accuracy_component(xc, yc, xv, yv, component, n_fold, random_state):\n",
    "    k_range = np.linspace(1, component,component)\n",
    "\n",
    "    kf=KFold(n_splits=n_fold, random_state=random_state, shuffle=True)\n",
    "    accuracy_validation=np.zeros((component,))\n",
    "    accuracy_test=np.zeros((component,))\n",
    "    precision_test=np.zeros((component,))\n",
    "    recall_test=np.zeros((component,))\n",
    "\n",
    "    best_p = 0\n",
    "    best_r = 0\n",
    "    best_a = 0\n",
    "    for j in range(component):\n",
    "        p=0\n",
    "        acc=0\n",
    "        model_pls=PLSRegression(n_components=j+1)\n",
    "        \n",
    "        yc_labels = pd.get_dummies(yc)\n",
    "        \n",
    "        model_pls.fit(xc,yc_labels)\n",
    "        y_pred = model_pls.predict(xv)\n",
    "        \n",
    "        y_pred = np.array([np.argmax(i) for i in y_pred])\n",
    "\n",
    "        accuracy_test[j]=accuracy_score(yv,y_pred)\n",
    "        precision_test[j]=recall_score(yv,y_pred)\n",
    "        recall_test[j]=precision_score(yv,y_pred)\n",
    "        for train_index, test_index in kf.split(xc):\n",
    "            X_train, X_test = xc[train_index], xc[test_index]\n",
    "            y_train, y_test = yc[train_index], yc[test_index]\n",
    "            \n",
    "            YC_labels = pd.get_dummies(y_train)\n",
    "            YV_labels=pd.get_dummies(y_test)\n",
    "            model_1=PLSRegression(n_components=j+1)\n",
    "            model_1.fit(X_train,YC_labels)\n",
    "            Y_pred = model_1.predict(X_test)\n",
    "            Y_pred = np.array([np.argmax(i1) for i1 in Y_pred])\n",
    "            acc=accuracy_score(y_test,Y_pred)+acc\n",
    "            p=p+1\n",
    "        accuracy_validation[j]=acc/p\n",
    "    #print(accuracy_validation)\n",
    "\n",
    "    #plt.plot(k_range, accuracy_validation.T, 'o-',label=\"Cross-validation\",color=\"r\")\n",
    "    #plt.plot(k_range, accuracy_test.T, 'o-',label=\"Test\",color=\"b\")\n",
    "    #plt.xlabel(\"N components\")\n",
    "    #plt.ylabel(\"Accuracy\")\n",
    "    #plt.legend(loc=\"best\")\n",
    "    #plt.rc('font',family='Times New Roman')\n",
    "    #plt.rcParams['font.size'] = 10\n",
    "    #plt.show()\n",
    "    precision_test=np.array(precision_test)\n",
    "    recall_test=np.array(recall_test)\n",
    "    accuracy_test=np.array(accuracy_test)\n",
    "    idx = np.argmax(accuracy_test)\n",
    "    print(idx+1, precision_test[idx], recall_test[idx], accuracy_test[idx])\n",
    "    best_p = precision_test[idx]\n",
    "    best_r = recall_test[idx]\n",
    "    best_a = accuracy_test[idx]\n",
    "    return best_p, best_r, best_a\n",
    "#seed_int = 0\n",
    "#np.random.seed(seed_int)\n",
    "##np.random.seed(int(time.time()))\n",
    "#random_state = np.random.randint(0, 1e4)\n",
    "#print('random_state:', random_state)\n",
    "#accuracy_component(X_train, y_train, X_test, y_test, component=30, n_fold=4, random_state=random_state)\n",
    "#x=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_state: 0\n",
      "21 0.873015873015873 0.9322033898305084 0.9047619047619048\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "#np.random.seed(int(time.time()))\n",
    "#random_state = np.random.randint(0, 1e4)\n",
    "random_state=0\n",
    "#X_=X.values\n",
    "#X_norm_=X_norm.values\n",
    "#label_=label.values\n",
    "\n",
    "#X_norm\n",
    "#spatial_features_norm\n",
    "#X_norm_with_spatial_features_norm\n",
    "\n",
    "#X_norm_with_spatial_features_norm_MRMR_norm\n",
    "#X_norm_mrmr_with_spatial_features_norm_mrmr\n",
    "\n",
    "#X_norm_with_spatial_features_norm_PCA_norm\n",
    "X_norm_ = pd.DataFrame(X_norm_mrmr_with_spatial_features_norm_mrmr).values\n",
    "label_ = pd.DataFrame(label).values\n",
    "\n",
    "seed_int = 0\n",
    "np.random.seed(seed_int)\n",
    "#np.random.seed(int(time.time()))\n",
    "random_state = 0\n",
    "#random_state = np.random.randint(0, 1e4)\n",
    "best_ps, best_rs, best_as = [], [], []\n",
    "print('random_state:', random_state)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_norm_, label_, test_size=0.30, random_state=random_state, shuffle=True, stratify=label)\n",
    "y_train=y_train.squeeze()\n",
    "y_test=y_test.squeeze()\n",
    "component = min(X_norm_.shape[1], 35)\n",
    "best_p, best_r, best_a = accuracy_component(X_train, y_train, X_test, y_test, component=component, n_fold=4, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start show results\n",
      "(420, 69) (294, 69) (126, 69) \n",
      "\n",
      "precision:\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.893216\n",
      "std      0.036219\n",
      "min      0.771930\n",
      "25%      0.870370\n",
      "50%      0.892857\n",
      "75%      0.917500\n",
      "max      0.963636 \n",
      "\n",
      "recall:\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.802540\n",
      "std      0.047716\n",
      "min      0.682540\n",
      "25%      0.777778\n",
      "50%      0.809524\n",
      "75%      0.841270\n",
      "max      0.904762 \n",
      "\n",
      "accuracy:\n",
      "                0\n",
      "count  100.000000\n",
      "mean     0.852619\n",
      "std      0.028499\n",
      "min      0.746032\n",
      "25%      0.833333\n",
      "50%      0.853175\n",
      "75%      0.873016\n",
      "max      0.912698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 28 7 6\n",
    "# 28 5 6 22 1\n",
    "# 28 5 13 5 1\n",
    "# 28 5 13 21 1\n",
    "model_params={'n_components': 21}\n",
    "np.random.seed(0)\n",
    "seed=True\n",
    "#X_norm\n",
    "#spatial_features_norm\n",
    "#X_norm_with_spatial_features_norm\n",
    "\n",
    "#X_norm_with_spatial_features_norm_MRMR_norm\n",
    "#X_norm_mrmr_with_spatial_features_norm_mrmr\n",
    "\n",
    "#X_norm_with_spatial_features_norm_PCA_norm\n",
    "X_norm_ = pd.DataFrame(X_norm_mrmr_with_spatial_features_norm_mrmr).values\n",
    "label_ = pd.DataFrame(label).values\n",
    "_ = repeated_tests(PLSRegression, X_norm_, label, split_ratio=0.3, test_num=100, seed=seed, model_params=model_params, one_hot_y=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA + DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = finalDf[['PC1', 'PC2', 'PC3']]\n",
    "y = finalDf[['class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_norm = StandardScaler().fit_transform(X)\n",
    "X = x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14,  7],\n",
       "       [ 7, 25]], dtype=int64)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "conf_mat, (conf_mat[0,0]+conf_mat[1,1])/conf_mat.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA + LogisticRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = finalDf[['PC1', 'PC2', 'PC3']]\n",
    "y = finalDf[['class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, shuffle=True, stratify=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=0.95)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=0.95)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=0.95)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 3)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all parameters not specified are set to their defaults\n",
    "# default solver is incredibly slow which is why it was changed to 'lbfgs'\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Software\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.predict(X_test[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.predict(X_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6792452830188679"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mahotas\n",
    "# load Data section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/BoyangDeng/BlueberryClassification/datasets/BlueberryScansforDestructiveTesting06132022/BadBlueberryScans/im_data_local/18')"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dirs = []\n",
    "for dir_name in save_dir_im_data_bad.iterdir():\n",
    "    sample_dirs.append(dir_name)\n",
    "sample_dirs = sorted(sample_dirs, key=lambda x:int(x.name))\n",
    "sample_idx=17\n",
    "sample_dirs[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mahotas\n",
    "import cv2\n",
    "path = (sample_dirs[sample_idx] / (str(100)+'_'+str(wavelengths[100])+'.png')).__str__()\n",
    "im = cv2.imread(path)\n",
    "\n",
    "im_color = cv2.applyColorMap(im, cv2.COLORMAP_JET )\n",
    "cv2.namedWindow('x', 0)\n",
    "cv2.imshow('x', im_color)\n",
    "cv2.waitKey(1)\n",
    "gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#import plotly.express as px\n",
    "#fig = px.imshow(im)\n",
    "##fig.update_traces(showscale=False)\n",
    "#fig.update_layout(yaxis={'visible': False, 'showticklabels': False},\n",
    "#                  xaxis={'visible': False, 'showticklabels': False})\n",
    "#fig.write_image(r\"F:\\test\\show_spatial.png\", width=1400, height=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## haralick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "features = mahotas.features.haralick(gray)#.mean(axis=0)\n",
    "im.shape, features.shape\n",
    "mean_feature=features.mean(axis=0)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\t\t\t\t\n",
    "fig.add_trace(go.Bar(\n",
    "    y=mean_feature,\n",
    "    x=np.arange(len(mean_feature)),\n",
    "))\n",
    "fig=plotly_fig_config(fig,title='Haralick Features')\n",
    "fig.show()\n",
    "#fig.write_image(os.path.join(tmp_save_dir,\"show_spatial_haralick.png\"), width=700, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 28),\n",
       " array([[  0.2191,   0.2735, 143.5274,   0.2735,   0.8835,   3.9465,\n",
       "           2.2357,   1.7108,   1.9167,   0.0744,   0.5841,  -0.5117,\n",
       "           0.7312,   0.7942,   0.2259,   0.338 , 147.2086,   0.338 ,\n",
       "           0.8933,   3.9641,   2.2676,   1.728 ,   1.9795,   0.0774,\n",
       "           0.6454,   0.5518,   0.7596,   0.8299]]))"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = {}\n",
    "options['dharalick'] = 3\n",
    "X  = eng.Bfx_haralick(gray, [], options)\n",
    "X = np.asarray(X)\n",
    "X.shape, X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gabor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create our gabor filters, and then apply them to our image\n",
    "from modeling_spatial_features import create_gaborfilter, apply_filter\n",
    "import cv2\n",
    "gfilters = create_gaborfilter()\n",
    "image_g = apply_filter(im, gfilters)\n",
    "\n",
    "im_color = cv2.applyColorMap(image_g, cv2.COLORMAP_JET )\n",
    "cv2.namedWindow('x', 1)\n",
    "cv2.imshow('x', im_color)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 67),\n",
       " array([[ 64.0406,  89.274 ,   3.5201,   3.7964,   5.6593,  15.7901,\n",
       "          33.2222, 122.9141,  87.1522,  74.7226,   3.5489,   3.6135,\n",
       "           5.3465,  14.8141,  34.9597, 107.5493,  95.2862,  59.3939,\n",
       "          14.8206,   3.4314,   4.7465,   9.6991,  32.7083,  86.3802,\n",
       "          87.1502,  74.7183,   3.781 ,   3.8601,   5.6948,  15.3251,\n",
       "          35.3988, 110.6253,  64.0736,  89.2555,   3.8249,   4.2491,\n",
       "           6.875 ,  16.7535,  33.619 , 126.7573,  87.1502,  74.7173,\n",
       "           3.8164,   4.0252,   6.4123,  16.4903,  39.4573, 113.7675,\n",
       "          95.2905,  59.4307,  14.8756,   3.5164,   5.2442,  11.0141,\n",
       "          37.5938,  90.9629,  87.1521,  74.723 ,   3.546 ,   3.5923,\n",
       "           5.3221,  15.0155,  38.0228, 110.0138, 126.7573,   3.4314,\n",
       "          35.9408]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = {}\n",
    "options['Lgabor'] = np.float32(8)\n",
    "options['Sgabor'] = np.float32(8)\n",
    "options['fhgabor'] = np.float32(2)\n",
    "options['flgabor'] = np.float32(0.1)\n",
    "options['Mgabor'] = np.float32(21)\n",
    "options['show'] = 1\n",
    "X  = eng.Bfx_gabor(gray, options)\n",
    "X = np.asarray(X)\n",
    "X.shape, X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_spatial_features import LocalBinaryPatterns\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "#desc = LocalBinaryPatterns(24, 8)\n",
    "desc = LocalBinaryPatterns(24, 3)\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0384, 0.0217, 0.0165, 0.0128, 0.0133, 0.014 , 0.0155, 0.0199,\n",
       "       0.0237, 0.0295, 0.0363, 0.0395, 0.0417, 0.0404, 0.0266, 0.0228,\n",
       "       0.0181, 0.0172, 0.0149, 0.0127, 0.0133, 0.0121, 0.0158, 0.02  ,\n",
       "       0.0441, 0.4189])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "hist = desc.describe(gray)\n",
    "data.append(hist)\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\t\t\t\t\n",
    "fig.add_trace(go.Bar(\n",
    "    y=hist,\t\n",
    "    x=np.arange(len(hist)),\n",
    "))\n",
    "fig=plotly_fig_config(fig,title='LBP Features')\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(tmp_save_dir,\"show_spatial_LBP.png\"), width=700, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AFSALab\\AppData\\Local\\Temp\\ipykernel_3880\\3222111.py:5: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  options['samples'] = np.float(8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 59)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = {}\n",
    "options['vdiv'] = 1\n",
    "options['hdiv'] = 1\n",
    "options['semantic'] = 0\n",
    "options['samples'] = np.float(8)\n",
    "options['mappingtype'] = 'u2'\n",
    "X = eng.Bfx_lbp(gray, [], options)\n",
    "X = np.asarray(X)\n",
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BSIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling_spatial_features import bsif\n",
    "\n",
    "gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "img_bsif, hist = bsif(gray)\n",
    "\n",
    "img_bsif=cv.normalize(img_bsif,  img_bsif, 0, 255, cv.NORM_MINMAX)\n",
    "img_bsif=img_bsif.astype(np.uint8)\n",
    "im_color = cv2.applyColorMap(img_bsif, cv2.COLORMAP_JET)\n",
    "cv2.namedWindow('x', 1)\n",
    "cv2.imshow('x', im_color)\n",
    "cv2.waitKey()\n",
    "#show images\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 8), sharex=True, sharey=True)\n",
    "#plt.subplots_adjust(0.15)\n",
    "#axes[0].imshow(gray, cmap=\"gray\")\n",
    "#axes[0].axis('off')\n",
    "#axes[0].set_title('Original Image', fontsize=20)\n",
    "#axes[1].imshow(img_bsif,cmap=\"gray\")\n",
    "#axes[1].axis('off')\n",
    "#axes[1].set_title('BSIF Image', fontsize=20)\n",
    "#fig.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#failed\n",
    "options = {}\n",
    "options['vdiv'] = np.float32(1)\n",
    "options['hdiv'] = np.float32(1)\n",
    "options['filter'] = np.float32(7)\n",
    "options['bits'] = np.float32(11)\n",
    "options['mode'] = 'h'\n",
    "X  = eng.Bfx_bsif(gray, [], options)\n",
    "X = np.asarray(X)\n",
    "X.shape, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spatial_feature_BSIF\n",
    "X = spatial_feature_BSIF.bsif(gray, mode='nh')\n",
    "X.shape, X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gray = gray.reshape(1, -1)\n",
    "gray = np.concatenate([gray, gray])\n",
    "\"\"\"\n",
    "need many samples\n",
    "\"\"\"\n",
    "transformer = FastICA(n_components=7,\n",
    "        random_state=0,\n",
    "        whiten='unit-variance')\n",
    "X_transformed = transformer.fit_transform(gray)\n",
    "X_transformed.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hu-Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0036],\n",
       "       [ 0.    ],\n",
       "       [ 0.    ],\n",
       "       [ 0.    ],\n",
       "       [ 0.    ],\n",
       "       [-0.    ],\n",
       "       [ 0.    ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#skimage.measure.moments_hu \n",
    "gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "#_,gray_t = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "M = cv2.moments(gray)\n",
    "huMoments = cv2.HuMoments(M)\n",
    "huMoments\n",
    "#xxx=huMoments.squeeze()\n",
    "#import plotly.graph_objects as go\n",
    "#fig = go.Figure()\t\t\t\t\n",
    "#fig.add_trace(go.Bar(\n",
    "#    y=xxx,\n",
    "#    x=np.arange(len(xxx)),\n",
    "#))\n",
    "#fig=plotly_fig_config(fig,title='Hu-Moments Features')\n",
    "#fig.show()\n",
    "#fig.write_image(os.path.join(tmp_save_dir,\"show_spatial_huMoments.png\"), width=700, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.4447],\n",
       "       [  5.8302],\n",
       "       [  8.3021],\n",
       "       [  8.8751],\n",
       "       [ 17.6909],\n",
       "       [-11.9522],\n",
       "       [ 17.5577]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0,7):\n",
    "   huMoments[i] = -1* np.copysign(1.0, huMoments[i]) * np.log10(abs(huMoments[i]))\n",
    "huMoments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that hu[0] is not comparable in magnitude as hu[6]. We can use use a log transform given below to bring them in the same range"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "central moments are translation invariant but not scale and rotation invariant  \n",
    "normalized central moments are translation and scale invariant but not rotation invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0036,  0.    ,  0.    ,  0.    ,  0.    , -0.    , -0.    ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skimage.measure import moments_hu, moments_normalized, moments_central\n",
    "gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "mu = moments_central(gray)\n",
    "nu = moments_normalized(mu)\n",
    "moments_hu(nu)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AFSALab\\.conda\\envs\\pytorch\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\AFSALab\\.conda\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.4SP5SUA7CBGXUEOC35YP2ASOICYYEQZZ.gfortran-win_amd64.dll\n",
      "c:\\Users\\AFSALab\\.conda\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\AFSALab\\.conda\\envs\\pytorch\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from modeling_spectral_data import load_spectral_mean_data_xy\n",
    "x, y = load_spectral_mean_data_xy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  pd.DataFrame(x)\n",
    "label = pd.DataFrame(y, columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 462)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.30, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(C=10.0, random_state=42)\n",
    "#model.fit(X_train, y_train)\n",
    "#model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "high time consumption\n",
    "10 featrure, 3 min\n",
    "128 featrure, 62min\n",
    "\"\"\"\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "n_features_to_select = 128\n",
    "sfs = SequentialFeatureSelector(model, n_features_to_select=n_features_to_select)\n",
    "sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 128)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sfs = sfs.transform(X_train)\n",
    "X_test_sfs = sfs.transform(X_test)\n",
    "X_train_sfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Software\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "f:\\Software\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8095238095238095"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(C=10.0, random_state=42)\n",
    "model.fit(X_train_sfs, y_train)\n",
    "model.score(X_test_sfs, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRMR\n",
    "minimum redundancy maximum relevance (MRMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X_norm\n",
    "spatial_features_norm\n",
    "X_norm_with_spatial_features_norm\n",
    "\"\"\"\n",
    "#eng.fsrmrmr(X_norm, label.values.astype(np.float32))\n",
    "idx_matlab = eng.fscmrmr(X_norm, label.values)\n",
    "idx_matlab = np.array(idx_matlab)\n",
    "feature_num = 303\n",
    "idx_spectral = idx_matlab[0, :feature_num]-1\n",
    "\n",
    "idx_matlab = eng.fscmrmr(spatial_features_norm, label.values)\n",
    "idx_matlab = np.array(idx_matlab)\n",
    "feature_num = 5\n",
    "idx_spatial = idx_matlab[0, :feature_num]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 308)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm_MRMR = X_norm[:, idx_spectral.astype(int)]\n",
    "spatial_features_norm_MRMR = spatial_features_norm[:, idx_spatial.astype(int)]\n",
    "X_norm_with_spatial_features_norm_MRMR = np.concatenate([X_norm_MRMR, spatial_features_norm_MRMR], 1)\n",
    "X_norm_with_spatial_features_norm_MRMR_norm = X_norm_with_spatial_features_norm_MRMR\n",
    "X_norm_with_spatial_features_norm_MRMR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((420, 303), (420, 5))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_X_norm_MRMR = r'D:\\BoyangDeng\\BlueberryClassification\\datasets\\mean_data_python\\X_norm_MRMR.npy'\n",
    "np.save(path_X_norm_MRMR, X_norm_MRMR)\n",
    "path_spatial_features_norm_MRMR = r'D:\\BoyangDeng\\BlueberryClassification\\datasets\\mean_data_python\\spatial_features_norm_MRMR.npy'\n",
    "np.save(path_spatial_features_norm_MRMR, spatial_features_norm_MRMR)\n",
    "X_norm_MRMR.shape, spatial_features_norm_MRMR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((420, 303), (420, 5))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm_MRMR = np.load(path_X_norm_MRMR, allow_pickle='TRUE')\n",
    "spatial_features_norm_MRMR = np.load(path_spatial_features_norm_MRMR, allow_pickle='TRUE')\n",
    "X_norm_MRMR.shape, spatial_features_norm_MRMR.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 28)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.8)\n",
    "X_norm_with_spatial_features_norm_PCA = pca.fit_transform(X_norm_with_spatial_features_norm)\n",
    "#X_norm_with_spatial_features_norm_PCA = X_norm_with_spatial_features_norm_PCA[:, pca.explained_variance_ratio_>0.01]\n",
    "X_norm_with_spatial_features_norm_PCA_norm = (X_norm_with_spatial_features_norm_PCA - X_norm_with_spatial_features_norm_PCA.mean(0)) / X_norm_with_spatial_features_norm_PCA.std(0)\n",
    "X_norm_with_spatial_features_norm_PCA_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3029, 0.1419, 0.0953, 0.0708, 0.0304, 0.0211, 0.017 , 0.0149,\n",
       "       0.0117, 0.0097, 0.0089, 0.008 , 0.0079, 0.0072, 0.0067, 0.006 ,\n",
       "       0.0052, 0.0045, 0.0041, 0.0039, 0.0036, 0.0034, 0.0032, 0.0031,\n",
       "       0.003 , 0.0029, 0.0027, 0.0027])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([459.6, 599.97, 690.66, 749.6, 839.76, 930.39], [50, 156, 224, 268, 335, 402])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_bands = [460, 600, 690, 750, 840, 930]\n",
    "target_values = []\n",
    "target_idxes = []\n",
    "for target in selected_bands:\n",
    "    nearest_value, nearest_idx= find_nearest(wavelengths, target)\n",
    "    target_values.append(nearest_value)\n",
    "    target_idxes.append(nearest_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_origin_data_good\n",
    "save_dir_im_data_good\n",
    "\n",
    "save_dir_origin_data_bad\n",
    "save_dir_im_data_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_1_42 good_43_84 good_dir\n",
    "# bad_1_42 bad_43_84 bad_85_126 bad_dir\n",
    "save_path_good = good_dir / 'good_1_42.npy'.__str__()\n",
    "save_path_good1 = good_dir / 'good_43_84.npy'.__str__()\n",
    "save_path_bad = bad_dir / 'bad_1_42.npy'.__str__()\n",
    "save_path_bad1 = bad_dir / 'bad_43_84.npy'.__str__()\n",
    "save_path_bad2 = bad_dir / 'bad_85_126.npy'.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_dict_good = np.load(save_path_good, allow_pickle='TRUE').item()\n",
    "hyper_dict_good1 = np.load(save_path_good1, allow_pickle='TRUE').item()\n",
    "hyper_dict_bad = np.load(save_path_bad, allow_pickle='TRUE').item()\n",
    "hyper_dict_bad1 = np.load(save_path_bad1, allow_pickle='TRUE').item()\n",
    "hyper_dict_bad2 = np.load(save_path_bad2, allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_dict_new = {}\n",
    "for key in hyper_dict_good:\n",
    "    hyper_dict_new[key] = hyper_dict_good[key]\n",
    "for key in hyper_dict_good1:\n",
    "    hyper_dict_new[key+42] = hyper_dict_good1[key]\n",
    "hyper_dict_new.keys()\n",
    "hyper_dict_good_tot = hyper_dict_new\n",
    "len(hyper_dict_good_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_dict_new = {}\n",
    "for key in hyper_dict_bad:\n",
    "    hyper_dict_new[key] = hyper_dict_bad[key]\n",
    "for key in hyper_dict_bad1:\n",
    "    hyper_dict_new[key+42] = hyper_dict_bad1[key]\n",
    "for key in hyper_dict_bad2:\n",
    "    hyper_dict_new[key+84] = hyper_dict_bad2[key]\n",
    "hyper_dict_new.keys()\n",
    "hyper_dict_bad_tot = hyper_dict_new\n",
    "len(hyper_dict_bad_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    " \n",
    "class HyperData(Dataset):\n",
    "    def __init__(self, root_dir, selected_bands, cls, hyper_dict_tot):\n",
    "        self.root_dir = root_dir\n",
    "        self.expand_size = (256, 256)\n",
    "\n",
    "        target_idxes = []\n",
    "        target_values = []\n",
    "        for target in selected_bands:\n",
    "            nearest_value, nearest_idx= find_nearest(wavelengths, target)\n",
    "            target_idxes.append(nearest_idx)\n",
    "            target_values.append(nearest_value)\n",
    "\n",
    "        self.target_idxes = target_idxes\n",
    "        self.target_values = target_values\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.cls = cls\n",
    "        self.hyper_dict_tot = hyper_dict_tot\n",
    "\n",
    "        sample_dirs = []\n",
    "        for dir_name in root_dir.iterdir():\n",
    "            sample_dirs.append(dir_name)\n",
    "        sample_dirs = sorted(sample_dirs, key=lambda x:int(x.name))\n",
    "        self.sample_dirs = sample_dirs\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        sample_dir = self.sample_dirs[idx]\n",
    "        imgs = []\n",
    "        haralick_features = []\n",
    "        for i_idx, _ in enumerate(self.target_idxes):\n",
    "            path = (sample_dir / (str(self.target_idxes[i_idx])+'_'+str(self.target_values[i_idx])+'.png')).__str__()\n",
    "            img = cv2.imread(path)\n",
    "            img = img[:, :, 0]\n",
    "            h, w = img.shape[:2]\n",
    "\n",
    "            one_im_haralick_features = mahotas.features.haralick(img).mean(axis=0)\n",
    "\n",
    "            expand_h, expand_w = self.expand_size\n",
    "            img = np.pad(img, pad_width=[(0, expand_h-h),(0, expand_w-w)], mode='constant', constant_values=0)\n",
    "\n",
    "            #fig = px.imshow(img)\n",
    "            #fig.show()\n",
    "\n",
    "            label = self.cls\n",
    "            imgs.append(img)\n",
    "            haralick_features.append(one_im_haralick_features)\n",
    "        imgs = np.array(imgs)\n",
    "        labels = np.array(label)\n",
    "        spectral_features = self.hyper_dict_tot[idx]\n",
    "        spectral_features = np.array(spectral_features)\n",
    "        spatial_features = np.array(haralick_features).reshape(-1, 13*len(self.target_idxes)).squeeze()\n",
    "        return imgs, labels, spectral_features, spatial_features\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.sample_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_dataset = HyperData(save_dir_im_data_good, selected_bands, 0, hyper_dict_good_tot)\n",
    "bad_dataset = HyperData(save_dir_im_data_bad, selected_bands, 1, hyper_dict_bad_tot)\n",
    "tot_dataset = good_dataset + bad_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 63)"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_num = len(tot_dataset)\n",
    "train_num = int(sample_num * 0.7)\n",
    "test_num = sample_num - train_num\n",
    "train_num, test_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "training_data, test_data = random_split(tot_dataset, [train_num, test_num], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y, spectral_features, spatial_features in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    print(f\"Shape of spectral_features: {spectral_features.shape} {spectral_features.dtype}\")\n",
    "    print(f\"Shape of spatial_features: {spatial_features.shape} {spatial_features.dtype}\")\n",
    "\n",
    "    print(y)\n",
    "    ims = X[0].numpy()\n",
    "    im_num = ims.shape[0]\n",
    "    for i_im in range(im_num):\n",
    "        fig = px.imshow(ims[i_im])\n",
    "        fig.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (cnn1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=1152, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "from torch import nn\n",
    "\n",
    "\"\"\"\n",
    "model design\n",
    "\"\"\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, c_input, device, final_cls, fc_tot_feature_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bias = torch.Tensor([128.0]).type(torch.float32).to(device)\n",
    "        self.scale = torch.Tensor([255.0]).type(torch.float32).to(device)\n",
    "\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            #transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        c_input = c_input\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(c_input, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.cnn2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool2d(2, stride=2, padding=1)\n",
    "\n",
    "        self.cnn3 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.max_pool2 = nn.MaxPool2d(2, stride=2, padding=1)\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            #nn.Linear(4096, 512),\n",
    "            nn.Linear(fc_tot_feature_num, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, final_cls)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, spectral_features=None, spatial_features=None):\n",
    "        x = self.preprocess(x)\n",
    "        x = (x - self.bias)/self.scale\n",
    "        x = self.cnn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        if spectral_features is not None:\n",
    "            x = torch.concat([x, spectral_features.type(torch.float32)], axis=1)\n",
    "        if spatial_features is not None:\n",
    "            x = torch.concat([x, spatial_features.type(torch.float32)], axis=1)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "c_input = len(selected_bands)\n",
    "model = NeuralNetwork(c_input, device, final_cls=2, fc_tot_feature_num=4636).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y, spectral_features, spatial_features) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        spectral_features, spatial_features = spectral_features.to(device), spatial_features.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X, spectral_features, spatial_features)\n",
    "        loss = loss_fn(pred, y.type(torch.int64))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y, spectral_features, spatial_features in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            spectral_features, spatial_features = spectral_features.to(device), spatial_features.to(device)\n",
    "\n",
    "            pred = model(X, spectral_features, spatial_features)\n",
    "            test_loss += loss_fn(pred, y.type(torch.int64)).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"good\",\n",
    "    \"bad\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try pretained resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "# or any of these variants\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "path_to_data = '/home/llj/ld/data/OpenEdit/train_latest/images/input_image/'\n",
    "BATCH_SIZE = 256\n",
    "nThreads = 4\n",
    "target_dir = 'targetdir'\n",
    "\n",
    "\n",
    "test_data = torchvision.datasets.ImageFolder(path_to_data, preprocess)\n",
    "image_names = test_data.samples\n",
    "data_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE,)\n",
    "model.to('cuda')\n",
    "\n",
    "probability = []\n",
    "count = 0\n",
    "result = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (x, y) in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
    "        x = x.to('cuda')\n",
    "        y = y.to('cuda')\n",
    "\n",
    "        output = model(x)\n",
    "        for index in range(output.shape[0]):\n",
    "            if output[index,703]>0.5:\n",
    "                result.append(image_names[count*BATCH_SIZE+index][0])\n",
    "        count += 1\n",
    "        # Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "        # print(output[-1])\n",
    "        # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "        # print(torch.nn.functional.softmax(output[-1], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# 修改全连接层的输出\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# 加载模型参数\n",
    "#checkpoint = torch.load(m_path)\n",
    "#resnet18.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "resnet18.to(device)\n",
    "resnet18.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdae7f16d060f2e4f4212789add6457d9369552c195e231f2a645ab38000492f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
